### A Convergence Theory for Deep Learning via Over-Parametrization
<arxiv.org/abs/1811.03962>
When network is overparametrized, SGD can find global minima in polynomial time.

### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies
<arxiv.org/abs/1906.00425>
Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Dynamical Isometry and a Mean Field Theory of RNNs
<arxiv.org/abs/1806.05394>
Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks
<arxiv.org/abs/1810.02054>
In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

### Steps Toward Deep Kernel Methods from Infinite Neural Networks
<arxiv.org/abs/1508/05133>
Devise stochastic kernels that encode the information of networks. 

## Physics Involved

### Asymptotics of wide networks from feynman diagrams
<arxiv.org/abs/1909.11304>
Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory
<arxiv.org/abs/2008.08601>
Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.
