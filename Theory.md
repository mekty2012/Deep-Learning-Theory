### A Convergence Theory for Deep Learning via Over-Parametrization

<https://www.arxiv.org/abs/1811.03962>

When network is overparametrized, SGD can find global minima in polynomial time.

### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies

<https://www.arxiv.org/abs/1906.00425>

Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks

<https://www.arxiv.org/abs/1810.02054>

In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

### Steps Toward Deep Kernel Methods from Infinite Neural Networks

<https://www.arxiv.org/abs/1508.05133>

Devise stochastic kernels that encode the information of networks. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

### Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

### Deep learning generalizes because the parameter-function map is biased towards simple functions

<https://www.arxiv.org/abs/1805.08522>

Using probability-complexity bound from algorithmic information theory, parameter-function map of many DNNs should be exponentially biased towards simple functions.

### Global inducing point varaitional posteriors for Bayesian neural networks and deep Gaussian processes

<https://www.arxiv.org/abs/2005.08140>

Consider the optimal approximate posterior over the top-layer weights, and show that it exhibits strong dependencies on the lower-layer weights, and also on Deep GP.
This idea uses learned global inducing points that is propagated through layer. 

### On Lazy Training in Differentiable Programming

<https://www.arxiv.org/abs/1812.07956>

Show that lazy training phenomenon is due to a choice of scaling, that makes the model behave as its linearization around the initialization.

### On the selection of Initialization and Activation Function for Deep Neural Networks

<https://www.arxiv.org/abs/1805.08266>

Show that for a class of ReLU-like activation functions, the information propagates deeper for an initialization at the edge of chaos.

### Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach

<https://www.arxiv.org/abs/1806.01316>

Investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value.
So it is locally flat in most dimensions, but strongly distorted in others.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width

<https://www.arxiv.org/abs/2002.04010>

Use k-th Taylor expansion of the neurl network at initialization, and show that the approximation error decay exponentially over k in wide neural networks.

### The large learning rate phase of deep learning: the catapult mechanism

<https://www.arxiv.org/abs/2003.02218>

Present a class of neural networks with solvable training dynamics, and see two learning rate phase with their phenomena.

### SGD Learns the Conjugate Kernel Class of the Network

<https://www.arxiv.org/abs/1702.08503>

Show that SGD is guaranteed to learn a function that is competitive with the best function in the conjugate kernel space, in polynomial time.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
ins cuh networks.

### Learning Overparametrized Neural Networks via Stochastic Gradient Descent on Structured Data

<https://www.arxiv.org/abs/1808.01204>

Prove that when the data comes from mixtures of well-separated distributions SGD learns a two-layer overparameterized ReLU-network with a small generalization error, even though the network can fit arbitrary labels.

### Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

<https://www.arxiv.org/abs/1901.08584>

Analyze training and generalization, give (1) why random label gives slower training (2) generalization bound independent of network size (3) learnability of a broad class of smooth functions.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

### Products of Many Large Random Matrices and Gradients in Deep Neural Networks

<https://www.arxiv.org/abs/1812.05994>

Given products of random matrices when the number of terms and the size of the matrices tend to infinity, show that logarithm of L2 norm of such a product applied to any fixed vector is asymptotically Gaussian.

### When Does Preconditioning Help or Hurt Generalization?

<https://www.arxiv.org/abs/2006.10732>

Prove an exact asymptotics bias-variance decompositions of the generalization error of overparametrized ridgeless regression under a general class of preconditioner, considering the inverse population Fisher information matrix as a particular example. 

### The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective

<https://www.arxiv.org/abs/2106.06529>

Decouples capacity and width via the genrealization of neural network to Deep Gaussian Process, aim to understand how width affects standard neural networks once they have sufficient capacity for a given modeling task. 

### The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization

<https://www.arxiv.org/abs/2106.04013>

Show that the ReLU ResNets exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio d/n. Show that ReLU ResNet is hypoactivated, that fewer than half of the ReLUs are activated.

### Implicit Acceleration and Feature Learning in Infinitely Wide Neural Networks with Bottlenecks

<https://www.arxiv.org/abs/2107.00364>

Analyze the learning dynamics of infinitely wide neural networks with a finite sized bottlenecks. This allows data dependent feature learning in its bottleneck representation, unlike NTK limit.

### A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs

<https://www.arxiv.org/abs/2106.04110>

Consider the DNNs trained with noisy gradient descent on a large training set and derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. 

### Generalization bounds for deep learning

<https://www.arxiv.org/abs/2012.04115>

Introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Focuse on generalization error upper bound, and inttroduce a categorisation of bounds depending on assumptions on the algorithm and data. 

### Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients

<https://www.arxiv.org/abs/2009.13447>

Explain the reason that resampling outperforms reweighting using tools from dynamical stability and stochastic asymptotics.

### Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks

<https://www.arxiv.org/abs/2108.12001>

Provide a theoretical justification for the finding that adversarial training shrinks two important characteristics of the logit distribution: the max logit values and the logit gaps are on average lower for AT models. 

### Approximation Properties of Deep ReLU CNNs

<https://arxiv.org/abs/2109.00190>

Analyzes the L2 approximation properties of deep ReLU convolutional neural networks on two-dimensional space. Using the decomposition of convolutional kernels, show the universal approximation.

### Reachability is NP-Complete Even for the Simplest Neural Networks

<https://arxiv.org/abs/2108.13179>

Investigate the complexity of the reachability problem for neural networks, showing that NP-hardness holds for even neural networks with just one layer and simple specifications.

### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

With the assumption of nonlinear conic approximation and unrealizable label vectors, show that a training problem with squared loss is necessarily unstable, i.e., its solution set depends discontinuously on the label vector in the training data. 

### Convex Geometry and Duality of Over-parameterized Neural Networks

<https://arxiv.org/abs/2002.11219>

Prove that an optimal solution to the regularized training problem can be characerized as extreme points of a convex set, so simple solutions are encouraged via its convex geometrical properties. 

### A Limititation of PAC-Bayes Framework

<https://arxiv.org/abs/2006.13508>

Present a limitation for the PAC-Bayes framework, by demonstrating that PAC-Bayes analysis can't prove that linear classification in 1D is learnable using just O(log(1/delta)/epsilon) examples.

### Quantized convolutional neural networks through the lens of partial differential equations

<https://www.arxiv.org/abs/2109.00095>

Explore ways to improved quantized CNNs using PDE-based perspective, harnessing the total variation approach to apply edge-aware smoothing.

### Exact priors of finite neural networks

<https://www.arxiv.org/abs/2104.11734>

Derive exact solutions for the output priors for individual input examples of a class of fintie fully-connected feedforward Bayesian neural networks.

### The emergence of a concept in shallow neural networks

<https://www.arxiv.org/abs/2109.00454>

Show that there exists a critical sample size beyond which the restricted boltzmann machines can learn archetypes. Leverage the formal equivalence beteen RBMs and Hopfield networks, obtain a phase diagram for both architectures which highlights the regions where learning can be accomplished.

### Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models

<https://www.arxiv.org/abs/2010.13933>

Analytically derive bias and variance in two minimal models, linear regression and two-layer neural newtork, using statistical physics. 

### Emergence of memory manifolds

<https://www.arxiv.org/abs/2109.03879>

Present a general principle called frozen stabilisation, allowing a family of neural networks to self-organise to a critical state exhibiting memory manifolds without parameter fine-tuning or symmetries.

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Provide a lower bound on the approximation rates for shallow neural networks, which are obtained by lower bounding the L2 metric entropy of the convex hull of the neural network basis functions. 

### Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training

<https://www.arxiv.org/abs/2101.12699>

Introduce Layer-Peeled Model which is a nonconvex yet analytically tractable optimization problem, that can better understand deep neural newtorks, obtained by sisolating the topmost layer from the remainder of the neural networks, with some constraints on the two parts of the network. Using this, prove that in class-balanced datasets, any solution forms a simplex equiangular tight frame, and show neural collapse in imbalanced problem.

### Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on Hoelder Class

<https://www.arxiv.org/abs/2103.00542>

For general continuous f on d dimensional box with continuity modulus, construct networks with sufficient approximation rate. This requires d^3/2 width, showing that this networks overcome the curse of dimensionality on Holder functions.

### Node Feature Kernels Increase Graph Convolutional Network Robustness

<https://www.arxiv.org/abs/2109.01785>

Using random matrix theory on GCN, show that if the graph is sufficiently random, the GCN fails to benefit from the node feature. then suggest the node feature kernel which solves this problem.

### A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning

<https://www.arxiv.org/abs/2109.02355>

Provides a succinct overview of this emerging theory of overparameterized ML that explains recent findings through a statistical signal processing perspective.

### Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks

<https://www.arxiv.org/abs/2006.13866>

Theoretically analyze the variance of sampling methods and show that, due to the composite structure of empirical risk, the variance of any sampling method can be decomposed into embedding approximation variance in the forward stage and stochastic gradient variance in the backward stage. Propose a decoupled variance reduction strategy.

### On the stability properties of Gated Recurrent Units neural networks

<https://www.arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteeing the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs. 

### Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation

<https://www.arxiv.org/abs/2002.11187>

Use GAN to estimate KL divergence, argue that high fluctuations in the estimates are a consequence of not controlling the complexity of the discriminator function space. Provide a theoretical underpinning and remedy for this problem by constructing a discriminator in the RKHS.

### Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs

<https://www.arxiv.org/abs/2006.16144>

Provide upper bound on the generalization error of PINNs approximating solutions of the forward problem for PDEs.

### A Unifying View on Implicit Bias in Training Linear Neural Networks

<https://www.arxiv.org/abs/2010.02501>

Propose a tensor formulation, and characterize the convergence direction as singular vectors, and show that gradient flow finds a stationary point or global minimum.

### The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks

<https://www.arxiv.org/abs/2109.06098>

Show the mathematical paradox, that any training procedure with a fixed architecture will yield neural networks that are either inaccurate or unstable. The key is that the stable and accurate neural networks must have variable dimensions depending on the input.

### On the regularized risk of distributionally robust learning over deep neural networks

<https://www.arxiv.org/abs/2109.06297>

Using tools from optimal transport theory, derive first order and second order approximations to the distributionally robust problem in terms of appropriate regularized risk minimization problems. 

### When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?

<https://www.arxiv.org/abs/2109.09444>

Provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound. Show that domain decomposition which decompose solution to simpler parts and make easier to solve, introduces a tradeoff for generalization, where the decomposition leads to less training data being available in each subdomain, prone to overfitting.

### On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods

<https://www.arxiv.org/abs/2109.10933>

Demonstrate that the norm test and inner product/orthogonality test are equivalent in terms of the convergence rates associated with SGD methods.

### Arbitrary-Depth Universal Approximation Theorems for Operator Neural Newtorks

<https://www.arxiv.org/abs/2109.11354>

Prove taht operator NNs of bounded width and arbitrary depth are universal approximators for continuous nonlinear operators. 

## Algebraic Topological Approach

### Topological Data Analysis of Decision Boundaries with Application to Model election

<https://www.arxiv.org/abs/1805.09949>

Propose the labeled complexes to perform persistent homology inference of decision boundaries in classification tasks, and provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples.

### Path Homologies of Deep Feedforward Networks

<https://www.arxiv.org/abs/1910.07617>

Characterize two types of directed homology for MLP, show that the directed flag homology reduces to computing the simplicical homology of the underlying undirected graph. This allows to investigate homological differences between NN architectures and their realized structure.

### Characterizing the Shape of Activation Space in Deep Neural Networks

<https://www.arxiv.org/abs/1901.09496>

Introduce a method for computing the persistent homology over the graphical activation structure of NN, which provides access to the task-relevant substructures activated throughout feed forward. Using this approach, show that existence of adversarial example is alternations to the dominant activation structures, suggesting the representation are sparse on the input space.

### Riemannian Curvature of Deep Neural Networks

<https://www.ieeexplore.ieee.org/abstract/document/8746812>

Define a method for calculating Riemann and Ricci curvature tensors for a trained neural net. 

### Topological Measurement of Deep Neural Networks Using Persistent Homology

<https://www.arxiv.org/abs/2106.03016>

Construct clique complex on trained DNNs, and compute the one-dimensional persistent homology of DNNs. This reveals the combinatorial effects of multiple neurons in DNNs at different resolution.

### Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology

<https://www.arxiv.org/abs/1812.09764>

Propose neural persistence, which is complexity measure on weighted stratified graphs, and show that neural persistence reflects best practices such as dropout and batch normalization.

### Estimate of the Neural Network Dimension using Algebraic Topology and Lie Theory

<https://www.arxiv.org/abs/204.02881>

Use persistent homology to investigate topological invariant of input space, then derive a decomposition of the underlying space with well known topology.

### Dive into Layers: Neural Network Capacity Bounding using Algebraic Geometry

<https://arxiv.org/abs/2109.01461>

Derive the upper bounds of the Betti numbers on each layer within the nwtwork, reducing the problem of architecture selection of a fully connected network boils down to choosing a suitable size of the network.


## Physics Involved

### Asymptotics of wide networks from feynman diagrams

<https://www.arxiv.org/abs/1909.11304>

Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory

<https://www.arxiv.org/abs/2008.08601>

Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.

### Finite size corrections for neural network Gaussian processes

<https://www.arxiv.org/abs/1908.10030>

Demonstrate that an ensemble of large finite FC network with a single hidden layer is well described by a Gaussian perturbed by the fourth Hermite polynomial, and the scale is 
inversely proportional to the number of units and that higher order terms decay more rapidly, recovering Edgeworth expansion.

### Non-Gaussian processes and neural networks at finite widths

<https://www.arxiv.org/abs/1910.00019>

Perturbatively extend NNGP correspondence to finite width neural network, yielding non-Gaussian processes as priors. This allows to track the flow of preactivation by
marginally integrating random variables, reminiscent of renormalization-group flow.
