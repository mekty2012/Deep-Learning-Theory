### A Convergence Theory for Deep Learning via Over-Parametrization

<https://www.arxiv.org/abs/1811.03962>

When network is overparametrized, SGD can find global minima in polynomial time.

### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies

<https://www.arxiv.org/abs/1906.00425>

Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks

<https://www.arxiv.org/abs/1810.02054>

In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

### Steps Toward Deep Kernel Methods from Infinite Neural Networks

<https://www.arxiv.org/abs/1508.05133>

Devise stochastic kernels that encode the information of networks. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

## Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

## Deep learning generalizes because the parameter-function map is biased towards simple functions

<https://www.arxiv.org/abs/1805.08522>

Using probability-complexity bound from algorithmic information theory, parameter-function map of many DNNs should be exponentially biased towards simple functions.

### Global inducing point varaitional posteriors for Bayesian neural networks and deep Gaussian processes

<https://www.arxiv.org/abs/2005.08140>

Consider the optimal approximate posterior over the top-layer weights, and show that it exhibits strong dependencies on the lower-layer weights, and also on Deep GP.
This idea uses learned global inducing points that is propagated through layer. 

## On Lazy Training in Differentiable Programming

<https://www.arxiv.org/abs/1812.07956>

Show that lazy training phenomenon is due to a choice of scaling, that makes the model behave as its linearization around the initialization.

### On the selection of Initialization and Activation Function for Deep Neural Networks

<https://www.arxiv.org/abs/1805.08266>

Show that for a class of ReLU-like activation functions, the information propagates deeper for an initialization at the edge of chaos.

### Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach

<https://www.arxiv.org/abs/1806.01316>

Investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value.
So it is locally flat in most dimensions, but strongly distorted in others.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width

<https://www.arxiv.org/abs/2002.04010>

Use k-th Taylor expansion of the neurl network at initialization, and show that the approximation error decay exponentially over k in wide neural networks.

### The large learning rate phase of deep learning: the catapult mechanism

<https://www.arxiv.org/abs/2003.02218>

Present a class of neural networks with solvable training dynamics, and see two learning rate phase with their phenomena.

### SGD Learns the Conjugate Kernel Class of the Network

<https://www.arxiv.org/abs/1702.08503>

Show that SGD is guaranteed to learn a function that is competitive with the best function in the conjugate kernel space, in polynomial time.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
ins cuh networks.

### Learning Overparametrized Neural Networks via Stochastic Gradient Descent on Structured Data

<https://www.arxiv.org/abs/1808.01204>

Prove that when the data comes from mixtures of well-separated distributions SGD learns a two-layer overparameterized ReLU-network with a small generalization error, even though the network can fit arbitrary labels.

### Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

<https://www.arxiv.org/abs/1901.08584>

Analyze training and generalization, give (1) why random label gives slower training (2) generalization bound independent of network size (3) learnability of a broad class of smooth functions.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

### Products of Many Large Random Matrices and Gradients in Deep Neural Networks

<https://www.arxiv.org/abs/1812.05994>

Given products of random matrices when the number of terms and the size of the matrices tend to infinity, show that logarithm of L2 norm of such a product applied to any fixed vector is asymptotically Gaussian.

### When Does Preconditioning Help or Hurt Generalization?

<https://www.arxiv.org/abs/2006.10732>

Prove an exact asymptotics bias-variance decompositions of the generalization error of overparametrized ridgeless regression under a general class of preconditioner, considering the inverse population Fisher information matrix as a particular example. 

## The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective

<https://www.arxiv.org/abs/2106.06529>

Decouples capacity and width via the genrealization of neural network to Deep Gaussian Process, aim to understand how width affects standard neural networks once they have sufficient capacity for a given modeling task. 

## The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization

<https://www.arxiv.org/abs/2106.04013>

Show that the ReLU ResNets exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio d/n. Show that ReLU ResNet is hypoactivated, that fewer than half of the ReLUs are activated.

### Implicit Acceleration and Feature Learning in Infinitely Wide Neural Networks with Bottlenecks

<https://www.arxiv.org/abs/2107.00364>

Analyze the learning dynamics of infinitely wide neural networks with a finite sized bottlenecks. This allows data dependent feature learning in its bottleneck representation, unlike NTK limit.

## A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs

<https://www.arxiv.org/abs/2106.04110>

Consider the DNNs trained with noisy gradient descent on a large training set and derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. 

### Generalization bounds for deep learning

<https://www.arxiv.org/abs/2012.04115>

Introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Focuse on generalization error upper bound, and inttroduce a categorisation of bounds depending on assumptions on the algorithm and data. 

### Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients

<https://www.arxiv.org/abs/2009.13447>

Explain the reason that resampling outperforms reweighting using tools from dynamical stability and stochastic asymptotics.

### Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks

<https://www.arxiv.org/abs/2108.12001>

Provide a theoretical justification for the finding that adversarial training shrinks two important characteristics of the logit distribution: the max logit values and the logit gaps are on average lower for AT models. 

### Approximation Properties of Deep ReLU CNNs

<https://arxiv.org/abs/2109.00190>

Analyzes the L2 approximation properties of deep ReLU convolutional neural networks on two-dimensional space. Using the decomposition of convolutional kernels, show the universal approximation.

### Reachability is NP-Complete Even for the Simplest Neural Networks

<https://arxiv.org/abs/2108.13179>

Investigate the complexity of the reachability problem for neural networks, showing that NP-hardness holds for even neural networks with just one layer and simple specifications.

### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

With the assumption of nonlinear conic approximation and unrealizable label vectors, show that a training problem with squared loss is necessarily unstable, i.e., its solution set depends discontinuously on the label vector in the training data. 

### Convex Geometry and Duality of Over-parameterized Neural Networks

<https://arxiv.org/abs/2002.11219>

Prove that an optimal solution to the regularized training problem can be characerized as extreme points of a convex set, so simple solutions are encouraged via its convex geometrical properties. 

### A Limitation of PAC-Bayes Framework

<https://arxiv.org/abs/2006.13508>

Present a limitation for the PAC-Bayes framework, by demonstrating that PAC-Bayes analysis can't prove that linear classification in 1D is learnable using just O(log(1/delta)/epsilon) examples.

### Quantized convolutional neural networks through the lens of partial differential equations

<https://www.arxiv.org/abs/2109.00095>

Explore ways to improved quantized CNNs using PDE-based perspective, harnessing the total variation approach to apply edge-aware smoothing.

### Exact priors of finite neural networks

<https://www.arxiv.org/abs/2104.11734>

Derive exact solutions for the output priors for individual input examples of a class of fintie fully-connected feedforward Bayesian neural networks.

### The emergence of a concept in shallow neural networks

<https://www.arxiv.org/abs/2109.00454>

Show that there exists a critical sample size beyond which the restricted boltzmann machines can learn archetypes. Leverage the formal equivalence beteen RBMs and Hopfield networks, obtain a phase diagram for both architectures which highlights the regions where learning can be accomplished.

### Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models

<https://www.arxiv.org/abs/2010.13933>

Analytically derive bias and variance in two minimal models, linear regression and two-layer neural newtork, using statistical physics. 

### Emergence of memory manifolds

<https://www.arxiv.org/abs/2109.03879>

Present a general principle called frozen stabilisation, allowing a family of neural networks to self-organise to a critical state exhibiting memory manifolds without parameter fine-tuning or symmetries.

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Provide a lower bound on the approximation rates for shallow neural networks, which are obtained by lower bounding the L2 metric entropy of the convex hull of the neural network basis functions. 

### Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training

<https://www.arxiv.org/abs/2101.12699>

Introduce Layer-Peeled Model which is a nonconvex yet analytically tractable optimization problem, that can better understand deep neural newtorks, obtained by sisolating the topmost layer from the remainder of the neural networks, with some constraints on the two parts of the network. Using this, prove that in class-balanced datasets, any solution forms a simplex equiangular tight frame, and show neural collapse in imbalanced problem.

### Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on Hoelder Class

<https://www.arxiv.org/abs/2103.00542>

For general continuous f on d dimensional box with continuity modulus, construct networks with sufficient approximation rate. This requires d^3/2 width, showing that this networks overcome the curse of dimensionality on Holder functions.

### Node Feature Kernels Increase Graph Convolutional Network Robustness

<https://www.arxiv.org/abs/2109.01785>

Using random matrix theory on GCN, show that if the graph is sufficiently random, the GCN fails to benefit from the node feature. then suggest the node feature kernel which solves this problem.

### A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning

<https://www.arxiv.org/abs/2109.02355>

Provides a succinct overview of this emerging theory of overparameterized ML that explains recent findings through a statistical signal processing perspective.

### Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks

<https://www.arxiv.org/abs/2006.13866>

Theoretically analyze the variance of sampling methods and show that, due to the composite structure of empirical risk, the variance of any sampling method can be decomposed into embedding approximation variance in the forward stage and stochastic gradient variance in the backward stage. Propose a decoupled variance reduction strategy.

### On the stability properties of Gated Recurrent Units neural networks

<https://www.arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteeing the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs. 

### Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation

<https://www.arxiv.org/abs/2002.11187>

Use GAN to estimate KL divergence, argue that high fluctuations in the estimates are a consequence of not controlling the complexity of the discriminator function space. Provide a theoretical underpinning and remedy for this problem by constructing a discriminator in the RKHS.

### Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs

<https://www.arxiv.org/abs/2006.16144>

Provide upper bound on the generalization error of PINNs approximating solutions of the forward problem for PDEs.

### A Unifying View on Implicit Bias in Training Linear Neural Networks

<https://www.arxiv.org/abs/2010.02501>

Propose a tensor formulation, and characterize the convergence direction as singular vectors, and show that gradient flow finds a stationary point or global minimum.

### The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks

<https://www.arxiv.org/abs/2109.06098>

Show the mathematical paradox, that any training procedure with a fixed architecture will yield neural networks that are either inaccurate or unstable. The key is that the stable and accurate neural networks must have variable dimensions depending on the input.

### On the regularized risk of distributionally robust learning over deep neural networks

<https://www.arxiv.org/abs/2109.06297>

Using tools from optimal transport theory, derive first order and second order approximations to the distributionally robust problem in terms of appropriate regularized risk minimization problems. 

### When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?

<https://www.arxiv.org/abs/2109.09444>

Provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound. Show that domain decomposition which decompose solution to simpler parts and make easier to solve, introduces a tradeoff for generalization, where the decomposition leads to less training data being available in each subdomain, prone to overfitting.

### On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods

<https://www.arxiv.org/abs/2109.10933>

Demonstrate that the norm test and inner product/orthogonality test are equivalent in terms of the convergence rates associated with SGD methods.

### Arbitrary-Depth Universal Approximation Theorems for Operator Neural Newtorks

<https://www.arxiv.org/abs/2109.11354>

Prove that operator NNs of bounded width and arbitrary depth are universal approximators for continuous nonlinear operators. 

### Convolutional Rectifier Networks as Generalized Tensor Decompositions

<https://www.arxiv.org/abs/1603.00162>

Describe a construction based on generalized tensor decompositions that transforms convolutional arithmetic circuits into convolutional rectifier networks, then use toold from the world of arithmetic circuits. Show that convolutional rectifier networks are universal with max pooling but not with average pooling. Also show that depth efficiency is weker with convolutional rectifier networks than convolutional arithmetic circuits.

### Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions

<https://www.arxiv.org/abs/1705.02302>

Through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features.

## Deep Learning and the Information Bottleneck Principle

<https://www.arxiv.org/abs/1503.02406>

Show that any DNN can be quantified by the mutual information between the layers and the input and output variables, and calculate the optimal information theoretical limits of the DNN and obtain finite sample generalization bounds.

## The Emergence of Spectral Universality in Deep Networks

<https://www.arxiv.org/abs/1802.09979>

Using the tools from free probability theory, prove a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparametrs including the nonlinearity, the weight and bias distirbutions, and the depth. 

## Nonlinear random matrix theory for deep learning

<https://proceedings.neurips.cc/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf>

Show that pointwise nonlinearities can be incorporated into a standard method of proof in random matrix theory known as the moments method.

## Learning Dynamics of Deep Networks Admit Low-rank Tensor Descriptions

<https://openreview.net/pdf?id=Hy7RHt1vz>

Propose a simple tensor decomposition model to study how hidden representations evolve over learning, which precisely extracts the correct dynamics of learning and closed form solutions.

### Understanding Black-box Predictions via Influence Functions

<https://arxiv.org/abs/1703.04730>

Show that even on non-convex and non-differentiable models, approximations to influence functions can still provide valuable information. 

### Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

<https://arxiv.org/abs/1610.09887>

Prove that various types of simple and natural functions, including indicators of balls and ellipses, non-linear radial functions, smooth non-linear functions, can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger.

### Spurious Local Minima are Common in Two-Layer ReLU Neural Networks

<https://arxiv.org/abs/1712.08968>

Show that two-layer ReLU networks w.r.t. the squared loss has local minima, even if the input distribution is standard Gaussian, dimension is arbitrarilty large, and orthonormal parameter vectors, using computer-assisted proof.

### Stronger generalization bounds for deep nets via a compression approach

<https://arxiv.org/abs/1802.05296>

Use an explicit and efficient compression, which yields generalization bounds via a simple compression-based framework, and provide some theoretical justification for widespread empirical success in compressing deep nets.

### Understanding Convolutional Neural Networks with Information Theory: An Initial Exploration

<https://arxiv.org/abs/1804.06537>

Show that the estimators enable straightforward measurement of information flow in realistic convolutional neural networks without any approximation, and introduce the partial information decomposition framework, develop three quantities to analyze the synergy and redundancy in convolutional layer representations.

### Topology-based Representative Datasets to Reduce Neural Network Training Resources

<https://arxiv.org/abs/1903.08519>

Prove that the accuracy of the learning process of a neural network on a representative dataset is similar to the accuracy on the original dataset, where representativeness is measured using persistence diagrams.

### Asymptotic Freeness of Layerwise Jacobians Caused by Invariance of Multilayer Perceptron: The Haar Orthogonal Case

<https://arxiv.org/abs/2103.13466>

Prove asymptotic freeness of layerwise Jacobians of multilayer perceptrons, using an invariance of the MLP. 

### Ridgeless Interpolation with Shallow ReLU Networks in 1D is Nearest Neighbor Curvature Extrapolation and Provably Generalizes on Lipscitz Fnctions

<https://arxiv.org/abs/2109.12960>

Prove a precise geometric description of all one layer ReLU networks with a single linear unit, with single input/output dimension, which interpolates a given dataset. Also show that ridgeless ReLU interpolants achieve the best possible generalization for learning 1d Lipscitz functions, up to universal constants.

### Searching for Minimal Optimal Neural Networks

<https://arxiv.org/abs/2109.13061>

Propose a rigorous mathematical framework for studying the asymptotic theory of the destructive technique, and prove that Adaptive group Lasso is consistent and can reconstruct the correct number of hidden nodes of one-hidden-layer feedforward networks with high probability.

### What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory

<https://arxiv.org/abs/2105.03361>

Develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. Propose a new function space, that captures the compositional structure associated deep neural networks. Derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space.

### Towards a theory of out-of-distribution learning

<https://arxiv.org/abs/2109.14501>

Define and prove the relationship between generalized notions of learnability, and show how this framework is sufficiently general to characterize transfer, multitask, meta, continual, and lifelong learning.

### Generalization Bounds For Meta-Learning: An Information-Theoretical Analysis

<https://arxiv.org/abs/2109.14595>

Derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms.

### On the Variance of the Fisher Information for Deep Learning

<https://arxiv.org/abs/2107.04205>

Investigate two estimators based on two equivalent representations of the FIM, and bound their variances and analyze how the parametric structure of a deep neural network can impact the variance.

### Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions

<https://arxiv.org/abs/2106.02619>

Prove that when a distribution has a structure that referred as Forward Super-Resolution, then training GANs using gradient descent ascent can indeed learn this distribution efficiently both in terms of sample and time complexities.

### On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow

<https://arxiv.org/abs/2011.02402>

Show that parametric kernelized gradient flow provides a descent direction minimizing the MMD on a statistical manifold of probability distributions.

### Avoiding pathologies in very deep networks

<https://arxiv.org/abs/1402.5836>

Show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, and propose an alternate architecture which does not suffer from this pathology.

### The Complexity of Gradient Descent: CLS = PPAD intersection PLS

<https://arxiv.org/abs/2011.01929>

Show that search problems that can be solved by performing gradient descent on a bounded convex polytopal domain is equal to intersection of PPAD and PLS. Also show that computing a KKT point of a continuously differentiable function over the domain [0,1]^2 is complete. 

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://arxiv.org/abs/2106.14997>

Prove sharp lower bounds on the approximation rates for shallow neural networks, obtained by lower bounding L2 metric entropy of the convex hull of basis functions. 

### Deep Network Approximation for Smooth Functions

<https://arxiv.org/abs/2001.03040>

Prove that multivariate polynomials can be approximated by deep ReLU networks of with small enough approximation error, then through local Taylor expansions, show that deep ReLU networks can approximate C^s functions.

### Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability

<https://arxiv.org/abs/2109.11792>

Show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability.

### Understanding How Over-Parameterization Leads to Acceleration: A case of learning a single teacher neuron

<https://arxiv.org/abs/2010.01637>

In the setting with single teacher neuron with quadratic activation and over parametrization realized by ahving multiple student neurons, provably show that over-parameterization helps the gradient descent iteration enter the neighborhood of a global optimal solution.

### Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion

<https://arxiv.org/abs/2107.09133>

Derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. Show that the key ingredient driving these dynamics is not the origianl training loss, but rather the combination of a modified loss.

### Certifiably Robust Variational Autoencoders

<https://arxiv.org/abs/2102.07559>

Derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount. Then show how these parameters can be controlled, providing a mechanism to ensure desired level of robustness.

### Bayesian Neural Network Priors Revisited

<https://arxiv.org/abs/2102.06571>

Find that CNN weights display strong spatial correlations, while FCNNs display heavy-tailed weight distributions.

### An Unconstrained Layer-Peeled Perspective on Neural Collapse

<https://arxiv.org/abs/2110.02796>

Prove that gradient flow on unconstrained layer-peeled model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Then prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon.

### Exploring the Common Principal Subspace of Deep Features in Neural Networks

<https://arxiv.org/abs/2110.02863>

Find that different DNNs trained with the same dataset share a common principal subspace in latent spaces no matter architectures and labels. Specifically, design a new metric P-vector to represent the principal subspace of dee features learned, and propose to measure angles between the principal subspaces using P-vectors, with small angles have been found.

### VC dimension of partially quantized neural networks in the overparameterized regime

<https://arxiv.org/abs/2110.02456>

Focus hyperplane arrangement neural networks, and show that HANNs can have VC dimension significantly smaller than the number of weights while being highly expressive.

### On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks

<https://arxiv.org/abs/1901.10371>

Show that adversarial training tends to promote simultaneously low-rank and sparse structure. In the reverse direction, when the low rank structure is promoted by nclear norm regularization, neural networks show significantly improved robustness.

### Optimizing Neural Networks via Koopman Operator Theory

<https://arxiv.org/abs/2006.02361>

Show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of MLPs over a non-trivial range of training time.

### Universality of Deep Neural Networks Lottery Tickets: A Renormalization Group Perspective

<https://arxiv.org/abs/2110.03210>

Using renormalization group theory, find that iterative magnitude pruning is a renormalization group scheme, which is the method used for discovering winning tickets.

### Spectral Pruning for Recurrent Neural Networks

<https://arxiv.org/abs/2105.10832>

Propose a pruning algorithm so called spectral pruning for RNN, and provide the generalization error bounds for compressed RNNs.

### Tighter Sparse Approximation Bounds for ReLU Neural Networks

<https://arxiv.org/abs/2110.03673>

Derive sparse neural network approximation bounds that refine previous works, and show that infinite-width neural network representations on bounded open sets are not unique.

### On the Optimal Memorization Power of ReLU Neural Networks

<https://arxiv.org/abs/2110.03187>

Show that networks can memorize any N points using sqrt(N) parameters with some separability assumptions, which is optimal up to logarithmic factors.

### Pathologies in priors and inference for Bayesian transformers

<https://arxiv.org/abs/2110.04020>

Weight-space inference in transformers does not work well, regardless of the approximate posterior. Also find that the prior is at least partially at fault but that it is very hard to find well-specified weight priors for these models.

### On the stability properties of Gated Recurrent Units neural networks

<https://arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteering the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs, which consist of nonlinear-inequalities on network's weights.

### A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Komogorov partial differential equations with constant diffusion and nonlinear drift coefficients

<https://arxiv.org/abs/1809.07321>

Prove that the number of parameters used to describe the employed DNN grows at most polynomially in both the PDE dimension d and the reciprocal of the prescribed approximation accuracy.

### Uniform error estimates for artificial neural network approximations for heat equations

<https://arxiv.org/abs/1911.09647>

Develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs in the uniform L infty sense. Prove that the number of parameters of an ANN to uniformly approximate the classical solution of the heat equation in a region \[a,b\]^d for a fixed time point T grows at most polynomially in the dimension d and the reciprocal of the approximation precision. 

### Approximation capabilities of neural networks on unbounded domains

<https://arxiv.org/abs/1910.09293>

Prove that a shallow neural network with some activation functions can arbitrarily well approximate any Lp integrable functions defined on R * \[0,1\]^n, and moreover integrable function on the Euclidean plane.

### Stability of Neural Networks on Manifold to Relative Perturbations

<https://arxiv.org/abs/2110.04702>

Prove that manifold neural networks composed of frequency ratio threshold filters, which separates the infinite-dimensional spectrum of the Laplace-Beltrami operator, are stable to relative operator perturbations. Observe that manifold neural networks exhibit a trade-off between stability and discriminability.

### How Well Generative Adversarial Networks Learn Distributions

<https://arxiv.org/abs/1811.03179>

Nonparametrically, derive the optimal minimax rates for distribution estimation under the adversarial framework. Parametrically, estabilsh a theory for general neural network classes that characterized the interplay on the choice of generator and discriminator pair.

### Phase Collapse in Neural Networks

<https://arxiv.org/abs/2110.05283>

By defining simplified complex-valued convolutional network architecture, which implements convolution with wavelet filters and uses a complex modulus to collapse phase variables, demonstrate that it is a different phase collapse mechanism which explains the ability to progressively eliminate spatial variability.

### Does Preprocessing Help Training Over-parameterized Neural Networks?

<https://arxiv.org/abs/2110.04622>

Design preprocessing algorithm for layer and input data, with convergence guarantee and lower train cost.

## Synthesizing Machine Learning Programs with PAC Guarantees via Statistical Sketching

<https://arxiv.org/abs/2110.05390>

Propose novel algorithms for sketching and synthesizing PAC programs, by leveraging ideas from statistical learning theory to provide statistical soundness guarantees.

### Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks

<https://arxiv.org/abs/2110.03825>

Provide a theoretical analysis explaning on following observations, that 1) model parameters does not necessarily help adversarial robustness, 2) reducing capacity at the last stage of the network can actually improve adversarial robustness, and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness.

### Implicit Bias of Linear Equivariant Networks

<https://arxiv.org/abs/2110.06084>

Show that L layer full width linear GCNNs trained via gradient descent in a binary classification task converge to solutions with low-rank Fourier matrix coefficients, regularized by the 2/L-Schatten matrix norm. This generalizes previous analysis on the implicit bias of linear CNNs to linear GCNNs over all finite groups, including the challenging setting of non-commutative symmetry groups.

## Imitiating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations

<https://arxiv.org/abs/2110.05960>

Model the evolution of features during deep learning training using a set of SDEs that each corresponds to a training sample, where each SDE constains a drift term that reflects the impact of backpropagation at an input on the features of all samples. This uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic - the impact is more significant on samples from the same class as the input - the featrues of the training data become linearly separable, vanishing trainin loss; otherwise, the features are not separable. Also show the emergence of a simple geometric structure called the neural collapse of the features.

### Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs

<https://arxiv.org/abs/2110.05518>

Show that the training of multiple three-layer ReLU sub-networks with weight decay regularization can be equivalently cast as a convex optimization problem in a higher dimensional space, where sparsity is enforced via a group l1-norm regularization. Then prove that equivalent convex problem can be globally optimized by a standard convex optimization solve with a polynomial-time complexity w.r.t. number of samples and data dimension.

### Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Pruned Neural Networks

<https://arxiv.org/abs/2110.05667>

Characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. Show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned. 

### A global convergence theory for deep ReLU implicit networks via over-parameterization

<https://arxiv.org/abs/2110.05645>

Show that a randomly initialized gradient descent converges to a global minimum at a linear rate for the square loss function if the implicit neural network is over parameterized.

### Non-convergence of stochastic gradient descent in the training of deep neural networks

<https://arxiv.org/abs/2006.07075>

Show that stochastic gradient descent can fail if depth is much larger than their width, and the number of random initialization does not increase to infinity fast enough.

### Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck

<https://arxiv.org/abs/2006.07522>

Analyze BNNs through the information bottleneck principle and observe that the training dynamics of BNNs is different from that of DNNs. While DNNs have a separate empirical risk minimization and representation compression phases, BNNs tend to find efficient hidden representations concurrently with label fitting.

### A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning

<https://arxiv.org/abs/1912.00827>

Analyze the performance of a simple regression model trained on the random features for a random weight matrix and random bias vector, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. Find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoencoding task.

### Towards Statistical and Computational Complexities of Polyak Step Size Gradient Descent

<https://arxiv.org/abs/2110.07810>

Demonstrate that the Polyak step size gradient descent iterates reach a final statistical radius of convergence around the true parameter after logarithmic number of iterations.

### Well-classified Examples are Underestimated in Classification with Deep Neural Networks

<https://arxiv.org/abs/2110.06537>

Theoretically show that giving less gradient for well-classified examples hinders representation learning, energy optimization, and the growth of margin. Propose to reward well-classified examples with additive bonuses to revive their contribution to learning.

### Detecting Modularity in Deep Neural Networks

<https://arxiv.org/abs/2110.08058>

Consider the problem of assessing the modularity exhibited by a partitioning of a network's neurons. Propose two proxies, importance and coherence measured by statistical methods. Then apply the proxies to partitionings generated by spectrally clustering neurons and show that these partitionings reveal groups of neurons that are important and coherent.

### Dropout as a Regularizer of Interaction Effects

<https://arxiv.org/abs/2007.00823>

Prove that dropout regularizes against higher-order interactions. 

### Uniform convergence may be unable to explain generalization in deep learning

<https://arxiv.org/abs/1902.04742>

Present examples of overparameterized linear classifiers and neural networks trained by gradient descent where uniform convergence provably cannot explain generalization, even if we take into account the implicit bias of GD. 

### On the capacity of deep generative networks for approximating distributions

<https://arxiv.org/abs/2101.12353>

Prove that neural networks can transform a low-dimensional source distribution to a distribution that is arbitrarily close to a high-dimensional target distribution, when the closeness are measured by Wasserstein distances and maximum mean discrepancy. 

### Understanding Convolutional Neural Networks from Theoretical Perspective via Volterra Convolution

<https://arxiv.org/abs/2110.09902>

Show that CNN is an approximation of the finite term Volterra convolution, whose order increases exponentially with the number of layers and kernel size increases exponentially with the strides.

### When Are Solutions Connected in Deep Networks?

<https://arxiv.org/abs/2102.09671>

Show that under generic assumptions on the features of intermediate layers, it suffices that the last two hiddne layers have order of sqrt(N) neurons, and if subsets of features at each layer are linearly separable, then no over-parameterization is needed to show the connectivity. 

### Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem

<https://arxiv.org/abs/2110.10295>

Prove that periodic points alone lead to suboptimal depth-width tradeoffs and improve upon them by demonstrating that certain "chaotic itineraries" give stronger exponential tradeoffs. Identify a phase transition to the chaotic regime that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy.

### Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks

<https://arxiv.org/abs/2110.10815>

Provide convergence guarantees with rates for deep linear networks for both continuous and discrete dynamics on FA algorithms.

### Analyzing the expressive power of graph neural networks in a spectral perspective

<https://www.researchgate.net/publication/349119879_ANALYZING_THE_EXPRESSIVE_POWER_OF_GRAPH_NEURAL_NETWORKS_IN_A_SPECTRAL_PERSPECTIVE>

By bridging the gap between the spectral and spatial design of graph convolutions, theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain.

### Early Stopping in Deep Networks: Double Descent and How to Eliminate It

<https://arxiv.org/abs/2007.10099>

Show that epoch-wise double descent arises by a superposition of two or more bias-variance tradeoff that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. Show this analytically for linear regression and a two-layer neural network.

# Algebraic Topological Approach

### Topological Data Analysis of Decision Boundaries with Application to Model election

<https://www.arxiv.org/abs/1805.09949>

Propose the labeled complexes to perform persistent homology inference of decision boundaries in classification tasks, and provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples.

### Path Homologies of Deep Feedforward Networks

<https://www.arxiv.org/abs/1910.07617>

Characterize two types of directed homology for MLP, show that the directed flag homology reduces to computing the simplicical homology of the underlying undirected graph. This allows to investigate homological differences between NN architectures and their realized structure.

### Characterizing the Shape of Activation Space in Deep Neural Networks

<https://www.arxiv.org/abs/1901.09496>

Introduce a method for computing the persistent homology over the graphical activation structure of NN, which provides access to the task-relevant substructures activated throughout feed forward. Using this approach, show that existence of adversarial example is alternations to the dominant activation structures, suggesting the representation are sparse on the input space.

### Riemannian Curvature of Deep Neural Networks

<https://www.ieeexplore.ieee.org/abstract/document/8746812>

Define a method for calculating Riemann and Ricci curvature tensors for a trained neural net. 

### Topological Measurement of Deep Neural Networks Using Persistent Homology

<https://www.arxiv.org/abs/2106.03016>

Construct clique complex on trained DNNs, and compute the one-dimensional persistent homology of DNNs. This reveals the combinatorial effects of multiple neurons in DNNs at different resolution.

### Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology

<https://www.arxiv.org/abs/1812.09764>

Propose neural persistence, which is complexity measure on weighted stratified graphs, and show that neural persistence reflects best practices such as dropout and batch normalization.

### Estimate of the Neural Network Dimension using Algebraic Topology and Lie Theory

<https://www.arxiv.org/abs/204.02881>

Use persistent homology to investigate topological invariant of input space, then derive a decomposition of the underlying space with well known topology.

### Dive into Layers: Neural Network Capacity Bounding using Algebraic Geometry

<https://arxiv.org/abs/2109.01461>

Derive the upper bounds of the Betti numbers on each layer within the network, reducing the problem of architecture selection of a fully connected network boils down to choosing a suitable size of the network.

### On Characterizing the Capacity of Neural Networks using Algebraic Topology

<https://arxiv.org/abs/1802.04443>

Show that the power of the topological capacity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. 

### The Intrinsic dimension of Images and Its Impact on Learning

<https://arxiv.org/abs/2104.08894>

Apply dimension estimation tools to popular datasets, find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Find that low dimensional datasets are easier for NNs to learn, and models solving these tasks generalize better from training to test data.

# Physics Involved

### Asymptotics of wide networks from feynman diagrams

<https://www.arxiv.org/abs/1909.11304>

Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory

<https://www.arxiv.org/abs/2008.08601>

Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.

### Finite size corrections for neural network Gaussian processes

<https://www.arxiv.org/abs/1908.10030>

Demonstrate that an ensemble of large finite FC network with a single hidden layer is well described by a Gaussian perturbed by the fourth Hermite polynomial, and the scale is 
inversely proportional to the number of units and that higher order terms decay more rapidly, recovering Edgeworth expansion.

### Non-Gaussian processes and neural networks at finite widths

<https://www.arxiv.org/abs/1910.00019>

Perturbatively extend NNGP correspondence to finite width neural network, yielding non-Gaussian processes as priors. This allows to track the flow of preactivation by
marginally integrating random variables, reminiscent of renormalization-group flow.

### Learning through atypical "phase transitions" in overparameterized neural networks

<https://arxiv.org/abs/2110.00683>

Use methods from statistical physics to analytically study the computational fallout of overparameterization in nonconvex neural network models. 

### A Theoretical Connection Between Statistical Physics and Reinforcement Learning

<https://arxiv.org/abs/1906.10228>

Construct a partition function from the ensemble of possible trajectories, which gives its own Bellman equation with solution tightly linked Boltzmann-like policy parameterizations.
