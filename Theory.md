### A Convergence Theory for Deep Learning via Over-Parametrization

<https://www.arxiv.org/abs/1811.03962>

When network is overparametrized, SGD can find global minima in polynomial time.

### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies

<https://www.arxiv.org/abs/1906.00425>

Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks

<https://www.arxiv.org/abs/1810.02054>

In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

### Steps Toward Deep Kernel Methods from Infinite Neural Networks

<https://www.arxiv.org/abs/1508.05133>

Devise stochastic kernels that encode the information of networks. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

### Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

### Deep learning generalizes because the parameter-function map is biased towards simple functions

<https://www.arxiv.org/abs/1805.08522>

Using probability-complexity bound from algorithmic information theory, parameter-function map of many DNNs should be exponentially biased towards simple functions.

### Global inducing point varaitional posteriors for Bayesian neural networks and deep Gaussian processes

<https://www.arxiv.org/abs/2005.08140>

Consider the optimal approximate posterior over the top-layer weights, and show that it exhibits strong dependencies on the lower-layer weights, and also on Deep GP.
This idea uses learned global inducing points that is propagated through layer. 

### On Lazy Training in Differentiable Programming

<https://www.arxiv.org/abs/1812.07956>

Show that lazy training phenomenon is due to a choice of scaling, that makes the model behave as its linearization around the initialization.

### On the selection of Initialization and Activation Function for Deep Neural Networks

<https://www.arxiv.org/abs/1805.08266>

Show that for a class of ReLU-like activation functions, the information propagates deeper for an initialization at the edge of chaos.

### Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach

<https://www.arxiv.org/abs/1806.01316>

Investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value.
So it is locally flat in most dimensions, but strongly distorted in others.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width

<https://www.arxiv.org/abs/2002.04010>

Use k-th Taylor expansion of the neurl network at initialization, and show that the approximation error decay exponentially over k in wide neural networks.

### The large learning rate phase of deep learning: the catapult mechanism

<https://www.arxiv.org/abs/2003.02218>

Present a class of neural networks with solvable training dynamics, and see two learning rate phase with their phenomena.

### SGD Learns the Conjugate Kernel Class of the Network

<https://www.arxiv.org/abs/1702.08503>

Show that SGD is guaranteed to learn a function that is competitive with the best function in the conjugate kernel space, in polynomial time.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
ins cuh networks.

### Learning Overparametrized Neural Networks via Stochastic Gradient Descent on Structured Data

<https://www.arxiv.org/abs/1808.01204>

Prove that when the data comes from mixtures of well-separated distributions SGD learns a two-layer overparameterized ReLU-network with a small generalization error, even though the network can fit arbitrary labels.

### Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

<https://www.arxiv.org/abs/1901.08584>

Analyze training and generalization, give (1) why random label gives slower training (2) generalization bound independent of network size (3) learnability of a broad class of smooth functions.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

### Products of Many Large Random Matrices and Gradients in Deep Neural Networks

<https://www.arxiv.org/abs/1812.05994>

Given products of random matrices when the number of terms and the size of the matrices tend to infinity, show that logarithm of L2 norm of such a product applied to any fixed vector is asymptotically Gaussian.

## Physics Involved

### Asymptotics of wide networks from feynman diagrams

<https://www.arxiv.org/abs/1909.11304>

Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory

<https://www.arxiv.org/abs/2008.08601>

Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.

### Finite size corrections for neural network Gaussian processes

<https://www.arxiv.org/abs/1908.10030>

Demonstrate that an ensemble of large finite FC network with a single hidden layer is well described by a Gaussian perturbed by the fourth Hermite polynomial, and the scale is 
inversely proportional to the number of units and that higher order terms decay more rapidly, recovering Edgeworth expansion.

### Non-Gaussian processes and neural networks at finite widths

<https://www.arxiv.org/abs/1910.00019>

Perturbatively extend NNGP correspondence to finite width neural network, yielding non-Gaussian processes as priors. This allows to track the flow of preactivation by
marginally integrating random variables, reminiscent of renormalization-group flow.
