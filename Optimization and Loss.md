### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

With the assumption of nonlinear conic approximation and unrealizable label vectors, show that a training problem with squared loss is necessarily unstable, i.e., its solution set depends discontinuously on the label vector in the training data. 

### Convex Geometry and Duality of Over-parameterized Neural Networks

<https://arxiv.org/abs/2002.11219>

Prove that an optimal solution to the regularized training problem can be characerized as extreme points of a convex set, so simple solutions are encouraged via its convex geometrical properties. 

### Spurious Local Minima are Common in Two-Layer ReLU Neural Networks

<https://arxiv.org/abs/1712.08968>

Show that two-layer ReLU networks w.r.t. the squared loss has local minima, even if the input distribution is standard Gaussian, dimension is arbitrarilty large, and orthonormal parameter vectors, using computer-assisted proof.

### Unveiling the structure of wide flat minima in neural networks

<https://arxiv.org/abs/2107.01163>

Show that wide flat minima arise as complex extensive structures from the coalscence of minima aroung 'high-margin' configurations. Despite being exponentially rare compared to zero-margin ones, high-margin minima tend to concentrate in particular regious, surrounded by other solutions of smaller margin.

### Entropic gradient descent algorithms and wide flat minima

<https://arxiv.org/abs/2006.07897>

Show that gaussian mixture classification's Bayes optimal pointwise estimators belongs to minimizers in wide flat regions, found by applying maximum flatness algorithms. Then using entropy-SGD and replicated-SGD, improve the generalization error.

### Escape saddle points by a simple gradient-descent based algorithm

<https://arxiv.org/abs/2111.14069>

Propose a simple gradient-based algorithm that outputs an epsilon approximate second-order stationary point, which is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points.

### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

Show that nonlinear conic approximation's effectivenss then linear approximation is reason of saddle point and spurious local minima, which cannot resolved using regularization. Illustrate that the improved approximation properties of neural networks and general nonlinear conic approximation instruments are linked to undesirable properties of the optimization problems.

### When Are Solutions Connected in Deep Networks?

<https://arxiv.org/abs/2102.09671>

Show that under generic assumptions on the features of intermediate layers, it suffices that the last two hiddne layers have order of sqrt(N) neurons, and if subsets of features at each layer are linearly separable, then no over-parameterization is needed to show the connectivity. 

### Convergence rates for the stochastic gradient descent method for non-convex opjective functions

<https://arxiv.org/abs/1904.01517>

Prove the local convergence to minima and estimates on the rate of convergence in the case of not necessarily globally convex nor contracting objective functions.

### Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions

<https://arxiv.org/abs/2112.07369>

Prove that under assumptions that the learning rates are sufficiently small but not L1 summable and target function is a constant function, the expectation of the risk converges to zero as step increases to infinity.

### On the existence of global minima and convergence analyses for gradient descent methods in the training of deep neural networks

<https://arxiv.org/abs/2112.09684>

Prove convergence of risk in gradient descent, when input data's probability distribution is piecewise polynomial, target function is also piecewise polynomial, and with at least one regular global minimum. Also show that there is global minimum for Lipscitz continuous function for shallow NN, and finally prove that gradient flow DE converges with polynomial rate.

### Taming neural networks with TUSLA: Non-convex learning via adaptive stochastic gradient Langevin algorithms

<https://arxiv.org/abs/2006.14514>

Use tamed unadjusted stocahstic Langevin algorithm to train NN, provide non-asymptotic analysis of convergence properties and finite-time guarantees for TUSLA to find approximate minimizer.

### On the Double Descent of Random Features Models Trained with SGD

<https://www.arxiv.org/abs/2110.06910>

Derive non-asymptotic error bound of random feature regression in high dimension with SGD training, show that due to unimodal variance and monotonic decrement of bias, there is double descent phenomenon.

### On generalization bounds for deep networks based on loss surface implicit regularization

<https://www.arxiv.org/abs/2201.04545>

Study how local geometry aroung local minima affects stochastic gradient descent with Gaussian noise, that the local geometry forces SGD to stay close to a low dimensional subspace giving implicit regularization, hence tighter bound for deep neural network. To derive generalization error bound, introduce notion of stagnation set around the local minima, then if stagnation occur, the generalization error is given by spectral norm of weights instead of number of parameters.

### The Implicit Regularization of Momentum Gradient Descent with Early Stopping

<https://www.arxiv.org/abs/2201.05405>

Study the implicit regularization of momentum gradient flow, show that its tendency is closer to ridge than gradient descent. Moreover prove that under t=sqrt(2/lambda) where lambda is tuning parameter of ridge regression, the risk of MGF is no more than 1.54 times that of ridge.

### On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport

<https://www.arxiv.org/abs/1805.09545>

Consider the discretization of unknown measure as mixture of particles, and a continuous time gradient descent on their weights and positions. Show that in this many-particle limit, the gradient flow converges to global minimizers involving Wasserstein gradient flow.

### On the Periodic Behavior of Neural Network Training with Batch Normalization with Weight Decay

<https://www.arxiv.org/abs/2106.15739>

Show that combined use of batch normalization and weight decay may result in periodic behavior of optimization behaviors. Derive the condition of this behavior both experimentally and theoretically.

### Stability of Deep Neural Networks via discrete rough paths

<https://www.arxiv.org/abs/2201.07506>

Based on stability bound of total p-variation of trained weights, interpret residual network as solutions to difference equations.

### Convergence of Deep Convolutional Neural Networks

<https://www.arxiv.org/abs/2109.13542>

Show that convergence of deep convolutional neural networks reduces to convergence of inifniite products of matrices with increasing sizes, and establish sufficient conditions for convergence of such infinite products.

### Improved Complexities for Stochastic Conditional Gradient Methods under Interpolation-like Conditions

<https://www.arxiv.org/abs/2006.08167>

Analyze stochastic conditional gradient method for constrained optimization problems, show that when the objective function is convex, it requires quadratic number of steps w.r.t. error.

### Power-law escape rate of SGD

<https://www.arxiv.org/abs/2105.09557>

Using the property of SGD noise to derive a SDE with simpler additive noise, show that the log loss barrier which is log ratio between local minimum loss and saddle loss, determines the escape rate of SGD from local minimum. 

### Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity

<https://www.arxiv.org/abs/2106.15933>

Consider training dynamics of Deep Linear Networks in low variance initialization, conjecture a saddle-to-saddle dynamics, gradient descent visits the neighborhoods of a sequence of saddles each corresponding to increasing rank, and reaches sparse global minimum. This conjecture is supported by a theorem for dynamics between first two saddles.

### How many degrees of freedom do we need to train deep networks: a loss landscape perspective

<https://www.arxiv.org/abs/2107.05802>

Find that there is a phase transition of dimensionality required to train the NN, which depends on initial loss and final loss. Theoretically explain the origin with its dependency, using Gordon's escape theorem that the training dimension plus Gaussian width of desired loss set must exceed the toal number of parameters to have large success probability.

### Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution

<https://www.arxiv.org/abs/2202.03524>

Using reformulation of optimization allowing for a new recursive algorithmic framework, prove convergence to epsilon global minimum with cubic time.

### Anticorrelated Noise Injection for Improved Generalization

<https://www.arxiv.org/abs/2202.02831>

Experimentally find that anticorrelated perturbation generalizes significantly better than GD and standard uncorrelated PGD, with theoretical analysis that Anti-PGD moves to wider minima where GD or PGD remains suboptimal regions.

### On Margin Maximization in Linear and ReLU Networks

<https://www.arxiv.org/abs/2110.02732>

It was shown that homogeneous networks trained with the exponential loss or the logistic loss's gradient flow converges to a KKT point of the max margin problem. This paper show that, the KKT point is not even a local optimum of max margin problem in many cases, and identify settings where local or global optimum can be guaranteed.

### Training neural networks using monotone variational inequality

<https://www.arxiv.org/abs/2202.08876>

Instead of traditional loss function, reduce training to another problem with convex structure, solving a monotone variational inequality. The solution can be founded by efficient procedure, with performance guarantee of l2 and l infty bound on model recovery accuracy and prediction accuracy with shallow linear neural networks. Also propose a practical algorithm called stochastic variational inequality, which gives competitive performance on SGD for FCNN and GNNs.

### Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization

<https://www.arxiv.org/abs/2103.01499>

Analyze Batch Normalization through the lens of convex optimization. Introduce an analytic framework based on convex duality, obtain exact convex representation of weight-decay regularized ReLU networks with BN, trainable in polytime. Show that optimal layer weights can be obtained as simple closed form formulas in high-dimensional overparameterized regimes. 

### Connecting Optimization and Generalization via Gradient Flow Path Length

<https://www.arxiv.org/abs/2202.10670>

Propose a framework to connect optimization with generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. Show that with proper initialization, gradient flow converges following a short path with explicit length estimate. Such estimate induces length-based generalization bound, showing that short path after convergence are associated with good generalization.

### Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time

<https://www.arxiv.org/abs/2107.10868>

Prove that Local SGD can optimize deep neural networks with ReLU activation function in polynomial time. Show that traditional approach using gradient Lipscitzness does not hold in ReLU nets, but the change between local model and average model will not change too much.

### Optimal Learning Rates of Deep Convolutional Neural Networks: Additive Ridge Functions

<https://www.arxiv.org/abs/2202.12119>

Show that for additive ridge functions, CNNs followed by one FC layer with ReLU activation can reach optimal mini-max rates.

### Benign Underfitting of Stochastic Gradient Descent

<https://www.arxiv.org/abs/2202.13361>

Prove that there exist problem instances where SGD solution exhibits both empirical risk and generalization gap of Omega(1), and show that SGD is not algorithmically stable, and its generalization ability can't be explained by uniform convergence or other known generalization bound techniques. 

### On the Power and Limitations of Random Features for Understanding Neural Networks

<https://www.arxiv.org/abs/1904.00687>

Review the techniques using random feature that the optimization dynamics behave as the initial random value, and argue that random features can't be used to learn even single neuron unless network size is exponentially large

### The loss landscape of deep linear neural networks: a second-order analysis

<https://www.arxiv.org/abs/2107.13289>

Study the optimization landscape of deep linear neural networks with the square loss. Characterize all critical points, which are global minimizers, strict saddle points, and non-strict saddle points, and all the associated critical values. 

### The Hidden Convex Optimization Landscape of Two-Layer ReLU Neural Networks: an Exact Characterization of the Optimal Solutions

<https://www.arxiv.org/abs/2006.05900>

Prove that all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization problem with cone constraint. Establish that the Clarke stationary points found by SGD correspond to global optimum of a subsampled convex problem, provide polynomial-time algorithm for checking whether network is at global minimum of training loss, provide an explicit construction of a continuous path between global minimum and any point, and characterize the minimal size of hidden layer so that loss landscape has no spurious valleys.

### Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum

<https://www.arxiv.org/abs/2203.11992>

Show that SGDm under covariate shift with fixed step size can be unstable and diverge, in particular, that SGDm under covariate shift is a parameteric oscilator, so can suffer from resonance. Approximate the learning system as a time-varying system of ODEs with applications of existing theory.

### Scaling Limit of Neural Networks with the Xavier Initialization and Convergence to a Global Minimum

<https://www.arxiv.org/abs/1907.04108>

Analyze the single-layer neural network with tha Xavier initialization in asymptotic regime of large number of hidden units and large numbers of SGD steps. The system can be viewed as stochastic system, analyzed with stochastic analysis, prove that neural network convergences in distribution to a random ODE with a Gaussian distribution, where normalization of Xavier initialization gives completely different result compared to mean-field limit. Due to the limit, optimization problem becomes convex and therefore converges to a global minimum.

### Convergence and Implicit Regularization Properties of Gradient Descent for Deep Residual Networks

<https://www.arxiv.org/abs/2204.07261>

Prove linear convergence of gradient descent to a global minimum, for the training of deep residual network with constant layer width and smooth activation function. Show that trained weight admits a scaling limit which is Hoelder continuous as the depth tends to infinity.

### Provable Convergence of Nesterov's Accelerated Gradient Method for Over-Parameterized Neural Networks

<https://www.arxiv.org/abs/2107.01832>

Analyze NAG in two-layer fully connected network with ReLU activation, show the convergence to global minimum at a non-asymptotic linear rate. Compared to convergence rate of GD, this shows NAG accelerates the training.

### On Feature Learning in Neural Networks with Global Convergence Guarantees

<https://www.arxiv.org/abs/2204.10782>

First show that gradient flow gives linear rate convergence to global minimum when input dimension is no less than the size of training set. Using this fact, show that training second to last layers with GF, prove a linear convergence of network. Also empirically show that unlike in the NTK regime, this model exhibits feature learning.

### Convergence of gradient descent for deep neural networks

<https://www.arxiv.org/abs/2203.16462>

Present a new criterion for convergence of gradient descent to a global minimum, which is provably more powerful than the best available criteria from the literature, the Lojasiewicz inequality.

### Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise

<https://www.arxiv.org/abs/2102.04297>

Show that the truncated SGD with heavy-tailed noise eliminate sharp local minima from training trajectory. First, the truncation threshold and the width of the attraction field dictate the order of the first exit time from the associated local minimum. Then, under conditions on loss function, as the learning rate decreases, the dynamics of heavy-tailed truncated SGD resemble the continuous-time Markov chain that never visits any sharp minima.

### Gradient Descent Optimizes Infinite-Depth ReLU Implicit Networks with Linear Widths

<https://www.arxiv.org/abs/2205.07463>

Studies the gradient descent of implicit neural network which has infinitely many layers. Study the convergence of both gradient flow and gradient descnet, and prove a global convergence at a linear rate widht linear widths.

### Topological properties of basins of attraction and expressiveness of width bounded neural networks

<https://www.arxiv.org/abs/2011.04923>

Consider the network with width not exceeding the input dimension, and prove that in this situation the basins of attraction are bounded and their complement cannot have bounded components. Also show that with more conditions, the basins are path-connected.

### Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks

<https://www.arxiv.org/abs/2205.13283>

Prove an embedding principle in depth, that loss landscape of an NN contains all critical points of the loss landscape of shallower NNs. Propose a critical lifting operator that lift the critical point of a shallow network to critical manifold of the target network, while preserving the outputs which can change the local minimum to strict saddle point. 

### Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks

<https://www.arxiv.org/abs/2105.05553>

Analyze the over-parameterized deep linear neural network, show that in wide enough hidden layers, the convergence rate of parameters is exponentially faster along the directions of the larger principal components of the data, nameing Principal Component bias. 

### Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent

<https://www.arxiv.org/abs/2002.04861>

Show that the training of two-layer ReLU network with gradient descent on a least-squares loss are not consistent, that they only finds a bad local minimum, since it is unable to move the biases far away from their initialization. And in these cases, the network essentially performs linear regression even if the target is nonlinear.

### On Gradient Descent Convergence beyond the Edge of Stability

<https://www.arxiv.org/abs/2206.04172>

Study a local condition for an unstable convergence where the step-size is larger than the admissibility threshold, and establish the global convergence of a two-layer single-neuron ReLU student network aligning with the teacher neuron in a large learning rate.

### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies

<https://www.arxiv.org/abs/1906.00425>

Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Understanding How Over-Parameterization Leads to Acceleration: A case of learning a single teacher neuron

<https://arxiv.org/abs/2010.01637>

In the setting with single teacher neuron with quadratic activation and over parametrization realized by having multiple student neurons, provably show that over-parameterization helps the gradient descent iteration enter the neighborhood of a global optimal solution.

### Towards Statistical and Computational Complexities of Polyak Step Size Gradient Descent

<https://arxiv.org/abs/2110.07810>

Demonstrate that the Polyak step size gradient descent iterates reach a final statistical radius of convergence around the true parameter after logarithmic number of iterations.

### Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs

<https://arxiv.org/abs/2206.00939>

Give precise description of the gradient flow dynamics of one-hidden-layer ReLU nets with the mean squared error, and show that it converges to zero loss with the implicit bias towards minimum variation norm.

### Feature Learning in L2-regularized DNNs: Attraction/Repulsion and Sparsity

<https://arxiv.org/abs/2205.15809>

Consider the loss surface of DNN with L2-regularization, and show that the loss in terms of the parameters can be reformulated in terms of layerwise activations. So each hidden representations are optimal w.r.t. attraction/repulsion problem and interpolate between the input and output representations, keeping as little information from the input as necessary.

### Support Vectors and Gradient Dynamics of Single-Neuron ReLU Networks

<https://www.arxiv.org/abs/2202.05510>

Examine the gradient flow dynamics in the parameter space when training snigle-neuron ReLU networks, and discover an implicit bias in terms of support vectors. Analyze this gradient flow w.r.t. the magnitude of the norm of initialization, and show that the norm of learned weights strictly increase. Finally prove the global convergence of single ReLU neuron with d=2.

### Convergence of Policy Gradient for Entropy Regularized MDPs with Neural Network Approximation in the Mean-Field Regime

<https://arxiv.org/abs/2201.07296>

Show that the softmax policy with shallow NN in a mean-field regime, with infinite-horizon, continuous state and action space, and entropy-regularized MDPs, the objective function increases along the gradient flow. Further, prove that is the regularization is sufficient, the gradient flow converges exponentially fast to the unique stationary solution.

### Neural Network Weights Do Not Converge to Stationary Points: An Invariant Measure Perspective

<https://arxiv.org/abs/2110.06256>

Find that the weight of NN do not converge to a stationary points even when the loss stabilizes. Propose a new perspective based on ergodic theory of dynamical system, and study the distribution of weight's dynamics, which converges to an approximate invariant measure.

### The Rate of Convergence of variation-Constrained Deep Neural Networks

<https://arxiv.org/abs/2106.12068>

Show that a class of variation-constrained neural network with any width, can achieve near-parameteric rate of convergence n^(-1/2+delta) for an arbitrarily small positive constant, showing that the function space need not to be large as believed.

### Bounding the Width of Neural Networks via Coupled Initialization -- A Worst Case Analysis

<https://arxiv.org/abs/2206.12802>

Show that by using same parameter twice for two-layer weight, show that the number of neuron required for convergence can be significantly decreased, for logistic loss and squared loss, implicitly also improving the running time bound also.

### Consistency of Neural Networks with Regularization

<https://www.arxiv.org/abs/2207.01538>

Show that the estimated neural network with regularization converge to true underlying function as the sample size increases, based on method of sieves and the theory on minimal neural networks.