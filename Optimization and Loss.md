### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

With the assumption of nonlinear conic approximation and unrealizable label vectors, show that a training problem with squared loss is necessarily unstable, i.e., its solution set depends discontinuously on the label vector in the training data. 

### Convex Geometry and Duality of Over-parameterized Neural Networks

<https://arxiv.org/abs/2002.11219>

Prove that an optimal solution to the regularized training problem can be characerized as extreme points of a convex set, so simple solutions are encouraged via its convex geometrical properties. 

### Spurious Local Minima are Common in Two-Layer ReLU Neural Networks

<https://arxiv.org/abs/1712.08968>

Show that two-layer ReLU networks w.r.t. the squared loss has local minima, even if the input distribution is standard Gaussian, dimension is arbitrarilty large, and orthonormal parameter vectors, using computer-assisted proof.

### Unveiling the structure of wide flat minima in neural networks

<https://arxiv.org/abs/2107.01163>

Show that wide flat minima arise as complex extensive structures from the coalscence of minima aroung 'high-margin' configurations. Despite being exponentially rare compared to zero-margin ones, high-margin minima tend to concentrate in particular regious, surrounded by other solutions of smaller margin.

### Entropic gradient descent algorithms and wide flat minima

<https://arxiv.org/abs/2006.07897>

Show that gaussian mixture classification's Bayes optimal pointwise estimators belongs to minimizers in wide flat regions, found by applying maximum flatness algorithms. Then using entropy-SGD and replicated-SGD, improve the generalization error.

### Escape saddle points by a simple gradient-descent based algorithm

<https://arxiv.org/abs/2111.14069>

Propose a simple gradient-based algorithm that outputs an epsilon approximate second-order stationary point, which is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points.

### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

Show that nonlinear conic approximation's effectivenss then linear approximation is reason of saddle point and spurious local minima, which cannot resolved using regularization. Illustrate that the improved approximation properties of neural networks and general nonlinear conic approximation instruments are linked to undesirable properties of the optimization problems.

### When Are Solutions Connected in Deep Networks?

<https://arxiv.org/abs/2102.09671>

Show that under generic assumptions on the features of intermediate layers, it suffices that the last two hiddne layers have order of sqrt(N) neurons, and if subsets of features at each layer are linearly separable, then no over-parameterization is needed to show the connectivity. 

### Convergence rates for the stochastic gradient descent method for non-convex opjective functions

<https://arxiv.org/abs/1904.01517>

Prove the local convergence to minima and estimates on the rate of convergence in the case of not necessarily globally convex nor contracting objective functions.

### Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions

<https://arxiv.org/abs/2112.07369>

Prove that under assumptions that the learning rates are sufficiently small but not L1 summable and target function is a constant function, the expectation of the risk converges to zero as step increases to infinity.

### On the existence of global minima and convergence analyses for gradient descent methods in the training of deep neural networks

<https://arxiv.org/abs/2112.09684>

Prove convergence of risk in gradient descent, when input data's probability distribution is piecewise polynomial, target function is also piecewise polynomial, and with at least one regular global minimum. Also show that there is global minimum for Lipscitz continuous function for shallow NN, and finally prove that gradient flow DE converges with polynomial rate.

### Taming neural networks with TUSLA: Non-convex learning via adaptive stochastic gradient Langevin algorithms

<https://arxiv.org/abs/2006.14514>

Use tamed unadjusted stocahstic Langevin algorithm to train NN, provide non-asymptotic analysis of convergence properties and finite-time guarantees for TUSLA to find approximate minimizer.

### On the Double Descent of Random Features Models Trained with SGD

<https://www.arxiv.org/abs/2110.06910>

Derive non-asymptotic error bound of random feature regression in high dimension with SGD training, show that due to unimodal variance and monotonic decrement of bias, there is double descent phenomenon.

### On generalization bounds for deep networks based on loss surface implicit regularization

<https://www.arxiv.org/abs/2201.04545>

Study how local geometry aroung local minima affects stochastic gradient descent with Gaussian noise, that the local geometry forces SGD to stay close to a low dimensional subspace giving implicit regularization, hence tighter bound for deep neural network. To derive generalization error bound, introduce notion of stagnation set around the local minima, then if stagnation occur, the generalization error is given by spectral norm of weights instead of number of parameters.

### The Implicit Regularization of Momentum Gradient Descent with Early Stopping

<https://www.arxiv.org/abs/2201.05405>

Study the implicit regularization of momentum gradient flow, show that its tendency is closer to ridge than gradient descent. Moreover prove that under t=sqrt(2/lambda) where lambda is tuning parameter of ridge regression, the risk of MGF is no more than 1.54 times that of ridge.

### On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport

<https://www.arxiv.org/abs/1805.09545>

Consider the discretization of unknown measure as mixture of particles, and a continuous time gradient descent on their weights and positions. Show that in this many-particle limit, the gradient flow converges to global minimizers involving Wasserstein gradient flow.

### On the Periodic Behavior of Neural Network Training with Batch Normalization with Weight Decay

<https://www.arxiv.org/abs/2106.15739>

Show that combined use of batch normalization and weight decay may result in periodic behavior of optimization behaviors. Derive the condition of this behavior both experimentally and theoretically.

### Stability of Deep Neural Networks via discrete rough paths

<https://www.arxiv.org/abs/2201.07506>

Based on stability bound of total p-variation of trained weights, interpret residual network as solutions to difference equations.

### Convergence of Deep Convolutional Neural Networks

<https://www.arxiv.org/abs/2109.13542>

Show that convergence of deep convolutional neural networks reduces to convergence of inifniite products of matrices with increasing sizes, and establish sufficient conditions for convergence of such infinite products.

### Improved Complexities for Stochastic Conditional Gradient Methods under Interpolation-like Conditions

<https://www.arxiv.org/abs/2006.08167>

Analyze stochastic conditional gradient method for constrained optimization problems, show that when the objective function is convex, it requires quadratic number of steps w.r.t. error.

### Power-law escape rate of SGD

<https://www.arxiv.org/abs/2105.09557>

Using the property of SGD noise to derive a SDE with simpler additive noise, show that the log loss barrier which is log ratio between local minimum loss and saddle loss, determines the escape rate of SGD from local minimum. 

### Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity

<https://www.arxiv.org/abs/2106.15933>

Consider training dynamics of Deep Linear Networks in low variance initialization, conjecture a saddle-to-saddle dynamics, gradient descent visits the neighborhoods of a sequence of saddles each corresponding to increasing rank, and reaches sparse global minimum. This conjecture is supported by a theorem for dynamics between first two saddles.

### How many degrees of freedom do we need to train deep networks: a loss landscape perspective

<https://www.arxiv.org/abs/2107.05802>

Find that there is a phase transition of dimensionality required to train the NN, which depends on initial loss and final loss. Theoretically explain the origin with its dependency, using Gordon's escape theorem that the training dimension plus Gaussian width of desired loss set must exceed the toal number of parameters to have large success probability.

### Finite-Sum Optimization: A New Perspective for Convergence to a Global Solution

<https://www.arxiv.org/abs/2202.03524>

Using reformulation of optimization allowing for a new recursive algorithmic framework, prove convergence to epsilon global minimum with cubic time.

### Anticorrelated Noise Injection for Improved Generalization

<https://www.arxiv.org/abs/2202.02831>

Experimentally find that anticorrelated perturbation generalizes significantly better than GD and standard uncorrelated PGD, with theoretical analysis that Anti-PGD moves to wider minima where GD or PGD remains suboptimal regions.

### On Margin Maximization in Linear and ReLU Networks

<https://www.arxiv.org/abs/2110.02732>

It was shown that homogeneous networks trained with the exponential loss or the logistic loss's gradient flow converges to a KKT point of the max margin problem. This paper show that, the KKT point is not even a local optimum of max margin problem in many cases, and identify settings where local or global optimum can be guaranteed.

### Training neural networks using monotone variational inequality

<https://www.arxiv.org/abs/2202.08876>

Instead of traditional loss function, reduce training to another problem with convex structure, solving a monotone variational inequality. The solution can be founded by efficient procedure, with performance guarantee of l2 and l infty bound on model recovery accuracy and prediction accuracy with shallow linear neural networks. Also propose a practical algorithm called stochastic variational inequality, which gives competitive performance on SGD for FCNN and GNNs.

### Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization

<https://www.arxiv.org/abs/2103.01499>

Analyze Batch Normalization through the lens of convex optimization. Introduce an analytic framework based on convex duality, obtain exact convex representation of weight-decay regularized ReLU networks with BN, trainable in polytime. Show that optimal layer weights can be obtained as simple closed form formulas in high-dimensional overparameterized regimes. 

### Connecting Optimization and Generalization via Gradient Flow Path Length

<https://www.arxiv.org/abs/2202.10670>

Propose a framework to connect optimization with generalization by analyzing the generalization error based on the length of optimization trajectory under the gradient flow algorithm after convergence. Show that with proper initialization, gradient flow converges following a short path with explicit length estimate. Such estimate induces length-based generalization bound, showing that short path after convergence are associated with good generalization.

### Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time

<https://www.arxiv.org/abs/2107.10868>

Prove that Local SGD can optimize deep neural networks with ReLU activation function in polynomial time. Show that traditional approach using gradient Lipscitzness does not hold in ReLU nets, but the change between local model and average model will not change too much.

### Optimal Learning Rates of Deep Convolutional Neural Networks: Additive Ridge Functions

<https://www.arxiv.org/abs/2202.12119>

Show that for additive ridge functions, CNNs followed by one FC layer with ReLU activation can reach optimal mini-max rates.

### Benign Underfitting of Stochastic Gradient Descent

<https://www.arxiv.org/abs/2202.13361>

Prove that there exist problem instances where SGD solution exhibits both empirical risk and generalization gap of Omega(1), and show that SGD is not algorithmically stable, and its generalization ability can't be explained by uniform convergence or other known generalization bound techniques. 

### On the Power and Limitations of Random Features for Understanding Neural Networks

<https://www.arxiv.org/abs/1904.00687>

Review the techniques using random feature that the optimization dynamics behave as the initial random value, and argue that random features can't be used to learn even single neuron unless network size is exponentially large

### The loss landscape of deep linear neural networks: a second-order analysis

<https://www.arxiv.org/abs/2107.13289>

Study the optimization landscape of deep linear neural networks with the square loss. Characterize all critical points, which are global minimizers, strict saddle points, and non-strict saddle points, and all the associated critical values. 

### The Hidden Convex Optimization Landscape of Two-Layer ReLU Neural Networks: an Exact Characterization of the Optimal Solutions

<https://www.arxiv.org/abs/2006.05900>

Prove that all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization problem with cone constraint. Establish that the Clarke stationary points found by SGD correspond to global optimum of a subsampled convex problem, provide polynomial-time algorithm for checking whether network is at global minimum of training loss, provide an explicit construction of a continuous path between global minimum and any point, and characterize the minimal size of hidden layer so that loss landscape has no spurious valleys.

### Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum

<https://www.arxiv.org/abs/2203.11992>

Show that SGDm under covariate shift with fixed step size can be unstable and diverge, in particular, that SGDm under covariate shift is a parameteric oscilator, so can suffer from resonance. Approximate the learning system as a time-varying system of ODEs with applications of existing theory.

### Scaling Limit of Neural Networks with the Xavier Initialization and Convergence to a Global Minimum

<https://www.arxiv.org/abs/1907.04108>

Analyze the single-layer neural network with tha Xavier initialization in asymptotic regime of large number of hidden units and large numbers of SGD steps. The system can be viewed as stochastic system, analyzed with stochastic analysis, prove that neural network convergences in distribution to a random ODE with a Gaussian distribution, where normalization of Xavier initialization gives completely different result compared to mean-field limit. Due to the limit, optimization problem becomes convex and therefore converges to a global minimum.

### Convergence and Implicit Regularization Properties of Gradient Descent for Deep Residual Networks

<https://www.arxiv.org/abs/2204.07261>

Prove linear convergence of gradient descent to a global minimum, for the training of deep residual network with constant layer width and smooth activation function. Show that trained weight admits a scaling limit which is Hoelder continuous as the depth tends to infinity.

### Provable Convergence of Nesterov's Accelerated Gradient Method for Over-Parameterized Neural Networks

<https://www.arxiv.org/abs/2107.01832>

Analyze NAG in two-layer fully connected network with ReLU activation, show the convergence to global minimum at a non-asymptotic linear rate. Compared to convergence rate of GD, this shows NAG accelerates the training.

### On Feature Learning in Neural Networks with Global Convergence Guarantees

<https://www.arxiv.org/abs/2204.10782>

First show that gradient flow gives linear rate convergence to global minimum when input dimension is no less than the size of training set. Using this fact, show that training second to last layers with GF, prove a linear convergence of network. Also empirically show that unlike in the NTK regime, this model exhibits feature learning.

### Convergence of gradient descent for deep neural networks

<https://www.arxiv.org/abs/2203.16462>

Present a new criterion for convergence of gradient descent to a global minimum, which is provably more powerful than the best available criteria from the literature, the Lojasiewicz inequality.