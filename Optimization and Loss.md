### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

With the assumption of nonlinear conic approximation and unrealizable label vectors, show that a training problem with squared loss is necessarily unstable, i.e., its solution set depends discontinuously on the label vector in the training data. 

### Convex Geometry and Duality of Over-parameterized Neural Networks

<https://arxiv.org/abs/2002.11219>

Prove that an optimal solution to the regularized training problem can be characerized as extreme points of a convex set, so simple solutions are encouraged via its convex geometrical properties. 

### Spurious Local Minima are Common in Two-Layer ReLU Neural Networks

<https://arxiv.org/abs/1712.08968>

Show that two-layer ReLU networks w.r.t. the squared loss has local minima, even if the input distribution is standard Gaussian, dimension is arbitrarilty large, and orthonormal parameter vectors, using computer-assisted proof.

### Unveiling the structure of wide flat minima in neural networks

<https://arxiv.org/abs/2107.01163>

Show that wide flat minima arise as complex extensive structures from the coalscence of minima aroung 'high-margin' configurations. Despite being exponentially rare compared to zero-margin ones, high-margin minima tend to concentrate in particular regious, surrounded by other solutions of smaller margin.

### Entropic gradient descent algorithms and wide flat minima

<https://arxiv.org/abs/2006.07897>

Show that gaussian mixture classification's Bayes optimal pointwise estimators belongs to minimizers in wide flat regions, found by applying maximum flatness algorithms. Then using entropy-SGD and replicated-SGD, improve the generalization error.

### Escape saddle points by a simple gradient-descent based algorithm

<https://arxiv.org/abs/2111.14069>

Propose a simple gradient-based algorithm that outputs an epsilon approximate second-order stationary point, which is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points.

### On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes

<https://arxiv.org/abs/2011.03293>

Show that nonlinear conic approximation's effectivenss then linear approximation is reason of saddle point and spurious local minima, which cannot resolved using regularization. Illustrate that the improved approximation properties of neural networks and general nonlinear conic approximation instruments are linked to undesirable properties of the optimization problems.

### When Are Solutions Connected in Deep Networks?

<https://arxiv.org/abs/2102.09671>

Show that under generic assumptions on the features of intermediate layers, it suffices that the last two hiddne layers have order of sqrt(N) neurons, and if subsets of features at each layer are linearly separable, then no over-parameterization is needed to show the connectivity. 

### Convergence rates for the stochastic gradient descent method for non-convex opjective functions

<https://arxiv.org/abs/1904.01517>

Prove the local convergence to minima and estimates on the rate of convergence in the case of not necessarily globally convex nor contracting objective functions.

### Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions

<https://arxiv.org/abs/2112.07369>

Prove that under assumptions that the learning rates are sufficiently small but not L1 summable and target function is a constant function, the expectation of the risk converges to zero as step increases to infinity.

### On the existence of global minima and convergence analyses for gradient descent methods in the training of deep neural networks

<https://arxiv.org/abs/2112.09684>

Prove convergence of risk in gradient descent, when input data's probability distribution is piecewise polynomial, target function is also piecewise polynomial, and with at least one regular global minimum. Also show that there is global minimum for Lipscitz continuous function for shallow NN, and finally prove that gradient flow DE converges with polynomial rate.

### Taming neural networks with TUSLA: Non-convex learning via adaptive stochastic gradient Langevin algorithms

<https://arxiv.org/abs/2006.14514>

Use tamed unadjusted stocahstic Langevin algorithm to train NN, provide non-asymptotic analysis of convergence properties and finite-time guarantees for TUSLA to find approximate minimizer.