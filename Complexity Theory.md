### Reachability is NP-Complete Even for the Simplest Neural Networks

<https://www.arxiv.org/abs/2108.13179>

Investigate the complexity of the reachability problem for neural networks, showing that NP-hardness holds for even neural networks with just one layer and simple specifications.

### The Complexity of Gradient Descent: CLS = PPAD intersection PLS

<https://www.arxiv.org/abs/2011.01929>

Show that search problems that can be solved by performing gradient descent on a bounded convex polytopal domain is equal to intersection of PPAD and PLS. Also show that computing a KKT point of a continuously differentiable function over the domain [0,1]^2 is complete. 

### Training Neural Networks is Exists R-complete

<https://www.arxiv.org/abs/2102.09798>

Show that complexity of finding weight with total error below the threshold is exists R-complete, which is a decision problem whether a system of polynomial equations and inequalities with integer coefficients and real unknowns has a solution, and strictly larger than NP.

### Training Fully Connected Neural Networks is exists R-complete

<https://www.arxiv.org/abs/2204.01368>

Show that empirical risk minimization is exists R-complete, even with two output neurons, two input neuron, only 13 different labels, hidden neuron is a constant fraction of the number of data point, and ReLU activation. Generalize the previous work.

### Learning (Very) Simple Generative Models Is Hard

<https://www.arxiv.org/abs/2205.16003>

For the distribution given by pushing standard Gaussian with unknown distribution, show that no polynomial-time algorithm can solve this learning problem.

### Recurrent Convolutional Neural Networks Learn Succinct Learning Algorithms

<https://www.arxiv.org/abs/2209.00735>

Exhibit a NN architecture that can learn any learning algorithm that is written in constant-size program, in polynomial time. 

### Fast Attention Requires Bounded Entries

<https://www.arxiv.org/abs/2302.13214>

Show that the approximation of attention weight matrix depends on the bound of the key, query, and values, where for bound o(sqrt(log n)) there is n^(1+o(1)) time algorithm that approximates attention matrix, where larger than this value makes impossible to approximate the attention matrix in subquadratic time, given strong exponential time hypothesis.