### Approximation Properties of Deep ReLU CNNs

<https://www.arxiv.org/abs/2109.00190>

Analyzes the L2 approximation properties of deep ReLU convolutional neural networks on two-dimensional space. Using the decomposition of convolutional kernels, show the universal approximation.

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Provide a lower bound on the approximation rates for shallow neural networks, which are obtained by lower bounding the L2 metric entropy of the convex hull of the neural network basis functions. 

### Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on Hoelder Class

<https://www.arxiv.org/abs/2103.00542>

For general continuous f on d dimensional box with continuity modulus, construct networks with sufficient approximation rate. This requires d^3/2 width, showing that this networks overcome the curse of dimensionality on Holder functions.

### Arbitrary-Depth Universal Approximation Theorems for Operator Neural Newtorks

<https://www.arxiv.org/abs/2109.11354>

Prove that operator NNs of bounded width and arbitrary depth are universal approximators for continuous nonlinear operators. 

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Prove sharp lower bounds on the approximation rates for shallow neural networks, obtained by lower bounding L2 metric entropy of the convex hull of basis functions. 

### Deep Network Approximation for Smooth Functions

<https://www.arxiv.org/abs/2001.03040>

Prove that multivariate polynomials can be approximated by deep ReLU networks of with small enough approximation error, then through local Taylor expansions, show that deep ReLU networks can approximate C^s functions.

### A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Komogorov partial differential equations with constant diffusion and nonlinear drift coefficients

<https://www.arxiv.org/abs/1809.07321>

Prove that the number of parameters used to describe the employed DNN grows at most polynomially in both the PDE dimension d and the reciprocal of the prescribed approximation accuracy.

### Uniform error estimates for artificial neural network approximations for heat equations

<https://www.arxiv.org/abs/1911.09647>

Develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs in the uniform L infty sense. Prove that the number of parameters of an ANN to uniformly approximate the classical solution of the heat equation in a region \[a,b\]^d for a fixed time point T grows at most polynomially in the dimension d and the reciprocal of the approximation precision. 

### Approximation capabilities of neural networks on unbounded domains

<https://www.arxiv.org/abs/1910.09293>

Prove that a shallow neural network with some activation functions can arbitrarily well approximate any Lp integrable functions defined on R * \[0,1\]^n, and moreover integrable function on the Euclidean plane.

### Approximation properties of Residual Neural Networks for Kolmogorov PDEs

<https://www.arxiv.org/abs/2111.00215>

Show that ResNets are able to approximate solutions of Kolmogorov PDEs with constant diffusion and possibly nonlinear drift coefficients without suffering the curse of dimensionality.

### Deep Learning in High Dimension: Neural Network Approximation of Analytic Functions in L2(R^d, gamma_d)

<https://www.arxiv.org/abs/2111.07080>

Prove expression rates for analytic function, L2 in the gaussian product measure, show the exponential convergence rate. The rate only depend on quantified holomorphy of F, to a product of strips in C^d. 

### Tighter Sparse Approximation Bounds for ReLU Neural Networks

<https://www.arxiv.org/abs/2110.03673>

Extend the previous work using Radon transform, define Radon-based semi-norms, that function admits an infinite-width neural network representation on a bounded open set when its norm is finite. Derive sparse finite width neural network approximation bounds, and show that infinite width representations are not unique.

### Optimal Approximation Rate of ReLU Netowrks in terms of Width and Depth

<https://www.arxiv.org/abs/2103.00502>

Construct ReLU networks approximating Hoelder continuous function on \[0,1\]^d, which is optimal up to constant.

### Optimal learning of high-dimensional classification problems using deep neural networks

<https://www.arxiv.org/abs/2112.12555>

Establish universal lower bounds for classification under regularity of decision boundary, and show that for locally Barron-regular decision boundary, the optimal estimation rate are independent of underlying dimension and can be realized by empirical risk minimization.

### A Unified and Constructive Framework for the Universality of Neural Networks

<https://www.arxiv.org/abs/2112.14877>

Design a framework for proving universality of activation, using it show various activations including Mish, SiLU, ELU, GELU and some new activation's universality.

### Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces

<https://www.arxiv.org/abs/2201.00217>

For nonparametric estimation of Lipscitz operators with deep neural networks, prove non-asymptotic upper bound for generalization error. With the assumption that the target operator exhibits a low dimensional structure, the error bound decays as sample size increases, where rate is determined by intrinsic dimension.

### Approximation capabilities of measure-preserving neural networks

<https://www.arxiv.org/abs/2106.10911>

For measure preserving neural networks like NICE and RevNets, if there is compact set in R^n > 1, it is able to approximate any measure preserving map which is bounded and injective in the Lp norm.

### A three layer neural network can represent any multivariate function

<https://www.arxiv.org/abs/2012.03016>

Prove that not only continuous functions, but also all discontinuous functions can be implemented by three-layer networks.

### Analysis of the Gradient Descent Algorithm for a Deep Neural Network Model with Skip-connections

<https://www.arxiv.org/abs/1904.05263>

Prove that GD can find global minimum exponentially fast, with generalization error established. If the target function is contained in RKHS induced by activation and initialization distribution, there exist generalizable early stopping solution in GD path.

### Advantages of Deep Neural Networks for Estimating Functions with Singularity on Hypersurfaces

<https://www.arxiv.org/abs/2011.02256>

Derive the generalization error of a DNN with optimal convergence rate on estimation on non smooth functions with singularities on hypersurfaces. Show that there is a phase where DNNs outperform other standard methods.

### Width is Less Important than Depth in ReLU Neural Networks

<https://www.arxiv.org/abs/2202.03841>

Show that any target network with inputs in d dimension can be approximated by a width O(d) network, with linearly over parameterized. 

### Approximation error of single hidden layer neural networks with fixed weights

<https://www.arxiv.org/abs/2202.03289>

Provides an explicit formula fo the approximation error of single hidden layer neural networks with two fixed weights.

### Approximation of Lipscitz Functions using Deep Spline Neural Networks

<https://www.arxiv.org/abs/2204.06233>

For Lipscitz contrained network, ReLU networks have provable disadvantage. Propose learnable spline activation function, and show that this choise is optimal among all component-wise 1-Lipscitz activation functions, in the sense that no other weight constrained architectures can approximate a larger class of functions.

### Two-layer neural networks with values in a Banach space

<https://www.arxiv.org/abs/2105.02095>

Study two-layer neural network with both domain and ranges are Banach spaces with separable preduals. For the lattice operation of taking positive part, prove the inverse and direct approximation theorems with Monte-Carlo rates. 

### Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks

<https://www.arxiv.org/abs/2109.08844>

Consider the problem of learning the function in second-order Radon-domain bounded variation space, and derive a minimax lower bound for the estimation problem for this function space, and show that the NN estimators are minimax optimal up to logarithmic factors, which do not depend on the curse of dimensionality.

### On the capacity of deep generative networks for approximating distributions

<https://www.arxiv.org/abs/2101.12353>

Prove that neural networks can transform a low-dimensional source distribution to a distribution that is arbitrarily close to a high-dimensional target distribution, when the closeness are measured by Wasserstein distances and maximum mean discrepancy. 

### Convolutional Rectifier Networks as Generalized Tensor Decompositions

<https://www.arxiv.org/abs/1603.00162>

Describe a construction based on generalized tensor decompositions that transforms convolutional arithmetic circuits into convolutional rectifier networks, then use tools from the world of arithmetic circuits. Show that convolutional rectifier networks are universal with max pooling but not with average pooling. Also show that depth efficiency is weaker with convolutional rectifier networks than convolutional arithmetic circuits.

### Deep neural networks can stably solve high-dimensional, noisy, non-linear inverse problems

<https://www.arxiv.org/abs/2206.00934>

Show that there exists a neural network that is robust to noise approximation of the finite-dimensional operator with Lipscitz-continuous inverse.

### Approximation in shift-invariant spaces with deep ReLU neural networks

<https://www.arxiv.org/abs/2005.11949>

Study the expressive power of deep ReLU NNs for approximating functions in dilated shift-invariant spaces, with approximate error bounds w.r.t. width and depth, for the Sobolev spaces and Besov spaces. Also give lower bounds of the Lp approximation error for Sobolev spaces.

### ReLU Deep Neural Networks from the Hierarchical Basis Perspective

<https://www.arxiv.org/abs/2105.04156>

Show that the approximation scheme of ReLU DNN for x^2 and xy are composition versions of the hierarchical basis approximation in finite element methods, which gives interpretation and proof of approximation of polynomial.

### Nonparametric regression with modified ReLU networks

<https://www.arxiv.org/abs/2207.08306>

Considering the modified ReLU-net which sparsifies the network parameters, show that these class of networks with regularization attain the minimax rate of prediction of unknown beta-smooth functions.

### Sharp Bounds on the Approximation Rates, Metric Entropy, and n-widths of Shallow Neural Networks

<https://www.arxiv.org/abs/2101.12365>

Gives an upper bound for non-linear approximation rates, metric entropy, and n-widths of their absolute convex hull, for smoothly parameterized dictionary. Then apply these result for dictionaries of ridge functions that correspond to shallow neural networks. Also provides the lower bound of the metric entropy and n-widths, which gives sharp lower bound on approximation rate for the ReLU^k activations and sigmoidal acitvation, with bounded variatiation.

### Neural Networks with linear threshold activations: structure and algorithms

<https://www.arxiv.org/abs/2111.08117>

Characterize the class of functions that are representable by linear threshold activation functions, and show that 2 hidden layer is necessary and sufficient. Also give precise bound on the sizes of neural networks to represent any function in the class.

### Do ReLU Networks Have An Edge When Approximating Compactly-Supported Functions?

<https://www.arxiv.org/abs/2204.11231>

Construct the refinement of the usual topology on locally integrable function spaces, where compactly-supported functions can only be approximated in L1-norm by functions with matchine discretized support. Then establish the universality of ReLU-NN with bilinear pooling in this topology.

### Why Do Networks Need Negative Weights?

<https://www.arxiv.org/abs/2208.03211>

Prove that deep neural netwroks with all non-negative weights are not universal approximators.

### Deep ReLU neural networks overcome the curse of dimensionality for partil integrodifferential equations

<https://www.arxiv.org/abs/2102.11707>

Show that the DNNs with ReLU activation functions are able to express viscosity solutions of linear PIDEs, which arises from a class of jump diffusions.

### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

<https://www.arxiv.org/abs/1610.09887>

Prove that various types of simple and natural functions, including indicators of balls and ellipses, non-linear radial functions, smooth non-linear functions, can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger.

### Towards Lower Bounds on the Depth of ReLU Neural Networks

<https://www.arxiv.org/abs/2105.14835>

Using techniques from mixed-integer optimization, polyhedral theory, tropical geometry, provide a counterbalance to the universal approximation theorem which suggest that a single hidden layer is sufficient for learning tasks. Inverstigate whether the class of exactly representablew functions strictly increases by adding more layers. Also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes.

### Shallow neural network represents of polynomials

<https://www.arxiv.org/abs/2208.08138>

Show that d-variate polynomials of degree R on interval can be represented as shallow neural networks with fixed width, and use this approximation to minimax optimal rate of convergence of unknown univariate regression function.

### Neural Network Approximation of Lipschitz Functions in High Dimensions with Applications to Inverse Problems

<https://www.arxiv.org/abs/2208.13305>

Assuming the existence of a linear Johnson-Lindenstrauss embedding of a high dimensional set to low dimensional cube, the Lipschitz function from high dimension can be represented by Lipschitz function from low dimension. Using this, show that the neural network can approximate a function with high dimensional input without exponential scale, using JL-embedding as first layer.

### Solving parametric partial differential equations with deep rectified quadratic unit neural networks

<https://www.arxiv.org/abs/2203.06973>

Derive an upper bound on the size of deep ReQU network for learning parameteric PDE, while taking full advantage of the inherent low-dimensionality of the solution manifolds. This upper bound is lower than the lower bound of ReLU network's complexity-bound.

### Extending the Universal Approximation Theorem for a Broad Class of Hypercomplex-Valued Neural Networks

<https://www.arxiv.org/abs/2203.02456>

Introduce the concept of non-degenerate hypercomplex algebra which includes complex numbers, quaternions, and tessariances, and state the universal approximation theorem for the hypercomplex-valued neural networks.

### From Monte Carlo to neural networks approximations of boundary value problems

<https://www.arxiv.org/abs/2209.01432>

First show that the solution to Poisson equation can be numerically approximated by Monte Carlo methods which slightly change the walk on the spheres algorithm, which is efficient w.r.t. approximation error without the curse of dimensionality. Then show that this Monte Carlo solver renders ReLU DNN solutions to Poisson problem, showing that the random DNN provides a small approximation error and low polynomial complexity in the dimension.

### Analytic function approximation by path norm regularized deep networks

<https://www.arxiv.org/abs/2104.02095>

Show that neural networks with absolute value activation function and the path norm, depth, width having logarithmic dependence on 1/eps can eps-approximate functions that are analytic on certain regions of complex numbers.

### Optimal bump functions for shallow ReLU networks: Weight decay, depth separation and the curse of dimensionality

<https://www.arxiv.org/abs/2209.01173>

Consider the data from radially symmetric distribution with target label 1 at the origin, 0 outside the unit ball, otherwise unknown. With weight decay regularization and in infinite neuron, infinite data limit, prove that a unique radially symmetric minimizer exists, with decay regularizer grow as input dimension. Show that the regularizer decrease exponentially in input dimension if target label is on smaller ball rather than origin. And show that NN with two hidden layers can approximate the target function without this curse of dimensionality.

### Achieve the Minimum Width of Neural Networks for Universal Approximation

<https://www.arxiv.org/abs/2209.11395>

Consider neural networks with an arbitrary set of activation functions, show that both continuous and Lp UAP on compact domain share a universal lower bound of max(dx, dy). The proof is based on the approximation power of neural ODE and ability to approximate it by neural network.

### Transformers Implement First-Order Logic with Majority Quantifiers

<https://www.arxiv.org/abs/2210.02671>

Show that any transformer neural network can be translated into an equivlaent fixed-size first-order logic formula which may also use majority quantifiers.

### Deep neural network expressivity for optimal stopping problems

<https://www.arxiv.org/abs/2210.10443>

Show that the value function and continuation value of an optimal stopping time of high dimensional discrete time Markov process can be approximated with error at most epsilon, with polynomial on dimension.

### A new activation for neural networks and its approximation

<https://www.arxiv.org/abs/2210.10264>

Propose new activation named 'DLU', and show that it has competitive performance for approximation to ReLU and rational network, while having several advantages.

### Simultaneous approximation of a smooth function and its derivatives by deep neural networks with piecewise-polynomial activations

<https://www.arxiv.org/abs/2206.09527>

Derive the required depth, width, and sparsity of the deep neural network to approximate any Hoelder smooth function in Hoelder norm, when all weights are bounded by 1.

### Limitations on approximation by deep and shallow neural networks

<https://www.arxiv.org/abs/2212.02223>

Prove the Carl's type ineuqalities for the error of approximation of compact sets by deep and shallow neural networks, which gives lower bound on how well we can approximate the functions in K.

### A general approximation lower bound in Lp norm, with applications to feed-forward neural networks

<https://www.arxiv.org/abs/2206.04360>

Show that the approximation of space F by space G in Lp norm can be described with the packing number and range of F, and fat-shattering dimension of G. Using this result, show the lower bound for special function classes like Hoelder balls or multivariate monotonic functions that matches with upper bound up to log factors.

### Your Transformer May Not be as Powerful as You Expect

<https://www.arxiv.org/abs/2205.13401>

Show that the general relative positional encoding based Transformer is not universal approximator as absolute positional encoding, because the softmax attention always generate the right stochastic matrix. Derive sufficient condition for universal approximation, and derive URPE attention that satisfy this condition which multiplies single Toeplitz matrix.

### Exponential ReLU Neural Network Approximation Rates for Point and Edge Singularities

<https://www.arxiv.org/abs/2010.12217>

Prove the exponential expressivity with ReLU NN in H1 function space, that is locally analytic but admits point singularities and edge singularities, for dimension 2 and 3. This imply the exponential expressivity for the elliptic boundary and eigenvalue problems.