### Approximation Properties of Deep ReLU CNNs

<https://arxiv.org/abs/2109.00190>

Analyzes the L2 approximation properties of deep ReLU convolutional neural networks on two-dimensional space. Using the decomposition of convolutional kernels, show the universal approximation.

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Provide a lower bound on the approximation rates for shallow neural networks, which are obtained by lower bounding the L2 metric entropy of the convex hull of the neural network basis functions. 

### Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on Hoelder Class

<https://www.arxiv.org/abs/2103.00542>

For general continuous f on d dimensional box with continuity modulus, construct networks with sufficient approximation rate. This requires d^3/2 width, showing that this networks overcome the curse of dimensionality on Holder functions.

### Arbitrary-Depth Universal Approximation Theorems for Operator Neural Newtorks

<https://www.arxiv.org/abs/2109.11354>

Prove that operator NNs of bounded width and arbitrary depth are universal approximators for continuous nonlinear operators. 

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://arxiv.org/abs/2106.14997>

Prove sharp lower bounds on the approximation rates for shallow neural networks, obtained by lower bounding L2 metric entropy of the convex hull of basis functions. 

### Deep Network Approximation for Smooth Functions

<https://arxiv.org/abs/2001.03040>

Prove that multivariate polynomials can be approximated by deep ReLU networks of with small enough approximation error, then through local Taylor expansions, show that deep ReLU networks can approximate C^s functions.

### A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Komogorov partial differential equations with constant diffusion and nonlinear drift coefficients

<https://arxiv.org/abs/1809.07321>

Prove that the number of parameters used to describe the employed DNN grows at most polynomially in both the PDE dimension d and the reciprocal of the prescribed approximation accuracy.

### Uniform error estimates for artificial neural network approximations for heat equations

<https://arxiv.org/abs/1911.09647>

Develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs in the uniform L infty sense. Prove that the number of parameters of an ANN to uniformly approximate the classical solution of the heat equation in a region \[a,b\]^d for a fixed time point T grows at most polynomially in the dimension d and the reciprocal of the approximation precision. 

### Approximation capabilities of neural networks on unbounded domains

<https://arxiv.org/abs/1910.09293>

Prove that a shallow neural network with some activation functions can arbitrarily well approximate any Lp integrable functions defined on R * \[0,1\]^n, and moreover integrable function on the Euclidean plane.

### Approximation properties of Residual Neural Networks for Kolmogorov PDEs

<https://arxiv.org/abs/2111.00215>

Show that ResNets are able to approximate solutions of Kolmogorov PDEs with constant diffusion and possibly nonlinear drift coefficients without suffering the curse of dimensionality.

### Deep Learning in High Dimension: Neural Network Approximation of Analytic Functions in L2(R^d, gamma_d)

<https://arxiv.org/abs/2111.07080>

Prove expression rates for analytic function, L2 in the gaussian product measure, show the exponential convergence rate. The rate only depend on quantified holomorphy of F, to a product of strips in C^d. 

### Tighter Sparse Approximation Bounds for ReLU Neural Networks

<https://arxiv.org/abs/2110.03673>

Extend the previous work using Radon transform, define Radon-based semi-norms, that function admits an infinite-width neural network representation on a bounded open set when its norm is finite. Derive sparse finite width neural network approximation bounds, and show that infinite width representations are not unique.

### Optimal Approximation Rate of ReLU Netowrks in terms of Width and Depth

<https://arxiv.org/abs/2103.00502>

Construct ReLU networks approximating Hoelder continuous function on \[0,1\]^d, which is optimal up to constant.

### Optimal learning of high-dimensional classification problems using deep neural networks

<https://arxiv.org/abs/2112.12555>

Establish universal lower bounds for classification under regularity of decision boundary, and show that for locally Barron-regular decision boundary, the optimal estimation rate are independent of underlying dimension and can be realized by empirical risk minimization.

### A Unified and Constructive Framework for the Universality of Neural Networks

<https://www.arxiv.org/abs/2112.14877>

Design a framework for proving universality of activation, using it show various activations including Mish, SiLU, ELU, GELU and some new activation's universality.

### Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces

<https://www.arxiv.org/abs/2201.00217>

For nonparametric estimation of Lipscitz operators with deep neural networks, prove non-asymptotic upper bound for generalization error. With the assumption that the target operator exhibits a low dimensional structure, the error bound decays as sample size increases, where rate is determined by intrinsic dimension.

### Approximation capabilities of measure-preserving neural networks

<https://www.arxiv.org/abs/2106.10911>

For measure preserving neural networks like NICE and RevNets, if there is compact set in R^n > 1, it is able to approximate any measure preserving map which is bounded and injective in the Lp norm.

### A three layer neural network can represent any multivariate function

<https://www.arxiv.org/abs/2012.03016>

Prove that not only continuous functions, but also all discontinuous functions can be implemented by three-layer networks.

### Analysis of the Gradient Descent Algorithm for a Deep Neural Network Model with Skip-connections

<https://www.arxiv.org/abs/1904.05263>

Prove that GD can find global minimum exponentially fast, with generalization error established. If the target function is contained in RKHS induced by activation and initialization distribution, there exist generalizable early stopping solution in GD path.

### Advantages of Deep Neural Networks for Estimating Functions with Singularity on Hypersurfaces

<https://www.arxiv.org/abs/2011.02256>

Derive the generalization error of a DNN with optimal convergence rate on estimation on non smooth functions with singularities on hypersurfaces. Show that there is a phase where DNNs outperform other standard methods.

### Width is Less Important than Depth in ReLU Neural Networks

<https://www.arxiv.org/abs/2202.03841>

Show that any target network with inputs in d dimension can be approximated by a width O(d) network, with linearly over parameterized. 

### Approximation error of single hidden layer neural networks with fixed weights

<https://www.arxiv.org/abs/2202.03289>

Provides an explicit formula fo the approximation error of single hidden layer neural networks with two fixed weights.

### Approximation of Lipscitz Functions using Deep Spline Neural Networks

<https://www.arxiv.org/abs/2204.06233>

For Lipscitz contrained network, ReLU networks have provable disadvantage. Propose learnable spline activation function, and show that this choise is optimal among all component-wise 1-Lipscitz activation functions, in the sense that no other weight constrained architectures can approximate a larger class of functions.

### Two-layer neural networks with values in a Banach space

<https://www.arxiv.org/abs/2105.02095>

Study two-layer neural network with both domain and ranges are Banach spaces with separable preduals. For the lattice operation of taking positive part, prove the inverse and direct approximation theorems with Monte-Carlo rates. 

### Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks

<https://www.arxiv.org/abs/2109.08844>

Consider the problem of learning the function in second-order Radon-domain bounded variation space, and derive a minimax lower bound for the estimation problem for this function space, and show that the NN estimators are minimax optimal up to logarithmic factors, which do not depend on the curse of dimensionality.

### On the capacity of deep generative networks for approximating distributions

<https://arxiv.org/abs/2101.12353>

Prove that neural networks can transform a low-dimensional source distribution to a distribution that is arbitrarily close to a high-dimensional target distribution, when the closeness are measured by Wasserstein distances and maximum mean discrepancy. 

### Convolutional Rectifier Networks as Generalized Tensor Decompositions

<https://www.arxiv.org/abs/1603.00162>

Describe a construction based on generalized tensor decompositions that transforms convolutional arithmetic circuits into convolutional rectifier networks, then use tools from the world of arithmetic circuits. Show that convolutional rectifier networks are universal with max pooling but not with average pooling. Also show that depth efficiency is weaker with convolutional rectifier networks than convolutional arithmetic circuits.

### Deep neural networks can stably solve high-dimensional, noisy, non-linear inverse problems

<https://arxiv.org/abs/2206.00934>

Show that there exists a neural network that is robust to noise approximation of the finite-dimensional operator with Lipscitz-continuous inverse.

### Approximation in shift-invariant spaces with deep ReLU neural networks

<https://arxiv.org/abs/2005.11949>

Study the expressive power of deep ReLU NNs for approximating functions in dilated shift-invariant spaces, with approximate error bounds w.r.t. width and depth, for the Sobolev spaces and Besov spaces. Also give lower bounds of the Lp approximation error for Sobolev spaces.

### ReLU Deep Neural Networks from the Hierarchical Basis Perspective

<https://www.arxiv.org/abs/2105.04156>

Show that the approximation scheme of ReLU DNN for x^2 and xy are composition versions of the hierarchical basis approximation in finite element methods, which gives interpretation and proof of approximation of polynomial.

### Nonparametric regression with modified ReLU networks

<https://www.arxiv.org/abs/2207.08306>

Considering the modified ReLU-net which sparsifies the network parameters, show that these class of networks with regularization attain the minimax rate of prediction of unknown beta-smooth functions.

### Sharp Bounds on the Approximation Rates, Metric Entropy, and n-widths of Shallow Neural Networks

<https://www.arxiv.org/abs/2101.12365>

Gives an upper bound for non-linear approximation rates, metric entropy, and n-widths of their absolute convex hull, for smoothly parameterized dictionary. Then apply these result for dictionaries of ridge functions that correspond to shallow neural networks. Also provides the lower bound of the metric entropy and n-widths, which gives sharp lower bound on approximation rate for the ReLU^k activations and sigmoidal acitvation, with bounded variatiation.

### Neural Networks with linear threshold activations: structure and algorithms

<https://www.arxiv.org/abs/2111.08117>

Characterize the class of functions that are representable by linear threshold activation functions, and show that 2 hidden layer is necessary and sufficient. Also give precise bound on the sizes of neural networks to represent any function in the class.

### Do ReLU Networks Have An Edge When Approximating Compactly-Supported Functions?

<https://www.arxiv.org/abs/2204.11231>

Construct the refinement of the usual topology on locally integrable function spaces, where compactly-supported functions can only be approximated in L1-norm by functions with matchine discretized support. Then establish the universality of ReLU-NN with bilinear pooling in this topology.

### Why Do Networks Need Negative Weights?

<https://www.arxiv.org/abs/2208.03211>

Prove that deep neural netwroks with all non-negative weights are not universal approximators.

### Deep ReLU neural networks overcome the curse of dimensionality for partil integrodifferential equations

<https://www.arxiv.org/abs/2102.11707>

Show that the DNNs with ReLU activation functions are able to express viscosity solutions of linear PIDEs, which arises from a class of jump diffusions.