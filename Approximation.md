### Approximation Properties of Deep ReLU CNNs

<https://arxiv.org/abs/2109.00190>

Analyzes the L2 approximation properties of deep ReLU convolutional neural networks on two-dimensional space. Using the decomposition of convolutional kernels, show the universal approximation.

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://www.arxiv.org/abs/2106.14997>

Provide a lower bound on the approximation rates for shallow neural networks, which are obtained by lower bounding the L2 metric entropy of the convex hull of the neural network basis functions. 

### Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on Hoelder Class

<https://www.arxiv.org/abs/2103.00542>

For general continuous f on d dimensional box with continuity modulus, construct networks with sufficient approximation rate. This requires d^3/2 width, showing that this networks overcome the curse of dimensionality on Holder functions.

### Arbitrary-Depth Universal Approximation Theorems for Operator Neural Newtorks

<https://www.arxiv.org/abs/2109.11354>

Prove that operator NNs of bounded width and arbitrary depth are universal approximators for continuous nonlinear operators. 

### Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks

<https://arxiv.org/abs/2106.14997>

Prove sharp lower bounds on the approximation rates for shallow neural networks, obtained by lower bounding L2 metric entropy of the convex hull of basis functions. 

### Deep Network Approximation for Smooth Functions

<https://arxiv.org/abs/2001.03040>

Prove that multivariate polynomials can be approximated by deep ReLU networks of with small enough approximation error, then through local Taylor expansions, show that deep ReLU networks can approximate C^s functions.

### A proof that deep artificial neural networks overcome the curse of dimensionality in the numerical approximation of Komogorov partial differential equations with constant diffusion and nonlinear drift coefficients

<https://arxiv.org/abs/1809.07321>

Prove that the number of parameters used to describe the employed DNN grows at most polynomially in both the PDE dimension d and the reciprocal of the prescribed approximation accuracy.

### Uniform error estimates for artificial neural network approximations for heat equations

<https://arxiv.org/abs/1911.09647>

Develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs in the uniform L infty sense. Prove that the number of parameters of an ANN to uniformly approximate the classical solution of the heat equation in a region \[a,b\]^d for a fixed time point T grows at most polynomially in the dimension d and the reciprocal of the approximation precision. 

### Approximation capabilities of neural networks on unbounded domains

<https://arxiv.org/abs/1910.09293>

Prove that a shallow neural network with some activation functions can arbitrarily well approximate any Lp integrable functions defined on R * \[0,1\]^n, and moreover integrable function on the Euclidean plane.

### Approximation properties of Residual Neural Networks for Kolmogorov PDEs

<https://arxiv.org/abs/2111.00215>

Show that ResNets are able to approximate solutions of Kolmogorov PDEs with constant diffusion and possibly nonlinear drift coefficients without suffering the curse of dimensionality.

### Deep Learning in High Dimension: Neural Network Approximation of Analytic Functions in L2(R^d, gamma_d)

<https://arxiv.org/abs/2111.07080>

Prove expression rates for analytic function, L2 in the gaussian product measure, show the exponential convergence rate. The rate only depend on quantified holomorphy of F, to a product of strips in C^d. 

### Tighter Sparse Approximation Bounds for ReLU Neural Networks

<https://arxiv.org/abs/2110.03673>

Extend the previous work using Radon transform, define Radon-based semi-norms, that function admits an infinite-width neural network representation on a bounded open set when its norm is finite. Derive sparse finite width neural network approximation bounds, and show that infinite width representations are not unique.

### Optimal Approximation Rate of ReLU Netowrks in terms of Width and Depth

<https://arxiv.org/abs/2103.00502>

Construct ReLU networks approximating Hoelder continuous function on \[0,1\]^d, which is optimal up to constant.

### Optimal learning of high-dimensional classification problems using deep neural networks

<https://arxiv.org/abs/2112.12555>

Establish universal lower bounds for classification under regularity of decision boundary, and show that for locally Barron-regular decision boundary, the optimal estimation rate are independent of underlying dimension and can be realized by empirical risk minimization.

### A Unified and Constructive Framework for the Universality of Neural Networks

<https://www.arxiv.org/abs/2112.14877>

Design a framework for proving universality of activation, using it show various activations including Mish, SiLU, ELU, GELU and some new activation's universality.

### Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces

<https://www.arxiv.org/abs/2201.00217>

For nonparametric estimation of Lipscitz operators with deep neural networks, prove non-asymptotic upper bound for generalization error. With the assumption that the target operator exhibits a low dimensional structure, the error bound decays as sample size increases, where rate is determined by intrinsic dimension.

### Approximation capabilities of measure-preserving neural networks

<https://www.arxiv.org/abs/2106.10911>

For measure preserving neural networks like NICE and RevNets, if there is compact set in R^n > 1, it is able to approximate any measure preserving map which is bounded and injective in the Lp norm.

### A three layer neural network can represent any multivariate function

<https://www.arxiv.org/abs/2012.03016>

Prove that not only continuous functions, but also all discontinuous functions can be implemented by three-layer networks.

### Analysis of the Gradient Descent Algorithm for a Deep Neural Network Model with Skip-connections

<https://www.arxiv.org/abs/1904.05263>

Prove that GD can find global minimum exponentially fast, with generalization error established. If the target function is contained in RKHS induced by activation and initialization distribution, there exist generalizable early stopping solution in GD path.

### Advantages of Deep Neural Networks for Estimating Functions with Singularity on Hypersurfaces

<https://www.arxiv.org/abs/2011.02256>

Derive the generalization error of a DNN with optimal convergence rate on estimation on non smooth functions with singularities on hypersurfaces. Show that there is a phase where DNNs outperform other standard methods.

### Width is Less Important than Depth in ReLU Neural Networks

<https://www.arxiv.org/abs/2202.03841>

Show that any target network with inputs in d dimension can be approximated by a width O(d) network, with linearly over parameterized. 

### Approximation error of single hidden layer neural networks with fixed weights

<https://www.arxiv.org/abs/2202.03289>

Provides an explicit formula fo the approximation error of single hidden layer neural networks with two fixed weights.

### Approximation of Lipscitz Functions using Deep Spline Neural Networks

<https://www.arxiv.org/abs/2204.06233>

For Lipscitz contrained network, ReLU networks have provable disadvantage. Propose learnable spline activation function, and show that this choise is optimal among all component-wise 1-Lipscitz activation functions, in the sense that no other weight constrained architectures can approximate a larger class of functions.