### The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies

<https://www.arxiv.org/abs/1906.00425>

Study relationship between the frequency of a function and the speed at which a neural network learns it.
Approximate by linear system, and compute eigenfunction which is spherical harmonic functions.
Empirically, theoretically, shallow NN without bias can't learn simple low frequency functions with odd frequencies.

### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### The large learning rate phase of deep learning: the catapult mechanism

<https://www.arxiv.org/abs/2003.02218>

Present a class of neural networks with solvable training dynamics, and see two learning rate phase with their phenomena.

### When Does Preconditioning Help or Hurt Generalization?

<https://www.arxiv.org/abs/2006.10732>

Prove an exact asymptotics bias-variance decompositions of the generalization error of overparametrized ridgeless regression under a general class of preconditioner, considering the inverse population Fisher information matrix as a particular example. 

### Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients

<https://www.arxiv.org/abs/2009.13447>

Explain the reason that resampling outperforms reweighting using tools from dynamical stability and stochastic asymptotics.

### Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks

<https://www.arxiv.org/abs/2108.12001>

Provide a theoretical justification for the finding that adversarial training shrinks two important characteristics of the logit distribution: the max logit values and the logit gaps are on average lower for AT models. 

### Quantized convolutional neural networks through the lens of partial differential equations

<https://www.arxiv.org/abs/2109.00095>

Explore ways to improved quantized CNNs using PDE-based perspective, harnessing the total variation approach to apply edge-aware smoothing.

### The emergence of a concept in shallow neural networks

<https://www.arxiv.org/abs/2109.00454>

Show that there exists a critical sample size beyond which the restricted boltzmann machines can learn archetypes. Leverage the formal equivalence beteen RBMs and Hopfield networks, obtain a phase diagram for both architectures which highlights the regions where learning can be accomplished.

### Emergence of memory manifolds

<https://www.arxiv.org/abs/2109.03879>

Present a general principle called frozen stabilisation, allowing a family of neural networks to self-organise to a critical state exhibiting memory manifolds without parameter fine-tuning or symmetries.

### Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training

<https://www.arxiv.org/abs/2101.12699>

Introduce Layer-Peeled Model which is a nonconvex yet analytically tractable optimization problem, that can better understand deep neural newtorks, obtained by sisolating the topmost layer from the remainder of the neural networks, with some constraints on the two parts of the network. Using this, prove that in class-balanced datasets, any solution forms a simplex equiangular tight frame, and show neural collapse in imbalanced problem.

### A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning

<https://www.arxiv.org/abs/2109.02355>

Provides a succinct overview of this emerging theory of overparameterized ML that explains recent findings through a statistical signal processing perspective.

### Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks

<https://www.arxiv.org/abs/2006.13866>

Theoretically analyze the variance of sampling methods and show that, due to the composite structure of empirical risk, the variance of any sampling method can be decomposed into embedding approximation variance in the forward stage and stochastic gradient variance in the backward stage. Propose a decoupled variance reduction strategy.

### On the stability properties of Gated Recurrent Units neural networks

<https://www.arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteeing the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs. 

### Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation

<https://www.arxiv.org/abs/2002.11187>

Use GAN to estimate KL divergence, argue that high fluctuations in the estimates are a consequence of not controlling the complexity of the discriminator function space. Provide a theoretical underpinning and remedy for this problem by constructing a discriminator in the RKHS.

### A Unifying View on Implicit Bias in Training Linear Neural Networks

<https://www.arxiv.org/abs/2010.02501>

Propose a tensor formulation, and characterize the convergence direction as singular vectors, and show that gradient flow finds a stationary point or global minimum.

### The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks

<https://www.arxiv.org/abs/2109.06098>

Show the mathematical paradox, that any training procedure with a fixed architecture will yield neural networks that are either inaccurate or unstable. The key is that the stable and accurate neural networks must have variable dimensions depending on the input.

### On the regularized risk of distributionally robust learning over deep neural networks

<https://www.arxiv.org/abs/2109.06297>

Using tools from optimal transport theory, derive first order and second order approximations to the distributionally robust problem in terms of appropriate regularized risk minimization problems. 

### When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?

<https://www.arxiv.org/abs/2109.09444>

Provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound. Show that domain decomposition which decompose solution to simpler parts and make easier to solve, introduces a tradeoff for generalization, where the decomposition leads to less training data being available in each subdomain, prone to overfitting.

### On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods

<https://www.arxiv.org/abs/2109.10933>

Demonstrate that the norm test and inner product/orthogonality test are equivalent in terms of the convergence rates associated with SGD methods.

### Convolutional Rectifier Networks as Generalized Tensor Decompositions

<https://www.arxiv.org/abs/1603.00162>

Describe a construction based on generalized tensor decompositions that transforms convolutional arithmetic circuits into convolutional rectifier networks, then use toold from the world of arithmetic circuits. Show that convolutional rectifier networks are universal with max pooling but not with average pooling. Also show that depth efficiency is weker with convolutional rectifier networks than convolutional arithmetic circuits.

### Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions

<https://www.arxiv.org/abs/1705.02302>

Through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features.

## Deep Learning and the Information Bottleneck Principle

<https://www.arxiv.org/abs/1503.02406>

Show that any DNN can be quantified by the mutual information between the layers and the input and output variables, and calculate the optimal information theoretical limits of the DNN and obtain finite sample generalization bounds.

## Learning Dynamics of Deep Networks Admit Low-rank Tensor Descriptions

<https://openreview.net/pdf?id=Hy7RHt1vz>

Propose a simple tensor decomposition model to study how hidden representations evolve over learning, which precisely extracts the correct dynamics of learning and closed form solutions.

### Understanding Black-box Predictions via Influence Functions

<https://arxiv.org/abs/1703.04730>

Show that even on non-convex and non-differentiable models, approximations to influence functions can still provide valuable information. 

### Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

<https://arxiv.org/abs/1610.09887>

Prove that various types of simple and natural functions, including indicators of balls and ellipses, non-linear radial functions, smooth non-linear functions, can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger.

### Understanding Convolutional Neural Networks with Information Theory: An Initial Exploration

<https://arxiv.org/abs/1804.06537>

Show that the estimators enable straightforward measurement of information flow in realistic convolutional neural networks without any approximation, and introduce the partial information decomposition framework, develop three quantities to analyze the synergy and redundancy in convolutional layer representations.

### Topology-based Representative Datasets to Reduce Neural Network Training Resources

<https://arxiv.org/abs/1903.08519>

Prove that the accuracy of the learning process of a neural network on a representative dataset is similar to the accuracy on the original dataset, where representativeness is measured using persistence diagrams.

### Ridgeless Interpolation with Shallow ReLU Networks in 1D is Nearest Neighbor Curvature Extrapolation and Provably Generalizes on Lipscitz Fnctions

<https://arxiv.org/abs/2109.12960>

Prove a precise geometric description of all one layer ReLU networks with a single linear unit, with single input/output dimension, which interpolates a given dataset. Also show that ridgeless ReLU interpolants achieve the best possible generalization for learning 1d Lipscitz functions, up to universal constants.

### Searching for Minimal Optimal Neural Networks

<https://arxiv.org/abs/2109.13061>

Propose a rigorous mathematical framework for studying the asymptotic theory of the destructive technique, and prove that Adaptive group Lasso is consistent and can reconstruct the correct number of hidden nodes of one-hidden-layer feedforward networks with high probability.

### What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory

<https://arxiv.org/abs/2105.03361>

Develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. Propose a new function space, that captures the compositional structure associated deep neural networks. Derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space.

### Towards a theory of out-of-distribution learning

<https://arxiv.org/abs/2109.14501>

Define and prove the relationship between generalized notions of learnability, and show how this framework is sufficiently general to characterize transfer, multitask, meta, continual, and lifelong learning.

### On the Variance of the Fisher Information for Deep Learning

<https://arxiv.org/abs/2107.04205>

Investigate two estimators based on two equivalent representations of the FIM, and bound their variances and analyze how the parametric structure of a deep neural network can impact the variance.

### Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions

<https://arxiv.org/abs/2106.02619>

Prove that when a distribution has a structure that referred as Forward Super-Resolution, then training GANs using gradient descent ascent can indeed learn this distribution efficiently both in terms of sample and time complexities.

### On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow

<https://arxiv.org/abs/2011.02402>

Show that parametric kernelized gradient flow provides a descent direction minimizing the MMD on a statistical manifold of probability distributions.

### Avoiding pathologies in very deep networks

<https://arxiv.org/abs/1402.5836>

Show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, and propose an alternate architecture which does not suffer from this pathology.

### Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability

<https://arxiv.org/abs/2109.11792>

Show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability.

### Understanding How Over-Parameterization Leads to Acceleration: A case of learning a single teacher neuron

<https://arxiv.org/abs/2010.01637>

In the setting with single teacher neuron with quadratic activation and over parametrization realized by ahving multiple student neurons, provably show that over-parameterization helps the gradient descent iteration enter the neighborhood of a global optimal solution.

### Certifiably Robust Variational Autoencoders

<https://arxiv.org/abs/2102.07559>

Derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount. Then show how these parameters can be controlled, providing a mechanism to ensure desired level of robustness.

### An Unconstrained Layer-Peeled Perspective on Neural Collapse

<https://arxiv.org/abs/2110.02796>

Prove that gradient flow on unconstrained layer-peeled model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Then prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon.

### Exploring the Common Principal Subspace of Deep Features in Neural Networks

<https://arxiv.org/abs/2110.02863>

Find that different DNNs trained with the same dataset share a common principal subspace in latent spaces no matter architectures and labels. Specifically, design a new metric P-vector to represent the principal subspace of dee features learned, and propose to measure angles between the principal subspaces using P-vectors, with small angles have been found.

### On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks

<https://arxiv.org/abs/1901.10371>

Show that adversarial training tends to promote simultaneously low-rank and sparse structure. In the reverse direction, when the low rank structure is promoted by nclear norm regularization, neural networks show significantly improved robustness.

### Optimizing Neural Networks via Koopman Operator Theory

<https://arxiv.org/abs/2006.02361>

Show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of MLPs over a non-trivial range of training time.

### Spectral Pruning for Recurrent Neural Networks

<https://arxiv.org/abs/2105.10832>

Propose a pruning algorithm so called spectral pruning for RNN, and provide the generalization error bounds for compressed RNNs.

### On the Optimal Memorization Power of ReLU Neural Networks

<https://arxiv.org/abs/2110.03187>

Show that networks can memorize any N points using sqrt(N) parameters with some separability assumptions, which is optimal up to logarithmic factors.

### On the stability properties of Gated Recurrent Units neural networks

<https://arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteering the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs, which consist of nonlinear-inequalities on network's weights.

### Stability of Neural Networks on Manifold to Relative Perturbations

<https://arxiv.org/abs/2110.04702>

Prove that manifold neural networks composed of frequency ratio threshold filters, which separates the infinite-dimensional spectrum of the Laplace-Beltrami operator, are stable to relative operator perturbations. Observe that manifold neural networks exhibit a trade-off between stability and discriminability.

### How Well Generative Adversarial Networks Learn Distributions

<https://arxiv.org/abs/1811.03179>

Nonparametrically, derive the optimal minimax rates for distribution estimation under the adversarial framework. Parametrically, estabilsh a theory for general neural network classes that characterized the interplay on the choice of generator and discriminator pair.

### Phase Collapse in Neural Networks

<https://arxiv.org/abs/2110.05283>

By defining simplified complex-valued convolutional network architecture, which implements convolution with wavelet filters and uses a complex modulus to collapse phase variables, demonstrate that it is a different phase collapse mechanism which explains the ability to progressively eliminate spatial variability.

### Does Preprocessing Help Training Over-parameterized Neural Networks?

<https://arxiv.org/abs/2110.04622>

Design preprocessing algorithm for layer and input data, with convergence guarantee and lower train cost.

### Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks

<https://arxiv.org/abs/2110.03825>

Provide a theoretical analysis explaning on following observations, that 1) model parameters does not necessarily help adversarial robustness, 2) reducing capacity at the last stage of the network can actually improve adversarial robustness, and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness.

### Implicit Bias of Linear Equivariant Networks

<https://arxiv.org/abs/2110.06084>

Show that L layer full width linear GCNNs trained via gradient descent in a binary classification task converge to solutions with low-rank Fourier matrix coefficients, regularized by the 2/L-Schatten matrix norm. This generalizes previous analysis on the implicit bias of linear CNNs to linear GCNNs over all finite groups, including the challenging setting of non-commutative symmetry groups.

### Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs

<https://arxiv.org/abs/2110.05518>

Show that the training of multiple three-layer ReLU sub-networks with weight decay regularization can be equivalently cast as a convex optimization problem in a higher dimensional space, where sparsity is enforced via a group l1-norm regularization. Then prove that equivalent convex problem can be globally optimized by a standard convex optimization solve with a polynomial-time complexity w.r.t. number of samples and data dimension.

### Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Pruned Neural Networks

<https://arxiv.org/abs/2110.05667>

Characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. Show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned. 

### Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck

<https://arxiv.org/abs/2006.07522>

Analyze BNNs through the information bottleneck principle and observe that the training dynamics of BNNs is different from that of DNNs. While DNNs have a separate empirical risk minimization and representation compression phases, BNNs tend to find efficient hidden representations concurrently with label fitting.

### Towards Statistical and Computational Complexities of Polyak Step Size Gradient Descent

<https://arxiv.org/abs/2110.07810>

Demonstrate that the Polyak step size gradient descent iterates reach a final statistical radius of convergence around the true parameter after logarithmic number of iterations.

### Well-classified Examples are Underestimated in Classification with Deep Neural Networks

<https://arxiv.org/abs/2110.06537>

Theoretically show that giving less gradient for well-classified examples hinders representation learning, energy optimization, and the growth of margin. Propose to reward well-classified examples with additive bonuses to revive their contribution to learning.

### Detecting Modularity in Deep Neural Networks

<https://arxiv.org/abs/2110.08058>

Consider the problem of assessing the modularity exhibited by a partitioning of a network's neurons. Propose two proxies, importance and coherence measured by statistical methods. Then apply the proxies to partitionings generated by spectrally clustering neurons and show that these partitionings reveal groups of neurons that are important and coherent.

### Dropout as a Regularizer of Interaction Effects

<https://arxiv.org/abs/2007.00823>

Prove that dropout regularizes against higher-order interactions. 

### On the capacity of deep generative networks for approximating distributions

<https://arxiv.org/abs/2101.12353>

Prove that neural networks can transform a low-dimensional source distribution to a distribution that is arbitrarily close to a high-dimensional target distribution, when the closeness are measured by Wasserstein distances and maximum mean discrepancy. 

### Understanding Convolutional Neural Networks from Theoretical Perspective via Volterra Convolution

<https://arxiv.org/abs/2110.09902>

Show that CNN is an approximation of the finite term Volterra convolution, whose order increases exponentially with the number of layers and kernel size increases exponentially with the strides.

### Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem

<https://arxiv.org/abs/2110.10295>

Prove that periodic points alone lead to suboptimal depth-width tradeoffs and improve upon them by demonstrating that certain "chaotic itineraries" give stronger exponential tradeoffs. Identify a phase transition to the chaotic regime that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy.

### Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks

<https://arxiv.org/abs/2110.10815>

Provide convergence guarantees with rates for deep linear networks for both continuous and discrete dynamics on FA algorithms.

### Analyzing the expressive power of graph neural networks in a spectral perspective

<https://www.researchgate.net/publication/349119879_ANALYZING_THE_EXPRESSIVE_POWER_OF_GRAPH_NEURAL_NETWORKS_IN_A_SPECTRAL_PERSPECTIVE>

By bridging the gap between the spectral and spatial design of graph convolutions, theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain.

### Early Stopping in Deep Networks: Double Descent and How to Eliminate It

<https://arxiv.org/abs/2007.10099>

Show that epoch-wise double descent arises by a superposition of two or more bias-variance tradeoff that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. Show this analytically for linear regression and a two-layer neural network.

### A Universal Law of Robustness via Isoperimetry

<https://arxiv.org/abs/2105.12806>

Show that smooth interpolation requires d times parameters than mere interpolation, where d is the ambient data dimension, for any smoothly parametrized function class with polynomial size weights, and any covaraiate distribution verifying isoperimetry.

### Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model

<https://arxiv.org/abs/2110.11805>

Analyze the whole temporal behavior of the genralization and training errors under gradient flow for the random feature model. Show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. Techniques are based on Cauchy complex integral representations of the errors with recent random matrix methods based on linear pencils.

### Wide Neural Networks Forget Less Catastrophically

<https://arxiv.org/abs/2110.11526>

Focus on the model and study the impact of width of the NN architecture on catastrophic forgetting, and show that width has a suprisingly significant effect. Study the learning dynamics of the network from various perspectives, including gradient norm, sparsity, orthogonalization, lazy training.

### The Equilibrium Hypothesis: Rethinking implicit regularization in Deep Neural Networks

<https://arxiv.org/abs/2110.11749>

Recent work showed that some layers are much more aligned with data labels than other layers, called impricial layer selection. Introduce and empirically validate the Equilibrium Hypothesis stating that the layers achieve some balance between forward and backward information loss are the ones with the highest alignment to data labels.

### On some theoretical limitations of Generative Adversarial Networks

<https://arxiv.org/abs/2110.10915>

Provide a new result based on Extreme Value Theory showing that GANs can't generate heavy tailed distributions.

### Faster Neural Network Training with Approximate Tensor Operations

<https://arxiv.org/abs/1805.08079>

Introduce a new technique for faster NN training using sample-based approximation to the tensor operations, prove that they provide the same convergence guarantees.

### Towards Lower Bounds on the Depth of ReLU Neural Networks

<https://arxiv.org/abs/2105.14835>

Using techniques from mixed-integer optimization, polyhedral theory, tropical geometry, provide a counterbalance to the universal approximation theorem which suggest that a single hidden layer is sufficient for learning tasks. Inverstigate whether the class of exactly representablt functions strictly increases by adding more layers. Also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes.

### Does the Data Induce Capacity Control in Deep Learning?

<https://arxiv.org/abs/2110.14163>

Show that the data correlation matrix, Hessian, Fisher Information Matrix all share 'sloppy' eigenspectrum where a large number of small eigenvalues are distributed uniformly over an exponentially large range. Show that this structure in the data can give to non-vacuous PAC-Bayes generalization bounds analytically.

### Open Problem: Tight Online Confidence Intervals for RKHS Elements

<https://arxiv.org/abs/2110.15458>

Formalize the question of online confidence intervals in the RKHS setting. It is still unclear whether the suboptimal regret bound is a fundamental shortcoming or artifact of the proof.

### What training reveals about neural network complexity

<https://arxiv.org/abs/2106.04186>

Explores Benevolent Training Hypothesis, that the complexity of target function can be deduced by training dynamics. Observe that the Lipscitz constant close to the training data affects various aspects of the parameter trajectory, with more complex network having longer trajectory, bigger variance. Show that NNs whose first layer bias is trained more steadily have bounded complexity, and find that steady training with dropout implies a training and data-dependent generalization bound growing poly-logarithmically with the number of parameters. 

### Framing RNN as a kernel method: A neural ODE approach

<https://arxiv.org/abs/2106.01202>

Show that under appropriate conditions, the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature, framing RNN as a kernel method in a suitable RKHS. Obtain theoretical guarantees on generalization and stability.

### Collapse of Deep and Narrow Neural Nets

<https://arxiv.org/abs/1808.04947>

Show that even for ReLU activation, deep and narrow NNs will converge to errorneous mean or median states of the target function depending on the loss with high probability. 

### Why Stable Learning Works? A Theory of Covariate Shift Generalization

<https://arxiv.org/abs/2111.02355>

Prove that under ideal conditions, stable learning algorithms could identify minimal stable variable set, that is minimal and optimal to deal with covariate shift generalization for common loss functions.

### Subquadratic Overparameterization for Shallow Neural Networks

<https://arxiv.org/abs/2111.01875>

Provide an analyical framework that allows to adopt standard initialization strategies, avoid lazy training, and train all layers simultaneously in basic shallow neural network while attaining a desirable subquadratic scaling on the network depth, using Polyak-Lojasiewicz condition, and random matrix theory.

### Diversity and Generalization in Neural Network Ensembles

<https://arxiv.org/abs/2110.13786>

Provide sound answers tot he following questions, how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms.

### Early-stopped neural networks are consistent

<https://arxiv.org/abs/2106.05932>

Show that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration.

### Multiple Descent: Design Your Own Generalization Curve

<https://arxiv.org/abs/2008.01036>

Show that the generalization curve can have an arbitrary number of peaks, and the locations of those peaks can be explicitly controlled in variable parameterized families of models on linear regression. The emergence of double descnet is due to the interaction between the properties of the data and the inductive bias of learnin algorithms.

### Improved Regularization and Robustness for Fine-tuning in Neural Networks

<https://arxiv.org/abs/2111.04578>

Present a PAC-Bayes generalization bound that depends on the distance traveled in each layer during fine-tuning and the noise stability. 

### Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel

<https://arxiv.org/abs/2107.12723>

Show oracle type bounds which reveal that the generalisation and excess risk of GD is controlle by an interpolating network with the shortest GD path from inistialisation. Also show that this analysis is tightesr then existing NTK-based risk bounds, and show that GD with early stoppping is constant.

### Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks

<https://arxiv.org/abs/2002.11318>

Prove a quantitative trade-off between spatial and adversarial robustness in a simple statistical setting. 

### On a Sparse Shortcut Topology of Artificial Neural Networks

<https://arxiv.org/abs/1811.09003>

Propose new shortcut architecture, and show that it can approximate any univariate continuous function in width-bounded setting, and show the generalization bound.

### ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation

<https://arxiv.org/abs/2102.06635>

Introduce the concept of Max-Affine Arithmetic Programs, and use them to show that undirected graph's minimum spanning tree and maximum flow computation is possible with NNs of cubic/quadratic width.

### The Three Stages of Learning Dynamics in High-Dimensional Kernel Methods

<https://arxiv.org/abs/2111.07167>

Study the training dynamics of gradient flow on kernel least-squares objective, which is limiting dynamics of SGD trained NNs. 

### Assessing Deep Neural Networks as Probability Estimators

<https://arxiv.org/abs/2111.08239>

Find that the likelihood probability density and the inter-categorical sparsity have greater impacts than the prior probability to DNN's classification uncertainty.

### Towards Understanding the Condensation of Neural Networks at Initial Training

<https://arxiv.org/abs/2105.11686>

Empirically, it is observed that input weights condense on isolated orientation with a small initialization. Show that maximal number of condensed orientation in the initial stage is twice the multiplicity of the acitvation function, where multiplicity is multiple roots of activation function at origin. 

### Depth Without the Magic: Inductive Bias of Natural Gradient Descent

<https://arxiv.org/abs/2111.11542>

Gradient descent has implicit inductive bias, that the parameterization gives different optimization trajectory. Natural gradient descent is approximately invariant to such parameterization, giving same trajectory and same minimum. Show that there exist learning problem where natural gradient descent fails to generalize while gradient descent performs well.

### Deep Autoencoders: From Understanding to Generalization Guarantees

<https://arxiv.org/abs/2009.09525>

Reformulate AEs by continuous piecewise affine structure, to show how AEs approximate the data manifold, giving some insights for reconstruction guarantees and interpretation of regularization guarantees. Design two new regularization that leverages the inherent symmetry learning, prove that the regularizations ensure the generalization with assumption on symmetry of the data with Lie group.

### Improved Fine-tuning by Leveraging Pre-training Data: Theory and Practice

<https://arxiv.org/abs/2111.12292>

Show that final prediction precision may have a weak dependency on the pre-trained model especially in the case of large training terations. Shows that the final performance can be improved when appropriate pre-training data is included in fine-tuning, and design a novel selection strategy to select a subset from pre-training data to help improve the generalization.

### The staircase property: How hierarchical structure can guide deep learning

<https://arxiv.org/abs/2108.10573>

Defines a staircase property for functions over the boolean hypercube, which posits that high-order Fourier coefficients are reachable from low-order Fourier coefficients along increasing chains. Prove that functions with staircase property can be learned in polynomial time using layerwise stochastic coordinate descent on regular neural network. 

### Gradient Starvation: A Learning Proclivity in Neural Networks

<https://arxiv.org/abs/2011.09468>

Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. Using tools from dynamical systems theory, identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data.

### Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks

<https://arxiv.org/abs/2111.12963>

Derive error bounds in Lebesgue and Sobolev norms to approximate arbitrary matrix-vector product using ReLU NN.

### Implicit Data-Driven Regularization in Deep Neural Networks under SGD

<https://arxiv.org/abs/2111.13331>

Analyze evolutions of weight matrices' spectra, and they are classified to Marchenko-Pastur, Marchenko-Pastur with few bleeding outliers, Heavy tailed spectrum. These are connected to the degree of regularization, and argue that degree depends on the quality of data.

### On Linear Stability of SGD and Input-Smoothness of Neural Networks

<https://arxiv.org/abs/2105.13462>

Show that SGD tends to impose constraints on high-order moments of the gradient noise, by a linear analysis of SGD aroung global minima. Identify Sobolev regularization effect of SGD, that SGD regularizes the Sobolev seminorms of the model functions w.r.t. the input data.

### How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?

<https://arxiv.org/abs/2012.12896>

Provide a formal framework connecting the robustness of a network to the alignments between its architecture and target functions. Hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise.

### On the rate of convergence of a classifier based on a Transformer encoder

<https://arxiv.org/abs/2111.14574>

The rate of convergence of the misclassification probability towards the optimal misclassification probability, and shown that this classifier is able to circumvent the curse of dimensionality.

### The Geometric Occam's Razor Implicit in Deep Learning

<https://arxiv.org/abs/2111.15090>

Over-parameterized neural networks trained with SGD are subject to a Geometric Occam's Razor, that they are implicitly regularized by the geometric model complexity, which is a Dirichlet energy of the function.

### Embedding Principle: a hierarchical structure of loss landscape of deep neural networks

<https://arxiv.org/abs/2111.15527>

Prove a general embedding principle of loss landscape of DNNs that unravels a hierarchical structure of the loss landscape of NNs, loss landscape of an NN contains all critical points of all the narrower NNs. Provide a gross estimate of the dimension of critical submanifolds embedded from critical points of narrower NNs. Prove an irreversibility property of any critical embedding.

### Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows

<https://arxiv.org/abs/2112.01363>

Design gradient based optimization for achieving acceleration, by first leveraging a continuous time framework for designing fixed-time stable dynamical systems, provigding a consistent discretization strategy such that the equiavlent discrete-time algorithm tracks the optimizer in a practically fixed number of iterations. Provide the convergence behavior of the proposed gradient flow and robustness to additive disturbances. 

### Asymptotic properties of one-layer artificial neural networks with sparse connectivity

<https://arxiv.org/abs/2112.00732>

Asymptotic of empirical distribution of parameters of a one-layer ANNs with sparse connectivity, with increasing number of parameter and iteration steps.

### Test Sample Accuracy Scales with Training Sample Density in Neural Networks

<https://arxiv.org/abs/2106.08365>

Propose an error function for piecewise linear NNs taking a local region of input space and smooth empirical training error, that is an average of empirical training erros from other regions weighted by network represenation distance. A bound on the expected smooth error for each region scales inversely with training sample desnsity in representation space.

### On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks

<https://arxiv.org/abs/2008.09052>

Define notion of a generic transversal ReLU neural network, and show that almost all ReLU networks are generic and transversal. Using the obstruction, prove that a decision region of a generic, transversal ReLU network with a single hidden layer of dimension n+1 can have no more than one bounded connected components.

### Continuous vs. Discrete Optimization of Deep Neural Networks

<https://arxiv.org/abs/2107.06608>

Find that the degree of approximation of gradient descent on gradient flow depends on the curvature around the gradient flow trajectory. Show that over DNNs with homogeneous activations, gradient flow trajectories enjoy favorable curvature, that they are well approximated by gradient descent.

### Theory of gating in recurrent neural network

<https://arxiv.org/abs/2007.14823>

Show that gating offers flexible control of two salient features, timescales and dimensionality.

### MomentumRNN: Integrating Momentum into Recurrent Neural Networks

<https://arxiv.org/abs/2006.06919>

Establish a connection between the hidden state dynamics in an RNN and gradient descent, integrating the momentum to this framework, prove that MomentumRNNs alleviate the vanishing gradient issue.

### A Complete Characterisation of ReLU-Invariant Distributions

<https://arxiv.org/abs/2112.06532>

Give a complete characterization of families of probability distributions that are invariant under the action of ReLU NN layers, proving that no invariant parameterised can exist unless one of follwoing holds, network's width is one, probability measure have finite support, and the parameterization is not locally Lipscitz continuous.

### On the Expected Complexity of Maxout Networks

<https://arxiv.org/abs/2107.00379>

Number of activation regions are used as a complexity measure, and it has shown that practical complexity of Deep ReLU networks is often far from the theoretical maximum. Show that this also occurs in maxout activation, and give nontrivial lower bounds on the complexity, finally gives that different initialization can increase speed of convergence.

### The Power of Contrast for Feature Learning: A Theoretical Analysis

<https://arxiv.org/abs/2110.02473>

By using connection between PCA and linear Autoencoder, GAN, contrast learning, show that contrastive learning outperforms autoender for both feature learning and downstream tasks.

### Neurashed: A Phenomenological Model for Imitating Deep Learning Training

<https://arxiv.org/abs/2112.09741>

Design a graphical model neurashed, which inherits hierarchically structured, optimized through SGD, and information evolving compressively, and enables insights into implicit regularization, information bottleneck, and local elasticity.

### Understanding and Mitigating Exploding Inverses in Invertible Neural Networks

<https://arxiv.org/abs/2006.09347>

Show that commonly used INN architectures suffer from explodinig inverses, and reveal failures including the non-applicability of the change-of-variables formula on in- and OOD data, incorrect gradients, inability to sample from normalizing flow. 

### Asymptotics of l2 Regularized Network Embeddings

<https://www.arxiv.org/abs/2201.01689>

Study effects of regularization on embedding in unsupervised random walk, and prove that under exchangeability assumption on the graphs, it leads to learning a nuclear-norm type penalized graphon. In particular, the exact form of penalty depends on the choice of subsampling method used.

### The dynamics of representation learning in shallow, non-linear autoencoders

<https://www.arxiv.org/abs/2201.02115>

Derive a set of asymptotically exact equations that describe the generalisation dynamics of autoenoders trained with SGD in the limit of high-dimensional inputs. 

### Fixed points of nonnegative neural networks

<https://www.arxiv.org/abs/2106.16239>

Derive condition for the existence of fixed points of nonnegative neural networks, by recognizing them as monotonic scalable functions within nonlinear Perron Frobenius theory, and show fixed point set's shape is often interval.

### Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaption

<https://www.arxiv.org/abs/2010.01184>

Covariate shift adaption usually suffer from small effective sample size, which is common in high dimensional setting. Focus on unified view connecting ESS, data dimensionality, and generalization in covariate shift adaption, and demonstrate how dimensionality reduction or feature selection increase the ESS. 

### Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks

<https://www.arxiv.org/abs/1810.02244>

Relate Graph Neural Network to 1-dimensional Weisfeiler-Leman graph isomorphism heuristics, show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic graphs, with same shortcomings. Propose a generalization of GNNs so-called k-dimensional GNNs that can take higher-order graph structures.

### How Powerful are Graph Neural Networks?

<https://www.arxiv.org/abs/1810.00826>

Characterize the discriminative power of GNN variants like Graph Convolution Networks or GraphSAGE, and show that they cannot distinguish certain simple graph structures, and develop provably most expressive architecture, which is as powerful as the Weisfeiler-Lehman graph isomorphism test.

### The Surprising Power of Graph Neural Networks with Random Node Initialization

<https://www.arxiv.org/abs/2010.01179>

Analyze the expressive power of GNNs with Random Node Initialization, prove that these models are universal.

### Augmented Neural ODE

<https://proceedings.neurips.cc/paper/2019/file/21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf>

Show that Neural ODEs learn representation preserving the topology of the input space, hence there is function that Neural ODE can not represent. Solve this limitation by Augmented Neural ODE.

### A mean-field optimal control formulation of deep learning

<https://link.springer.com/article/10.1007/s40687-018-0172-y>

Introduces the mathematical formulation of viewing population risk minimization as mean-field optimal control problem, and prove stability condition of the Hamilton-Jacobi-Bellman type and Pontryagin type. By mean-field Pontryagin's maximum principle, establish quantitative relationships between population and empirical learning problem.

### Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantics Segmentatio

<https://www.arxiv.org/abs/2103.11594>

Even with extremely noisy label on semantic segmentation, DNN still provide similar segmenetation performance as trained with original ground truth, ndicating that DNNs learn structures hidden in labels rather than pixel-level labels. Referring this structure as meta-structure, define this formally as spatial density distribution showing both theoretically and experimentally how this explains the behavior.

### An unfeasiability view of neural network learning

<https://www.arxiv.org/abs/2201.00945>

Define notion of a continuously differentiable perfect learning algorithm, and show that such algorithms don't exist given that length of the data set exceeds the number of involved parameters, with logistic, tanh, sin activation.

### A Kernel-Expanded Stochastic Neural Network

<https://www.arxiv.org/abs/2201.05319>

Design new architecture which incorporates support vector regression at its first layer, allowing to break the high-dimensional nonconvex training of neural network to series of low-dimensional convex optimization, and can be trained using imputation-regularized optimization, with a theoretical guarantee to global convergence.

### De Rham compatible Deep Neural Networks

<https://www.arxiv.org/abs/2201.05395>

Construct classes of neural networks with ReLU and BiSU activation, emulating the lowest order Finite Element spaces on regular simplicical partitions of polygonal domains for 2, 3 dimension. 
