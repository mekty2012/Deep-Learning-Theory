### Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

<https://www.arxiv.org/abs/1602.05897>

Show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space.

### The large learning rate phase of deep learning: the catapult mechanism

<https://www.arxiv.org/abs/2003.02218>

Present a class of neural networks with solvable training dynamics, and see two learning rate phase with their phenomena.

### Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients

<https://www.arxiv.org/abs/2009.13447>

Explain the reason that resampling outperforms reweighting using tools from dynamical stability and stochastic asymptotics.

### The emergence of a concept in shallow neural networks

<https://www.arxiv.org/abs/2109.00454>

Show that there exists a critical sample size beyond which the restricted boltzmann machines can learn archetypes. Leverage the formal equivalence beteen RBMs and Hopfield networks, obtain a phase diagram for both architectures which highlights the regions where learning can be accomplished.

### Emergence of memory manifolds

<https://www.arxiv.org/abs/2109.03879>

Present a general principle called frozen stabilisation, allowing a family of neural networks to self-organise to a critical state exhibiting memory manifolds without parameter fine-tuning or symmetries.

### Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training

<https://www.arxiv.org/abs/2101.12699>

Introduce Layer-Peeled Model which is a nonconvex yet analytically tractable optimization problem, that can better understand deep neural newtorks, obtained by sisolating the topmost layer from the remainder of the neural networks, with some constraints on the two parts of the network. Using this, prove that in class-balanced datasets, any solution forms a simplex equiangular tight frame, and show neural collapse in imbalanced problem.

### A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning

<https://www.arxiv.org/abs/2109.02355>

Provides a succinct overview of this emerging theory of overparameterized ML that explains recent findings through a statistical signal processing perspective.

### Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation

<https://www.arxiv.org/abs/2002.11187>

Use GAN to estimate KL divergence, argue that high fluctuations in the estimates are a consequence of not controlling the complexity of the discriminator function space. Provide a theoretical underpinning and remedy for this problem by constructing a discriminator in the RKHS.

### A Unifying View on Implicit Bias in Training Linear Neural Networks

<https://www.arxiv.org/abs/2010.02501>

Propose a tensor formulation, and characterize the convergence direction as singular vectors, and show that gradient flow finds a stationary point or global minimum.

### On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods

<https://www.arxiv.org/abs/2109.10933>

Demonstrate that the norm test and inner product/orthogonality test are equivalent in terms of the convergence rates associated with SGD methods.

### Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions

<https://www.arxiv.org/abs/1705.02302>

Through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features.

## Deep Learning and the Information Bottleneck Principle

<https://www.arxiv.org/abs/1503.02406>

Show that any DNN can be quantified by the mutual information between the layers and the input and output variables, and calculate the optimal information theoretical limits of the DNN and obtain finite sample generalization bounds.

## Learning Dynamics of Deep Networks Admit Low-rank Tensor Descriptions

<https://openreview.net/pdf?id=Hy7RHt1vz>

Propose a simple tensor decomposition model to study how hidden representations evolve over learning, which precisely extracts the correct dynamics of learning and closed form solutions.

### Understanding Black-box Predictions via Influence Functions

<https://arxiv.org/abs/1703.04730>

Show that even on non-convex and non-differentiable models, approximations to influence functions can still provide valuable information. 

### Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

<https://arxiv.org/abs/1610.09887>

Prove that various types of simple and natural functions, including indicators of balls and ellipses, non-linear radial functions, smooth non-linear functions, can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger.

### Understanding Convolutional Neural Networks with Information Theory: An Initial Exploration

<https://arxiv.org/abs/1804.06537>

Show that the estimators enable straightforward measurement of information flow in realistic convolutional neural networks without any approximation, and introduce the partial information decomposition framework, develop three quantities to analyze the synergy and redundancy in convolutional layer representations.

### Topology-based Representative Datasets to Reduce Neural Network Training Resources

<https://arxiv.org/abs/1903.08519>

Prove that the accuracy of the learning process of a neural network on a representative dataset is similar to the accuracy on the original dataset, where representativeness is measured using persistence diagrams.

### Ridgeless Interpolation with Shallow ReLU Networks in 1D is Nearest Neighbor Curvature Extrapolation and Provably Generalizes on Lipscitz Functions

<https://arxiv.org/abs/2109.12960>

Prove a precise geometric description of all one layer ReLU networks with a single linear unit, with single input/output dimension, which interpolates a given dataset. Also show that ridgeless ReLU interpolants achieve the best possible generalization for learning 1d Lipscitz functions, up to universal constants.

### Searching for Minimal Optimal Neural Networks

<https://arxiv.org/abs/2109.13061>

Propose a rigorous mathematical framework for studying the asymptotic theory of the destructive technique, and prove that Adaptive group Lasso is consistent and can reconstruct the correct number of hidden nodes of one-hidden-layer feedforward networks with high probability.

### What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory

<https://arxiv.org/abs/2105.03361>

Develop a variational framework to understand the properties of functions learned by fitting deep neural networks with rectified linear unit activations to data. Propose a new function space, that captures the compositional structure associated deep neural networks. Derive a representer theorem showing that deep ReLU networks are solutions to regularized data fitting problems over functions from this space.

### Towards a theory of out-of-distribution learning

<https://arxiv.org/abs/2109.14501>

Define and prove the relationship between generalized notions of learnability, and show how this framework is sufficiently general to characterize transfer, multitask, meta, continual, and lifelong learning.

### On the Variance of the Fisher Information for Deep Learning

<https://arxiv.org/abs/2107.04205>

Investigate two estimators based on two equivalent representations of the FIM, and bound their variances and analyze how the parametric structure of a deep neural network can impact the variance.

### Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions

<https://arxiv.org/abs/2106.02619>

Prove that when a distribution has a structure that referred as Forward Super-Resolution, then training GANs using gradient descent ascent can indeed learn this distribution efficiently both in terms of sample and time complexities.

### On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow

<https://arxiv.org/abs/2011.02402>

Show that parametric kernelized gradient flow provides a descent direction minimizing the MMD on a statistical manifold of probability distributions.

### Avoiding pathologies in very deep networks

<https://arxiv.org/abs/1402.5836>

Show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, and propose an alternate architecture which does not suffer from this pathology.

### Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability

<https://arxiv.org/abs/2109.11792>

Show that regularized MDPs satisfy a certain quadratic growth criterion, which is sufficient to establish stability.

### An Unconstrained Layer-Peeled Perspective on Neural Collapse

<https://arxiv.org/abs/2110.02796>

Prove that gradient flow on unconstrained layer-peeled model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Then prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon.

### Exploring the Common Principal Subspace of Deep Features in Neural Networks

<https://arxiv.org/abs/2110.02863>

Find that different DNNs trained with the same dataset share a common principal subspace in latent spaces no matter architectures and labels. Specifically, design a new metric P-vector to represent the principal subspace of dee features learned, and propose to measure angles between the principal subspaces using P-vectors, with small angles have been found.

### Optimizing Neural Networks via Koopman Operator Theory

<https://arxiv.org/abs/2006.02361>

Show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of MLPs over a non-trivial range of training time.

### Spectral Pruning for Recurrent Neural Networks

<https://arxiv.org/abs/2105.10832>

Propose a pruning algorithm so called spectral pruning for RNN, and provide the generalization error bounds for compressed RNNs.

### On the Optimal Memorization Power of ReLU Neural Networks

<https://arxiv.org/abs/2110.03187>

Show that networks can memorize any N points using sqrt(N) parameters with some separability assumptions, which is optimal up to logarithmic factors.

### Stability of Neural Networks on Manifold to Relative Perturbations

<https://arxiv.org/abs/2110.04702>

Prove that manifold neural networks composed of frequency ratio threshold filters, which separates the infinite-dimensional spectrum of the Laplace-Beltrami operator, are stable to relative operator perturbations. Observe that manifold neural networks exhibit a trade-off between stability and discriminability.

### Phase Collapse in Neural Networks

<https://arxiv.org/abs/2110.05283>

By defining simplified complex-valued convolutional network architecture, which implements convolution with wavelet filters and uses a complex modulus to collapse phase variables, demonstrate that it is a different phase collapse mechanism which explains the ability to progressively eliminate spatial variability.

### Does Preprocessing Help Training Over-parameterized Neural Networks?

<https://arxiv.org/abs/2110.04622>

Design preprocessing algorithm for layer and input data, with convergence guarantee and lower train cost.

### Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs

<https://arxiv.org/abs/2110.05518>

Show that the training of multiple three-layer ReLU sub-networks with weight decay regularization can be equivalently cast as a convex optimization problem in a higher dimensional space, where sparsity is enforced via a group l1-norm regularization. Then prove that equivalent convex problem can be globally optimized by a standard convex optimization solve with a polynomial-time complexity w.r.t. number of samples and data dimension.

### Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Pruned Neural Networks

<https://arxiv.org/abs/2110.05667>

Characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. Show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned. 

### Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck

<https://arxiv.org/abs/2006.07522>

Analyze BNNs through the information bottleneck principle and observe that the training dynamics of BNNs is different from that of DNNs. While DNNs have a separate empirical risk minimization and representation compression phases, BNNs tend to find efficient hidden representations concurrently with label fitting.

### Well-classified Examples are Underestimated in Classification with Deep Neural Networks

<https://arxiv.org/abs/2110.06537>

Theoretically show that giving less gradient for well-classified examples hinders representation learning, energy optimization, and the growth of margin. Propose to reward well-classified examples with additive bonuses to revive their contribution to learning.

### Detecting Modularity in Deep Neural Networks

<https://arxiv.org/abs/2110.08058>

Consider the problem of assessing the modularity exhibited by a partitioning of a network's neurons. Propose two proxies, importance and coherence measured by statistical methods. Then apply the proxies to partitionings generated by spectrally clustering neurons and show that these partitionings reveal groups of neurons that are important and coherent.

### Dropout as a Regularizer of Interaction Effects

<https://arxiv.org/abs/2007.00823>

Prove that dropout regularizes against higher-order interactions. 

### Understanding Convolutional Neural Networks from Theoretical Perspective via Volterra Convolution

<https://arxiv.org/abs/2110.09902>

Show that CNN is an approximation of the finite term Volterra convolution, whose order increases exponentially with the number of layers and kernel size increases exponentially with the strides.

### Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem

<https://arxiv.org/abs/2110.10295>

Prove that periodic points alone lead to suboptimal depth-width tradeoffs and improve upon them by demonstrating that certain "chaotic itineraries" give stronger exponential tradeoffs. Identify a phase transition to the chaotic regime that exactly coincides with an abrupt shift in other notions of function complexity, including VC-dimension and topological entropy.

### Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks

<https://arxiv.org/abs/2110.10815>

Provide convergence guarantees with rates for deep linear networks for both continuous and discrete dynamics on FA algorithms.

### Early Stopping in Deep Networks: Double Descent and How to Eliminate It

<https://arxiv.org/abs/2007.10099>

Show that epoch-wise double descent arises by a superposition of two or more bias-variance tradeoff that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. Show this analytically for linear regression and a two-layer neural network.

### A Universal Law of Robustness via Isoperimetry

<https://arxiv.org/abs/2105.12806>

Show that smooth interpolation requires d times parameters than mere interpolation, where d is the ambient data dimension, for any smoothly parametrized function class with polynomial size weights, and any covaraiate distribution verifying isoperimetry.

### Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model

<https://arxiv.org/abs/2110.11805>

Analyze the whole temporal behavior of the genralization and training errors under gradient flow for the random feature model. Show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. Techniques are based on Cauchy complex integral representations of the errors with recent random matrix methods based on linear pencils.

### Wide Neural Networks Forget Less Catastrophically

<https://arxiv.org/abs/2110.11526>

Focus on the model and study the impact of width of the NN architecture on catastrophic forgetting, and show that width has a suprisingly significant effect. Study the learning dynamics of the network from various perspectives, including gradient norm, sparsity, orthogonalization, lazy training.

### The Equilibrium Hypothesis: Rethinking implicit regularization in Deep Neural Networks

<https://arxiv.org/abs/2110.11749>

Recent work showed that some layers are much more aligned with data labels than other layers, called impricial layer selection. Introduce and empirically validate the Equilibrium Hypothesis stating that the layers achieve some balance between forward and backward information loss are the ones with the highest alignment to data labels.

### On some theoretical limitations of Generative Adversarial Networks

<https://arxiv.org/abs/2110.10915>

Provide a new result based on Extreme Value Theory showing that GANs can't generate heavy tailed distributions.

### Faster Neural Network Training with Approximate Tensor Operations

<https://arxiv.org/abs/1805.08079>

Introduce a new technique for faster NN training using sample-based approximation to the tensor operations, prove that they provide the same convergence guarantees.

### Towards Lower Bounds on the Depth of ReLU Neural Networks

<https://arxiv.org/abs/2105.14835>

Using techniques from mixed-integer optimization, polyhedral theory, tropical geometry, provide a counterbalance to the universal approximation theorem which suggest that a single hidden layer is sufficient for learning tasks. Inverstigate whether the class of exactly representablt functions strictly increases by adding more layers. Also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes.

### Does the Data Induce Capacity Control in Deep Learning?

<https://arxiv.org/abs/2110.14163>

Show that the data correlation matrix, Hessian, Fisher Information Matrix all share 'sloppy' eigenspectrum where a large number of small eigenvalues are distributed uniformly over an exponentially large range. Show that this structure in the data can give to non-vacuous PAC-Bayes generalization bounds analytically.

### Open Problem: Tight Online Confidence Intervals for RKHS Elements

<https://arxiv.org/abs/2110.15458>

Formalize the question of online confidence intervals in the RKHS setting. It is still unclear whether the suboptimal regret bound is a fundamental shortcoming or artifact of the proof.

### What training reveals about neural network complexity

<https://arxiv.org/abs/2106.04186>

Explores Benevolent Training Hypothesis, that the complexity of target function can be deduced by training dynamics. Observe that the Lipscitz constant close to the training data affects various aspects of the parameter trajectory, with more complex network having longer trajectory, bigger variance. Show that NNs whose first layer bias is trained more steadily have bounded complexity, and find that steady training with dropout implies a training and data-dependent generalization bound growing poly-logarithmically with the number of parameters. 

### Collapse of Deep and Narrow Neural Nets

<https://arxiv.org/abs/1808.04947>

Show that even for ReLU activation, deep and narrow NNs will converge to errorneous mean or median states of the target function depending on the loss with high probability. 

### Why Stable Learning Works? A Theory of Covariate Shift Generalization

<https://arxiv.org/abs/2111.02355>

Prove that under ideal conditions, stable learning algorithms could identify minimal stable variable set, that is minimal and optimal to deal with covariate shift generalization for common loss functions.

### Subquadratic Overparameterization for Shallow Neural Networks

<https://arxiv.org/abs/2111.01875>

Provide an analyical framework that allows to adopt standard initialization strategies, avoid lazy training, and train all layers simultaneously in basic shallow neural network while attaining a desirable subquadratic scaling on the network depth, using Polyak-Lojasiewicz condition, and random matrix theory.

### Diversity and Generalization in Neural Network Ensembles

<https://arxiv.org/abs/2110.13786>

Provide sound answers to the following questions, how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms.

### Early-stopped neural networks are consistent

<https://arxiv.org/abs/2106.05932>

Show that gradient descent with early stopping achieves population risk arbitrarily close to optimal in terms of not just logistic and misclassification losses, but also in terms of calibration.

### Multiple Descent: Design Your Own Generalization Curve

<https://arxiv.org/abs/2008.01036>

Show that the generalization curve can have an arbitrary number of peaks, and the locations of those peaks can be explicitly controlled in variable parameterized families of models on linear regression. The emergence of double descnet is due to the interaction between the properties of the data and the inductive bias of learnin algorithms.

### Improved Regularization and Robustness for Fine-tuning in Neural Networks

<https://arxiv.org/abs/2111.04578>

Present a PAC-Bayes generalization bound that depends on the distance traveled in each layer during fine-tuning and the noise stability. 

### Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel

<https://arxiv.org/abs/2107.12723>

Show oracle type bounds which reveal that the generalisation and excess risk of GD is controlle by an interpolating network with the shortest GD path from inistialisation. Also show that this analysis is tightesr then existing NTK-based risk bounds, and show that GD with early stoppping is constant.

### On a Sparse Shortcut Topology of Artificial Neural Networks

<https://arxiv.org/abs/1811.09003>

Propose new shortcut architecture, and show that it can approximate any univariate continuous function in width-bounded setting, and show the generalization bound.

### ReLU Neural Networks of Polynomial Size for Exact Maximum Flow Computation

<https://arxiv.org/abs/2102.06635>

Introduce the concept of Max-Affine Arithmetic Programs, and use them to show that undirected graph's minimum spanning tree and maximum flow computation is possible with NNs of cubic/quadratic width.

### The Three Stages of Learning Dynamics in High-Dimensional Kernel Methods

<https://arxiv.org/abs/2111.07167>

Study the training dynamics of gradient flow on kernel least-squares objective, which is limiting dynamics of SGD trained NNs. 

### Assessing Deep Neural Networks as Probability Estimators

<https://arxiv.org/abs/2111.08239>

Find that the likelihood probability density and the inter-categorical sparsity have greater impacts than the prior probability to DNN's classification uncertainty.

### Towards Understanding the Condensation of Neural Networks at Initial Training

<https://arxiv.org/abs/2105.11686>

Empirically, it is observed that input weights condense on isolated orientation with a small initialization. Show that maximal number of condensed orientation in the initial stage is twice the multiplicity of the acitvation function, where multiplicity is multiple roots of activation function at origin. 

### Depth Without the Magic: Inductive Bias of Natural Gradient Descent

<https://arxiv.org/abs/2111.11542>

Gradient descent has implicit inductive bias, that the parameterization gives different optimization trajectory. Natural gradient descent is approximately invariant to such parameterization, giving same trajectory and same minimum. Show that there exist learning problem where natural gradient descent fails to generalize while gradient descent performs well.

### Improved Fine-tuning by Leveraging Pre-training Data: Theory and Practice

<https://arxiv.org/abs/2111.12292>

Show that final prediction precision may have a weak dependency on the pre-trained model especially in the case of large training terations. Shows that the final performance can be improved when appropriate pre-training data is included in fine-tuning, and design a novel selection strategy to select a subset from pre-training data to help improve the generalization.

### The staircase property: How hierarchical structure can guide deep learning

<https://arxiv.org/abs/2108.10573>

Defines a staircase property for functions over the boolean hypercube, which posits that high-order Fourier coefficients are reachable from low-order Fourier coefficients along increasing chains. Prove that functions with staircase property can be learned in polynomial time using layerwise stochastic coordinate descent on regular neural network. 

### Gradient Starvation: A Learning Proclivity in Neural Networks

<https://arxiv.org/abs/2011.09468>

Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. Using tools from dynamical systems theory, identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data.

### Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks

<https://arxiv.org/abs/2111.12963>

Derive error bounds in Lebesgue and Sobolev norms to approximate arbitrary matrix-vector product using ReLU NN.

### Implicit Data-Driven Regularization in Deep Neural Networks under SGD

<https://arxiv.org/abs/2111.13331>

Analyze evolutions of weight matrices' spectra, and they are classified to Marchenko-Pastur, Marchenko-Pastur with few bleeding outliers, Heavy tailed spectrum. These are connected to the degree of regularization, and argue that degree depends on the quality of data.

### On Linear Stability of SGD and Input-Smoothness of Neural Networks

<https://arxiv.org/abs/2105.13462>

Show that SGD tends to impose constraints on high-order moments of the gradient noise, by a linear analysis of SGD aroung global minima. Identify Sobolev regularization effect of SGD, that SGD regularizes the Sobolev seminorms of the model functions w.r.t. the input data.

### On the rate of convergence of a classifier based on a Transformer encoder

<https://arxiv.org/abs/2111.14574>

The rate of convergence of the misclassification probability towards the optimal misclassification probability, and shown that this classifier is able to circumvent the curse of dimensionality.

### The Geometric Occam's Razor Implicit in Deep Learning

<https://arxiv.org/abs/2111.15090>

Over-parameterized neural networks trained with SGD are subject to a Geometric Occam's Razor, that they are implicitly regularized by the geometric model complexity, which is a Dirichlet energy of the function.

### Embedding Principle: a hierarchical structure of loss landscape of deep neural networks

<https://arxiv.org/abs/2111.15527>

Prove a general embedding principle of loss landscape of DNNs that unravels a hierarchical structure of the loss landscape of NNs, loss landscape of an NN contains all critical points of all the narrower NNs. Provide a gross estimate of the dimension of critical submanifolds embedded from critical points of narrower NNs. Prove an irreversibility property of any critical embedding.

### Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent Flows

<https://arxiv.org/abs/2112.01363>

Design gradient based optimization for achieving acceleration, by first leveraging a continuous time framework for designing fixed-time stable dynamical systems, provigding a consistent discretization strategy such that the equiavlent discrete-time algorithm tracks the optimizer in a practically fixed number of iterations. Provide the convergence behavior of the proposed gradient flow and robustness to additive disturbances. 

### Asymptotic properties of one-layer artificial neural networks with sparse connectivity

<https://arxiv.org/abs/2112.00732>

Asymptotic of empirical distribution of parameters of a one-layer ANNs with sparse connectivity, with increasing number of parameter and iteration steps.

### Test Sample Accuracy Scales with Training Sample Density in Neural Networks

<https://arxiv.org/abs/2106.08365>

Propose an error function for piecewise linear NNs taking a local region of input space and smooth empirical training error, that is an average of empirical training erros from other regions weighted by network represenation distance. A bound on the expected smooth error for each region scales inversely with training sample desnsity in representation space.

### On transversality of bent hyperplane arrangements and the topological expressiveness of ReLU neural networks

<https://arxiv.org/abs/2008.09052>

Define notion of a generic transversal ReLU neural network, and show that almost all ReLU networks are generic and transversal. Using the obstruction, prove that a decision region of a generic, transversal ReLU network with a single hidden layer of dimension n+1 can have no more than one bounded connected components.

### Continuous vs. Discrete Optimization of Deep Neural Networks

<https://arxiv.org/abs/2107.06608>

Find that the degree of approximation of gradient descent on gradient flow depends on the curvature around the gradient flow trajectory. Show that over DNNs with homogeneous activations, gradient flow trajectories enjoy favorable curvature, that they are well approximated by gradient descent.

### A Complete Characterisation of ReLU-Invariant Distributions

<https://arxiv.org/abs/2112.06532>

Give a complete characterization of families of probability distributions that are invariant under the action of ReLU NN layers, proving that no invariant parameterised can exist unless one of follwoing holds, network's width is one, probability measure have finite support, and the parameterization is not locally Lipscitz continuous.

### On the Expected Complexity of Maxout Networks

<https://arxiv.org/abs/2107.00379>

Number of activation regions are used as a complexity measure, and it has shown that practical complexity of Deep ReLU networks is often far from the theoretical maximum. Show that this also occurs in maxout activation, and give nontrivial lower bounds on the complexity, finally gives that different initialization can increase speed of convergence.

### The Power of Contrast for Feature Learning: A Theoretical Analysis

<https://arxiv.org/abs/2110.02473>

By using connection between PCA and linear Autoencoder, GAN, contrast learning, show that contrastive learning outperforms autoender for both feature learning and downstream tasks.

### Neurashed: A Phenomenological Model for Imitating Deep Learning Training

<https://arxiv.org/abs/2112.09741>

Design a graphical model neurashed, which inherits hierarchically structured, optimized through SGD, and information evolving compressively, and enables insights into implicit regularization, information bottleneck, and local elasticity.

### Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaption

<https://www.arxiv.org/abs/2010.01184>

Covariate shift adaption usually suffer from small effective sample size, which is common in high dimensional setting. Focus on unified view connecting ESS, data dimensionality, and generalization in covariate shift adaption, and demonstrate how dimensionality reduction or feature selection increase the ESS. 

### A mean-field optimal control formulation of deep learning

<https://link.springer.com/article/10.1007/s40687-018-0172-y>

Introduces the mathematical formulation of viewing population risk minimization as mean-field optimal control problem, and prove stability condition of the Hamilton-Jacobi-Bellman type and Pontryagin type. By mean-field Pontryagin's maximum principle, establish quantitative relationships between population and empirical learning problem.

### Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantics Segmentation

<https://www.arxiv.org/abs/2103.11594>

Even with extremely noisy label on semantic segmentation, DNN still provide similar segmenetation performance as trained with original ground truth, ndicating that DNNs learn structures hidden in labels rather than pixel-level labels. Referring this structure as meta-structure, define this formally as spatial density distribution showing both theoretically and experimentally how this explains the behavior.

### An unfeasiability view of neural network learning

<https://www.arxiv.org/abs/2201.00945>

Define notion of a continuously differentiable perfect learning algorithm, and show that such algorithms don't exist given that length of the data set exceeds the number of involved parameters, with logistic, tanh, sin activation.

### A Kernel-Expanded Stochastic Neural Network

<https://www.arxiv.org/abs/2201.05319>

Design new architecture which incorporates support vector regression at its first layer, allowing to break the high-dimensional nonconvex training of neural network to series of low-dimensional convex optimization, and can be trained using imputation-regularized optimization, with a theoretical guarantee to global convergence.

### The Many Faces of Adversarial Risk

<https://www.arxiv.org/abs/2201.08956>

Make the definition of adversarial risk rigorous, generalize Strassen's theorem to unbalanced optimal transport setting, show the pure Nash equilibrium between adversary and algorithm, and characterize adversarial risk by the minimum Bayes error between a pair of distributions to the infinity Wasserstein uncertainty sets.

### Post-training Quantization for Neural Networks with Provable Guarantees

<https://www.arxiv.org/abs/2201.11113>

Modify GPFQ, a post-trainig NN quantization method based on greedy path following, and prove that for quantizing a single-layer network, the relative square error decys linearly in the number of weights.

### The Implicit Bias of Benign Overfitting

<https://www.arxiv.org/abs/2201.11489>

Show that benign overfitting, where a predictor perfectly fits noisy training data while having low expected loss, is biased towards to certain types of problems, so that it is not general behaviors. In classification setting, prove that the max-margin predictor is asymptotically biased towards minimizing the expected squared hinge loss.

### Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks

<https://www.arxiv.org/abs/2201.11729>

By dynamical system approach, analyzes the implicit regularization in hierarchical tensor factorization, establish the implicit regularization towards low rank, which translates to locality.

### Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications

<https://www.arxiv.org/abs/2111.12143>

Define criticality by partial Jacobian, which is jacobian between preactivations in different layers, and derive recurrence relation between norms of partial Jacobians with analyzing criticality.

### Fluctuations, Bias, Variance & Ensembles of Learners: Exact Asymptotics for Convex Losses in High-Dimension

<https://www.arxiv.org/abs/2201.13383>

Provide a complete description of the asymptotic joint distribution of the empirical risk minimizers for generic convex loss and regularisation in the high dimensional limit. 

### Interplay between depth of neural networks and locality of target functions

<https://www.arxiv.org/abs/2201.12082>

Introduce k-local and k-global functions, and find that depth is beneficial for learning local functions but detrimental to learning global functions.

### Deep Contrastive Learning is Provably (almost) Principal Component Analysis

<https://www.arxiv.org/abs/2201.12680>

Show that contrastive learning has a game-theoretical formulation, where max-player maximizes contrastiveness, min-player puts weights on pairs of samples with similar representation. Show that max player reduces to PCA for deep linear networks, with all local minima as global minima. This is also extended to 2-layer ReLU networks, and prove that feature composition is preferred then single dominant feature under strong augmentation.

### Implicit Regularization Towards Rank Minimization in ReLU Networks

<https://www.arxiv.org/abs/2201.12760>

Prove that GF on ReLU networks may no longer tend to minimize ranks, while revealing that ReLU networks of sufficient depth are provably biased towards low-rank solution.

### Spectral Analysis and Fixed Point Stability of Deep Neural Dynamics

<https://www.arxiv.org/abs/2011.13492>

Analyze the eigenvalue spectra and stability of discrete-time dynamics systems parameterized by DNNs, viewing neural network as affine parameter varying maps, and analyze using classical system methods. 

### Deep Layer-wise Networks Have Closed-Form Weights

<https://www.arxiv.org/abs/2202.01210>

Show that layer-wise network, which trains one layer at a time, has a closed form weight given by kernel mean embedding with global optimum. 

### Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks

<https://www.arxiv.org/abs/2105.05553>

In over-parameterized deep linear network with enough width, the convergence rate of parameters is exponentially fast along the larger principal components of the data, with rate governed by the singular value, named principal-component bias. Discuss how this may explain benefits of early stopping and why deep networks converge slowly with random labels.

### The Implicit Bias of Gradient Descent on Generalized Gated Linear Networks

<https://www.arxiv.org/abs/2202.02649>

Derive infinite-time training limit of a mathematically tractable class of deep nonlinear neural networks, gated linear networks, and generalize to gated networks described by general homogeneous polynomials. Using this, show how architectural constraints and implicit bias affect performance, and that theory captures a substantial portion of the inductive bias of ReLU networks.

### Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path

<https://www.arxiv.org/abs/2106.02073>

Neural Collapse behavior, that last layer features collapse to class-mean, also happens in MSE loss, which is easier to analyze compared to CE loss. Using this, decompose MSE loss to two terms, where one term is directly related to NC. Using this, introduce the central path where the linear classifier stays MSE-optimal for feature, and study the renormalized gradient flow about the central path, and derive the exact dynamics predicting NC.

### Benign Overfitting in Two-layer Convolutional Neural Networks

<https://www.arxiv.org/abs/2202.06526>

Show that when the signal-to-noise ratio satisfies a certain condition, a two-layer CNN with gradient descent arbitrary small training and test loss, giving benign overfitting. Conversely if this condition does not hold, the CNN only achieve constant level test loss, giving harmful overfitting.

### A spectral-based analysis of the separation between two-layer neural networks and linear methods

<https://www.arxiv.org/abs/2108.04964>

Propose a spectral based approach to analyze how two-layer neural networks separate from linear methods. This can be reduced to estimating the Kolmogorov width of two-layer neural networks, which can be characterized using the spectrum of an associated kernel. This allows upper bound, lower bound, and identifying explicit hard functions, and systematic study of choice of activation's effect on the separation.

### How and what to learn:The modes of machine learning

<https://www.arxiv.org/abs/2202.13829>

Propose a new approach named weight pathway analysis, which decomposes a neural network into a series of subnetworks of weight pathways. Using WPA, discover that a neural network stores and utilizes information in a holographic way, that the network encodes all training samples in a coherent structure. Also reveal two learning mode of a neural newtwork, linear and nonlinear, where the former extracts linearly separable features, and the latter extracts linearly inseparable features.

### Phenomenology of Double Descent in Finite-Width Neural Networks

<https://www.arxiv.org/abs/2203.07337>

Study the population loss with its lower bound using influence functions, which connetcs the spectrum of the Hessian at the optimum, and exhibit a double descent behaviour at the interpolation threshold.

### An error analysis of generative adversarial networks for learning distributions

<https://www.arxiv.org/abs/2105.13010>

Establish the convergence rate of GANs under collection of integral probability metrics defined through Hoelder class lke Wasserstein distance. Also show that GANs are able to adaptively learn data distributions with low-dimensional structures or have Hoelder densities, with proper architecture. In particular, show that for low-dimensional structure, the convergence rate depends on intrinsic dimension, not ambient dimension.

### Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data

<https://www.arxiv.org/abs/2010.03622>

Under 'expansion' assumption that low probability subset of data must expand to a neighborhood with large probability relative to the subset, prove that the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy w.r.t. ground-truth labels. Also provide generalization bound and sample complexity guarantee for neural nets.

### Deep Learning meets Nonparameteric Regression: Are Weight-Decayed DNNs Locally Adaptive?

<https://www.arxiv.org/abs/2204.09664>

Using parallel NN variant of ReLU networks, show that the standard weight decay is equivalent to promoting lp-sparsity of the coefficient vector. Using this equivalence, establish that by tuning only the weight decay, such parallel NN achieves an estimation error arbitrarily close to the minimax rates for both Besov and BV classes.

### An Unconstrained Layer-Peeled Perspective on Neural Collapse

<https://www.arxiv.org/abs/2110.02796>

Introduce a surrogate model called the uncontrained layer-peeled model, and prove that gradient flow on this model converges to critical points of a minimum norm separation problem exhibiting neural collapse. Show that this model with cross-entropy loss has a benign global landscape, allowing to prove that all the critical points are strict saddle points except the global minimizers exhibiting the neural collpase.

### On the Implicit Bias Towards Minimal Depth of Deep Neural Networks

<https://www.arxiv.org/abs/2202.09028>

Study the implicit bias of SGD to favor low-depth solutions when training deep neural networks. Empirically found that neural collapse appears even in intermediate layers, and strengthens when increasing the number of layers, which is evidence of low-depth bias. Characterize notion of effective depth by measuring the minimial layer enjoying neural collapse, and show that effective depth monotonically increases when training with extended portions of random labels.

### Sahllow Univariate ReLu Networks as Splines: Initialization, Loss Surface, Hessian, & Gradient Flow Dnamics

<https://www.arxiv.org/abs/2008.01772>

Reparameterize the ReLU NN as continuous piecewise linear spline, and study the learning dynamics of univariate ReLU NN with this spline view. Develop a simple view of the structure of the loss surface including critical, fixed points and Hessian. Also show that standard initialization gives very flat function.

### Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks

<https://www.arxiv.org/abs/1906.04267>

Propose a system for calculating the scaling constant for layers and weights. Argue that the network is preconditioned by the scaling, an argue that geometric mean of fan-in and fan-out should be used for initialization of the variance of weights.

### Why GANs are overkill for NLP

<https://www.arxiv.org/abs/2205.09838>

Show that, while it seems that maximizing likelihood is different than minimizing distinguishability criteria, this distinction is artifical and only holds for limited models. And show that minimizing KL-divergence is a more efficient approach to effectively minimizing the same distinguishability.

### The Gaussian equivalence of generative models for learning with shallow neural networks

<https://www.arxiv.org/abs/2006.14709>

Establish rigorous conditions for the Gaussian Equivalence between single layer neural network and Gaussian models, with convergence rate. Use this equivalence to derive a closed set of equations of generalization performance of two-layer neural network trained with SGD or full batch pre-learned feature. 

### Quasi-Equivalence of Width and Depth of Neural Networks

<https://www.arxiv.org/abs/2002.02515>

Formulate two transforms for mapping an arbitrary ReLU network to a wide network and a deep network respectively, for either regression or classification. 

### Adversarial Noises Are Linearly Separable for (Nearly) Random Neural Networks

<https://www.arxiv.org/abs/2206.04316>

Prove that the adversarial noises crafted by one-step gradient methods are linearly separable, for a two-layer network. The proof idea is to show that the label infromation can be efficiently propagated to the input while keeping the linear separability.