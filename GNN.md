### Stability and Generalization Capabilities of Message Passing Graph Neural Networks

<https://www.arxiv.org/abs/2202.00645>

In graph classification where graph is sampled from different random graph model, derive a non-asymptotic bound on the generalization gap between the empirical and statistical loss, which decreases to zero as the graphs become larger. 

### Constant Time Graph Neural Networks

<https://www.arxiv.org/abs/1901.07868>

GNNs approximate huge graph by sampling the node, and this paper proves whether the query complexity for node sampling is constant time, for different activation, architecture, and forward/backward.

### Stability and Generalization Capabilities of Message Passing Graph Neural Networks

<https://www.arxiv.org/abs/2202.00645>

Assuming some random graph models, derive a non-asymptotic bound on the generalization gap between the empirical loss and statistical loss for NPMM, which decreases to zero as the graphs become larger.

### Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks

<https://www.arxiv.org/abs/2006.13866>

Theoretically analyze the variance of sampling methods and show that, due to the composite structure of empirical risk, the variance of any sampling method can be decomposed into embedding approximation variance in the forward stage and stochastic gradient variance in the backward stage. Propose a decoupled variance reduction strategy.

### Analyzing the expressive power of graph neural networks in a spectral perspective

<https://www.researchgate.net/publication/349119879_ANALYZING_THE_EXPRESSIVE_POWER_OF_GRAPH_NEURAL_NETWORKS_IN_A_SPECTRAL_PERSPECTIVE>

By bridging the gap between the spectral and spatial design of graph convolutions, theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain.

### Asymptotics of l2 Regularized Network Embeddings

<https://www.arxiv.org/abs/2201.01689>

Study effects of regularization on embedding in unsupervised random walk, and prove that under exchangeability assumption on the graphs, it leads to learning a nuclear-norm type penalized graphon. In particular, the exact form of penalty depends on the choice of subsampling method used.

### Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks

<https://www.arxiv.org/abs/1810.02244>

Relate Graph Neural Network to 1-dimensional Weisfeiler-Leman graph isomorphism heuristics, show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic graphs, with same shortcomings. Propose a generalization of GNNs so-called k-dimensional GNNs that can take higher-order graph structures.

### How Powerful are Graph Neural Networks?

<https://www.arxiv.org/abs/1810.00826>

Characterize the discriminative power of GNN variants like Graph Convolution Networks or GraphSAGE, and show that they cannot distinguish certain simple graph structures, and develop provably most expressive architecture, which is as powerful as the Weisfeiler-Lehman graph isomorphism test.

### The Surprising Power of Graph Neural Networks with Random Node Initialization

<https://www.arxiv.org/abs/2010.01179>

Analyze the expressive power of GNNs with Random Node Initialization, prove that these models are universal.

### Transferability Properties of Graph Neural Networks

<https://www.arxiv.org/abs/2112.04629>

For the setting of training graph on moderate size and testing on large graphs, use the graph limit of graphons, and define graph filter and graphon filter to formulate graph/graphon convolution neural network. Using this formulation, bound the error of transferring in same graphon. Show that tranference error decreases with graph size, and graph filters have a transferability-discriminiability tradeoff.

### Graph Neural Networks Are More Powerful Than we Think

<https://www.arxiv.org/abs/2205.09801>

Despite the previous result on limitation of expressivity of GNN by WL algorithm, but alternatively show that this is only the case when the input vector is the vector of all ones. Rather, show that GNNs can distinguish between any graphs that differ in at least one eigenvalue.

### Generalization Analysis of Message Passing Neural Networks on Large Random Graphs

<https://www.arxiv.org/abs/2202.00645>

Show that when training a MPNN on a dataset from random graph models, the generalization gap increases in the complexity of the MPNN, and decreases by the number of samples and average number of nodes. 

### Graph Neural Network Sensitivity Under Probabilistic Error Model

<https://www.arxiv.org/abs/2203.07831>

Study the effect of a probabilistic graph error model on the performance of GCNs. Prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability.

### Implicit Bias of Linear Equivariant Networks

<https://arxiv.org/abs/2110.06084>

Show that L layer full width linear GCNNs trained via gradient descent in a binary classification task converge to solutions with low-rank Fourier matrix coefficients, regularized by the 2/L-Schatten matrix norm. This generalizes previous analysis on the implicit bias of linear CNNs to linear GCNNs over all finite groups, including the challenging setting of non-commutative symmetry groups.

### We Cannot Guarantee Safety: The Undecidability of Graph Neural Network Verification

<https://arxiv.org/abs/2206.05070>

Show that the graph classifier verification is undecidable, however the node classification is verifiable when degree of graph is restricted.

### Lower and Upper Bounds for Numbers of Linear Regions of Graph Convolutional Networks

<https://arxiv.org/abs/2206.00228>

Present the estimates for the number of linear regions of the GCNs, particularlay the optimal upper bound for one-layer GCN and both bounds for multi-layer case, where multi-layer has exponentially many regions than one-layer.