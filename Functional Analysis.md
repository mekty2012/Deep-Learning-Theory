### A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case

<https://www.arxiv.org/abs/1910.01635>

When Euclidean norm of the weight is bounded for infinite width network, show that L1 norm of Radon transform of (d+1)/2 power Laplacian is related, and show that any function in Sobolev space can be represented with bounded norm. 

### The Role of Neural Network Activation Functions

<https://www.arxiv.org/abs/1910.02333>

Through spline theory, study importance of other activations like leaky-ReLU, weight decay or path norm regularization, and skip connection, using relation between NN training and infninite dimensional optimization over Banach space whose solutions are well known to be fractional and polynomical splines, and the choice of Banach space depends on the choice of activation.

### Banach Space Representer Theorems for Neural Networks and Ridge Splines

<https://www.arxiv.org/abs/2006.05626>

In continuous domain linear inverse problems with total-variation-like-regularization in Radon domain, derive a representer theorem that finite width single layer NNs are solution.

### How do infinite width bounded norm networks look in function space?

<https://www.arxiv.org/abs/1902.05040>

Show that minimal network norm for representing f with bounded norm is max(int |f''|dx, |f'(-\infty) + f'(+\infty)|), so minimal norm fit is linear spline interpolation.