### A Convergence Theory for Deep Learning via Over-Parametrization

<https://www.arxiv.org/abs/1811.03962>

When network is overparametrized, SGD can find global minima in polynomial time.

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks

<https://www.arxiv.org/abs/1810.02054>

In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

## Deep learning generalizes because the parameter-function map is biased towards simple functions

<https://www.arxiv.org/abs/1805.08522>

Using probability-complexity bound from algorithmic information theory, parameter-function map of many DNNs should be exponentially biased towards simple functions.

### Learning Overparametrized Neural Networks via Stochastic Gradient Descent on Structured Data

<https://www.arxiv.org/abs/1808.01204>

Prove that when the data comes from mixtures of well-separated distributions SGD learns a two-layer overparameterized ReLU-network with a small generalization error, even though the network can fit arbitrary labels.

### Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

<https://www.arxiv.org/abs/1901.08584>

Analyze training and generalization, give (1) why random label gives slower training (2) generalization bound independent of network size (3) learnability of a broad class of smooth functions.

### Generalization bounds for deep learning

<https://www.arxiv.org/abs/2012.04115>

Introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Focuse on generalization error upper bound, and inttroduce a categorisation of bounds depending on assumptions on the algorithm and data. 

### Stronger generalization bounds for deep nets via a compression approach

<https://arxiv.org/abs/1802.05296>

Use an explicit and efficient compression, which yields generalization bounds via a simple compression-based framework, and provide some theoretical justification for widespread empirical success in compressing deep nets.

### Generalization Bounds For Meta-Learning: An Information-Theoretical Analysis

<https://arxiv.org/abs/2109.14595>

Derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms.

### VC dimension of partially quantized neural networks in the overparameterized regime

<https://arxiv.org/abs/2110.02456>

Focus hyperplane arrangement neural networks, and show that HANNs can have VC dimension significantly smaller than the number of weights while being highly expressive.

### Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning

<https://arxiv.org/abs/2105.14099>

Relaxing the assumption that distribution of observed task and target task is equal, develop two PAC-Bayes bounds for the few-shot learning setting, thereby bridging the gap between practice and PAC_Bayesian theories.

### A generalization gap estimation for overparameterized models via Langevin functional variance

<https://arxiv.org/abs/2112.03660>

Show that a functional variance characterizes the generalization gap even in overparameterized settings. Propose a computationally efficient approximation of the function variance, a Langevin approximation of the functional variance.

### Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs

<https://www.arxiv.org/abs/2006.16144>

Provide upper bound on the generalization error of PINNs approximating solutions of the forward problem for PDEs.

### PACMAN: PAC-style bounds accounting for the Mismatch between Accuracy and Negative log-loss

<https://arxiv.org/abs/2112.05547>

Introduce an analysis based on point-wise PAC approach over the generalization error accounting mismatch of training loss and test accuracy.

### Differentiable PAC-Bayes Objectives with Partially Aggregated Neural Networks

<https://arxiv.org/abs/2006.12228>

Show how averaging over an ensembles of stochastic neural networks enables new partially-aggregated estimators, leading provably lower-varaince to gradietn estimates, and reformulate a PAC-Bayesian bound to derive optimisable differentiable objective. 

### How Much Over-parameterization is Sufficient to Learn Deep ReLU Networks?

<https://www.arxiv.org/abs/1911.12360>

Show the optimization and generalization under polylogarithmic width network w.r.t. n and epsilon^-1.