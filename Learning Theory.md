### A Convergence Theory for Deep Learning via Over-Parametrization

<https://www.arxiv.org/abs/1811.03962>

When network is overparametrized, SGD can find global minima in polynomial time.

### Gradient Descent Provably Optimizes Over-parametrized Neural Networks

<https://www.arxiv.org/abs/1810.02054>

In two-layer FC ReLU NN, if width is large enough and no two inputs are parallel, gradient descent converges to a globally optimal solution at a linear convergence rate.

## Deep learning generalizes because the parameter-function map is biased towards simple functions

<https://www.arxiv.org/abs/1805.08522>

Using probability-complexity bound from algorithmic information theory, parameter-function map of many DNNs should be exponentially biased towards simple functions.

### Learning Overparametrized Neural Networks via Stochastic Gradient Descent on Structured Data

<https://www.arxiv.org/abs/1808.01204>

Prove that when the data comes from mixtures of well-separated distributions SGD learns a two-layer overparameterized ReLU-network with a small generalization error, even though the network can fit arbitrary labels.

### Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks

<https://www.arxiv.org/abs/1901.08584>

Analyze training and generalization, give (1) why random label gives slower training (2) generalization bound independent of network size (3) learnability of a broad class of smooth functions.

### Generalization bounds for deep learning

<https://www.arxiv.org/abs/2012.04115>

Introduce desiderata for techniques that predict generalization errors for deep learning models in supervised learning. Focuse on generalization error upper bound, and inttroduce a categorisation of bounds depending on assumptions on the algorithm and data. 

### Stronger generalization bounds for deep nets via a compression approach

<https://arxiv.org/abs/1802.05296>

Use an explicit and efficient compression, which yields generalization bounds via a simple compression-based framework, and provide some theoretical justification for widespread empirical success in compressing deep nets.

### Generalization Bounds For Meta-Learning: An Information-Theoretical Analysis

<https://arxiv.org/abs/2109.14595>

Derive a novel information-theoretic analysis of the generalization property of meta-learning algorithms.

### VC dimension of partially quantized neural networks in the overparameterized regime

<https://arxiv.org/abs/2110.02456>

Focus hyperplane arrangement neural networks, and show that HANNs can have VC dimension significantly smaller than the number of weights while being highly expressive.

### Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning

<https://arxiv.org/abs/2105.14099>

Relaxing the assumption that distribution of observed task and target task is equal, develop two PAC-Bayes bounds for the few-shot learning setting, thereby bridging the gap between practice and PAC_Bayesian theories.

### A generalization gap estimation for overparameterized models via Langevin functional variance

<https://arxiv.org/abs/2112.03660>

Show that a functional variance characterizes the generalization gap even in overparameterized settings. Propose a computationally efficient approximation of the function variance, a Langevin approximation of the functional variance.

### Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs

<https://www.arxiv.org/abs/2006.16144>

Provide upper bound on the generalization error of PINNs approximating solutions of the forward problem for PDEs.

### PACMAN: PAC-style bounds accounting for the Mismatch between Accuracy and Negative log-loss

<https://arxiv.org/abs/2112.05547>

Introduce an analysis based on point-wise PAC approach over the generalization error accounting mismatch of training loss and test accuracy.

### Differentiable PAC-Bayes Objectives with Partially Aggregated Neural Networks

<https://arxiv.org/abs/2006.12228>

Show how averaging over an ensembles of stochastic neural networks enables new partially-aggregated estimators, leading provably lower-varaince to gradietn estimates, and reformulate a PAC-Bayesian bound to derive optimisable differentiable objective. 

### How Much Over-parameterization is Sufficient to Learn Deep ReLU Networks?

<https://www.arxiv.org/abs/1911.12360>

Show the optimization and generalization under polylogarithmic width network w.r.t. n and epsilon^-1.

### Approximation bounds for norm constrained neural networks with applications to regression and GANs

<https://www.arxiv.org/abs/2201.09418>

Prove approximation capacity of ReLU NN with norm constraint on the weights, especially upper and lower bound of approximation error of smooth function class, where lower bound comes from Rademacher complexity. Using this bounds, analyze convergence of regression and distribution estimation by GANs.

### Weight Expansion: A New Perspective on Dropout and Generalization

<https://www.arxiv.org/abs/2201.09209>

Define weight expansion which is the signed volume of a parallelotope spanned by column or row vectors of the weight covariance matrix, show that weight expansion is an effective means of increasing the generalization in a PAC Bayesian setting, and prove that dropout leads to weight expansion.

### Generalization Error Bounds on Deep Learning with Markov Datasets

<https://www.arxiv.org/abs/2201.11059>

Derive upper bounds on generalization errors for deep NNs with Markov datasets, based on Koltchinskii and Panchenko's approach for bounding the generalization error of combined classifiers. 

### Stability and Generalization Capabilities of Message Passing Graph Neural Networks

<https://www.arxiv.org/abs/2202.00645>

In graph classification where graph is sampled from different random graph model, derive a non-asymptotic bound on the generalization gap between the empirical and statistical loss, which decreases to zero as the graphs become larger. 

### Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks

<https://www.arxiv.org/abs/1910.02934>

Show that gradient descent's solution consistutes small subset of entire function class, however is sufficiently large to guarantee small training error. Also gives generalization gap that is logarithmic to depth.