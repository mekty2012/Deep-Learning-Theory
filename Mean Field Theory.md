### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

## Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
in such networks.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

