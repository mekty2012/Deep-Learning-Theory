### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

## Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
in such networks.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

### Limiting fluctuations and trajectorial stability of multilayer neural networks with mean field training

<https://arxiv.org/abs/2110.15954>

Leveraging on the neuronal embedding framework, systemically derive a system of dynamical equations called second-order MF limit, capturing the limiting fluctuation distribution. Applying this result, show a stability property of gradient descent MF training, that training trajectory progressively biases towards a solution with minimal fluctuation.

### Mean-field Analysis of Piecewise Linear Solutions for Wide ReLU Networks

<https://arxiv.org/abs/2111.02278>

In mean-field view, shwo that at convergence, the ReLU network implements a piecewise linear map with at most three 'knot' points, points where the tangent of the ReLU network estimator changes. 

### Mean-Field and Kinetic Descriptions of Neural Differential Equations

<https://arxiv.org/abs/2001.04294>

Analyze steady states and sensitivity w.r.t. the parameters of the ResNets, where the study of moments provides insights on the choice of the parameters. Modification of the microscopic dynmaics leads to a Fokker-Planck formulation of the network, where the concept of network training is replaced by the task of fitting distributions.

### Overparameterization of deep ResNet: zero loss and mean-field analysis

<https://arxiv.org/abs/2105.14417>

Using a mean-field-limit, prove that the gradient descent becomes a gradient flow for a probability distribution characterized by a PDE in the large NN limit. And show that the solution converges to a zero-loss solution, suggesting that the training of the ResNet gives a near-zero loss. 

### Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification

<https://arxiv.org/abs/2006.06098>

Analyze the learning dynamics of SGD for a single-layer NN classifying a high-dimensional Gaussian mixture in binary classification. Define a particular stochastic process where SGD is modelled by stochastic gradient flow, then apply dynamical mean-field theory to track the dynamics via a self-consistent stochastic process.

### On the Global Convergence of Gradient Descent for multi-layer ResNets in the mean-field regime.

<https://arxiv.org/abs/2110.02926>

Study global convergence of gradient descent in ResNet using mean-field analysis, translating the training process by gradient flow PDE and examine the convergence properties. Show that sufficiently large ResNet can be globally minimized by first-order optimization methods.

### On the selection of Initialization and Activation Function for Deep Neural Networks

<https://www.arxiv.org/abs/1805.08266>

Show that for a class of ReLU-like activation functions, the information propagates deeper for an initialization at the edge of chaos.

### Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach

<https://www.arxiv.org/abs/1806.01316>

Investigate the asymptotic statistics of the FIM's eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value. So it is locally flat in most dimensions, but strongly distorted in others.

### Unified Field Theory for Deep and Recurrent Neural Networks

<https://arxiv.org/abs/2112.05589>

Present a unified and systematic derivation of the mean-field theory for both architectures from first principles by employing established methods from statistical physics. While mean-field equations are different to temporal structure, yet yield identical Gaussian kernels when readouts are taken. Find that convergence towards the mean-field theory is slower for recurrent networks than for deep networks, which depends non-trivally on the parameters of the weight priors and depth, time steps. 

### Global Convergence of ResNets: From fininte to infinite width using linear parameterization

<https://arxiv.org/abs/2112.05531>

Design ResNet with residual blok having linear parameterization, which admits infinite width/depth limit. Prove a local Polyak-Lojasiewicz inequality, thus every critical point is a global minimizer and a local convergence result of GD holds, similar to lazy regime.

### Mean Field Analysis of Neural Networks: A Law of Large Numbers

<https://www.arxiv.org/abs/1805.01053>

Prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation, considered as law of large numbers for neural network. As a consequence, show that trained parameters asymptotically become independent called as propagation of chaos.

### Mean Field Analysis of Neural Networks: A Central Limit Theorem

<https://www.arxiv.org/abs/1808.09372>

Describes the neural network's fluctuations around its mean-field limit, haiving a Gaussian distribution and satisfy a stochastic partial differential equation. 

### A Mean Field View of the Landscape of Two-Layers Neural Networks

<https://www.arxiv.org/abs/1804.06561>

Prove that SGD dynamics is captured by a certain non-linear PDE named as distributional dynamics, and show how DD can be used to prove convergence of SGD with ideal generalization error. This allows to average out some complexities of neural networks.

### Convergence of policy gradient for entropy regularized MDPs with neural network approximation in the mean-field regime

<https://www.arxiv.org/abs/2201.07296>

Study the policy gradient for infinite-horizon, continuous state/action space, entropy regularized Markov decision processes, and a softmax policy with one hidden layer neural network in mean-field regime. Show that objective function increases along the 2-Wasserstein metric gradient flow, and with enough regularization, the gradient flow exponentially fast to unique stationary solution.

### Particle Dual Averaging: Optimization of Mean Field Neural Networks with Global Convergence Rate Analysis

<https://www.arxiv.org/abs/2012.15477>

Propose particle dual averaging method which generalizes dual averaging in convex optimization to optimization over probability distribution, and establish global convergence in learning two-layer mean field neural networks.

### Replica mean field theory for the generalisation gap of deep neural networks

<https://www.arxiv.org/abs/2201.11022>

Employ replica mean field theory to compute generalization gap with quenched features, in teacher-student scenario and quadratic loss regression. 

### Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks

<https://www.arxiv.org/abs/2202.00293>

Investigate the cross-over between over-parameterized regime and narrow network in high-dimensional setting, study the interplay between the learning rate, time scale, number of hidden units, builds on deterministic description of SGD in high-dimensions from statsitical physics.

### A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth

<https://www.arxiv.org/abs/2003.05508>

Propose continuum limit of Deep ResNet as ODE where depth tends to infinity, whose local minimums are all global. 

### Extended critical regimes of deep neural networks

<https://www.arxiv.org/abs/2203.12967>

Extends the mean field theory for DNNs, and show that heavy-tailed weights enable the emergence of an extended critical regime without fine-tuning of parameters. In this extended critical regime, DNNs exhibit rich and complex propagation dynamics.