### Dynamical Isometry and a Mean Field Theory of RNNs

<https://www.arxiv.org/abs/1806.05394>

Use mean field theory and random matrix theory to develop a theory for signal propagation in RNNs. 

### A Mean Field Theory of Batch Normalization

<https://www.arxiv.org/abs/1902.08129>

Develop a mean field theory for batch normalization, show that batch normalization is cause of gradient explosion.

## Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks

<https://www.arxiv.org/abs/1806.05393>

Use mean field theory for signal propagation, characterize the conditions for dynamical isometry, train vanilla CNNs with 10,000 layers.

### Mean Field Residual Networks: On the Edge of Chaos

<https://www.arxiv.org/abs/1712.08969>

Show that adding skip connections allow the network to adopt subexponential dynamics, where usually it is exponential.

### A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off

<https://www.arxiv.org/abs/1906.00771>

Apply mean-field techniques to networks with quantized activations, to evaluate degrade in signal propagation, and derive initialization scheme that maximize signal propagation
in such networks.

### How to Initialize your Network? Robust Initialization for WeightNorm & ResNets

<https://www.arxiv.org/abs/1906.02341>

Use mean field approximation, define parameter initialization that avoids explosion or vanishment of information for weight normalized networks.

### Exponential expressivity in deep neural networks through transient chaos

<https://www.arxiv.org/abs/1606.05340>

Using Riemannian geometry with the mean field theory, study the nature of signal propagation in DNN. Show the chaotic phase networks computes nonlinear functions with curvature growing exponentially with depth, which can't be approximated by shallow network. 

### Limiting fluctuations and trajectorial stability of multilayer neural networks with mean field training

<https://arxiv.org/abs/2110.15954>

Leveraging on the neuronal embedding framework, systemically derive a system of dynamical equations called second-order MF limit, capturing the limiting fluctuation distribution. Applying this result, show a stability property of gradient descent MF training, that training trajectory progressively biases towards a solution with minimal fluctuation.

### Mean-field Analysis of Piecewise Linear Solutions for Wide ReLU Networks

<https://arxiv.org/abs/2111.02278>

In mean-field view, shwo that at convergence, the ReLU network implements a piecewise linear map with at most three 'knot' points, points where the tangent of the ReLU network estimator changes. 

### Mean-Field and Kinetic Descriptions of Neural Differential Equations

<https://arxiv.org/abs/2001.04294>

Analyze steady states and sensitivity w.r.t. the parameters of the ResNets, where the study of moments provides insights on the choice of the parameters. Modification of the microscopic dynmaics leads to a Fokker-Planck formulation of the network, where the concept of network training is replaced by the task of fitting distributions.

### Overparameterization of deep ResNet: zero loss and mean-field analysis

<https://arxiv.org/abs/2105.14417>

Using a mean-field-limit, prove that the gradient descent becomes a gradient flow for a probability distribution characterized by a PDE in the large NN limit. And show that the solution converges to a zero-loss solution, suggesting that the training of the ResNet gives a near-zero loss. 

### Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification

<https://arxiv.org/abs/2006.06098>

Analyze the learning dynamics of SGD for a single-layer NN classifying a high-dimensional Gaussian mixture in binary classification. Define a particular stochastic process where SGD is modelled by stochastic gradient flow, then apply dynamical mean-field theory to track the dynamics via a self-consistent stochastic process.

### On the Global Convergence of Gradient Descent for multi-layer ResNets in the mean-field regime.

<https://arxiv.org/abs/2110.02926>

Study global convergence of gradient descent in ResNet using mean-field analysis, translating the training process by gradient flow PDE and examine the convergence properties. Show that sufficiently large ResNet can be globally minimized by first-order optimization methods.
