## Noether's Learning Dynamics: Role of Symmetry Breaking in Neural Networks

<https://arxiv.org/abs/2105.02716>

Develop a theoretical framework studying the geometry of learning dynamics, reveal a key nechanism of explicit symmetry breaking behind the efficiency and stability. Model the discrete learning dynmaics of gradient descent using a continuous time Lagrangian formulation, and identify kinetic symmetry breaking, and generalize Neother's theorem to KSB and show how KSB introduces a mechanisms of implicit adaptive optimization.

### Understanding the Generalization Benefit of Model Invariance from a Data Perspective

<https://arxiv.org/abs/2111.05529>

Studies the generalization benefit of model invariance by introducing the sample cover induced by transformations. Proveide refined generalization bounds for invariant models based on the sample cover. 

### Exploiting Invariance in Training Deep Neural Networks

<https://arxiv.org/abs/2103.16634>

Introduce a feature transform techniques that imposes invariance properties. Enforce GL(n) invariance property with global statistics from a batch.

### Provably Strict Generalisation Benefit for Invariance in Kernel Methods

<https://arxiv.org/abs/2106.02346>

From function space perspective, derive non-zero generalization benefit of incorporating invariance in kernel ridge regression when target is invariant to the action of a compact group.

### Meta-Learning Symmetries by Reparameterization

<https://arxiv.org/abs/2007.02933>

Present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data, which can provably represent equivariance inducing parameter sharing for any group of symmetry transformations.

### On the Universality of Invariant Networks

<https://arxiv.org/abs/1901.09342>

Present two main results, any subgroup G of Sn are universal if high-order tensors are allowed, and there are groups that higher-order tensors are unavoidable for universality.

### Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks

<https://www.arxiv.org/abs/2201.07934>

Define adaptive symmetry, which characterizes invariance of variance, where a system explores different pathways of evolution with equal probabilibty, and complex functional structure emerges from accumulation of adaptive symmetry breaking. Characterize the optimization process of DNN system as an extended adaptive-symmetry breaking process.

### Data-Driven emergence of convolutional structure in neural networks

<https://www.arxiv.org/abs/2202.00565>

Show that emergence of pattern that FC network learns convolutional structure, is due to non-Gaussian higher-order local structure of inputs. Provide an analytical and numerical characterisation of the pattern formation mechanisms.

### Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations

<https://www.arxiv.org/abs/2202.10638>

Develop gradient-based method for selecting the data augmentation. Relies on phrasing data augmentation as an invariance in the prior distribution and learning it using Bayesian model selection. Use a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as objective.

### Learning Invariant Weights in Neural Networks

<https://www.arxiv.org/abs/2202.12439>

Following the previous approach that marginal likelihood offers a way to learn invariances in GP, propose a weight-space equivalent approach, by minimizing a lower bound on the marginal likelihood to learn invariances.