## Noether's Learning Dynamics: Role of Symmetry Breaking in Neural Networks

<https://arxiv.org/abs/2105.02716>

Develop a theoretical framework studying the geometry of learning dynamics, reveal a key nechanism of explicit symmetry breaking behind the efficiency and stability. Model the discrete learning dynmaics of gradient descent using a continuous time Lagrangian formulation, and identify kinetic symmetry breaking, and generalize Neother's theorem to KSB and show how KSB introduces a mechanisms of implicit adaptive optimization.

### Understanding the Generalization Benefit of Model Invariance from a Data Perspective

<https://arxiv.org/abs/2111.05529>

Studies the generalization benefit of model invariance by introducing the sample cover induced by transformations. Proveide refined generalization bounds for invariant models based on the sample cover. 

### Exploiting Invariance in Training Deep Neural Networks

<https://arxiv.org/abs/2103.16634>

Introduce a feature transform techniques that imposes invariance properties. Enforce GL(n) invariance property with global statistics from a batch.

### Provably Strict Generalisation Benefit for Invariance in Kernel Methods

<https://arxiv.org/abs/2106.02346>

From function space perspective, derive non-zero generalization benefit of incorporating invariance in kernel ridge regression when target is invariant to the action of a compact group.

### Meta-Learning Symmetries by Reparameterization

<https://arxiv.org/abs/2007.02933>

Present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data, which can provably represent equivariance inducing parameter sharing for any group of symmetry transformations.

### On the Universality of Invariant Networks

<https://arxiv.org/abs/1901.09342>

Present two main results, any subgroup G of Sn are universal if high-order tensors are allowed, and there are groups that higher-order tensors are unavoidable for universality.