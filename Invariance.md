## Noether's Learning Dynamics: Role of Symmetry Breaking in Neural Networks

<https://arxiv.org/abs/2105.02716>

Develop a theoretical framework studying the geometry of learning dynamics, reveal a key nechanism of explicit symmetry breaking behind the efficiency and stability. Model the discrete learning dynmaics of gradient descent using a continuous time Lagrangian formulation, and identify kinetic symmetry breaking, and generalize Neother's theorem to KSB and show how KSB introduces a mechanisms of implicit adaptive optimization.

### Understanding the Generalization Benefit of Model Invariance from a Data Perspective

<https://arxiv.org/abs/2111.05529>

Studies the generalization benefit of model invariance by introducing the sample cover induced by transformations. Proveide refined generalization bounds for invariant models based on the sample cover. 

### Exploiting Invariance in Training Deep Neural Networks

<https://arxiv.org/abs/2103.16634>

Introduce a feature transform techniques that imposes invariance properties. Enforce GL(n) invariance property with global statistics from a batch.

### Provably Strict Generalisation Benefit for Invariance in Kernel Methods

<https://arxiv.org/abs/2106.02346>

From function space perspective, derive non-zero generalization benefit of incorporating invariance in kernel ridge regression when target is invariant to the action of a compact group.

### Meta-Learning Symmetries by Reparameterization

<https://arxiv.org/abs/2007.02933>

Present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data, which can provably represent equivariance inducing parameter sharing for any group of symmetry transformations.

### On the Universality of Invariant Networks

<https://arxiv.org/abs/1901.09342>

Present two main results, any subgroup G of Sn are universal if high-order tensors are allowed, and there are groups that higher-order tensors are unavoidable for universality.

### Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks

<https://www.arxiv.org/abs/2201.07934>

Define adaptive symmetry, which characterizes invariance of variance, where a system explores different pathways of evolution with equal probabilibty, and complex functional structure emerges from accumulation of adaptive symmetry breaking. Characterize the optimization process of DNN system as an extended adaptive-symmetry breaking process.

### Data-Driven emergence of convolutional structure in neural networks

<https://www.arxiv.org/abs/2202.00565>

Show that emergence of pattern that FC network learns convolutional structure, is due to non-Gaussian higher-order local structure of inputs. Provide an analytical and numerical characterisation of the pattern formation mechanisms.

### Invariance Learning in Deep Neural Networks with Differentiable Laplace Approximations

<https://www.arxiv.org/abs/2202.10638>

Develop gradient-based method for selecting the data augmentation. Relies on phrasing data augmentation as an invariance in the prior distribution and learning it using Bayesian model selection. Use a differentiable Kronecker-factored Laplace approximation to the marginal likelihood as objective.

### Learning Invariant Weights in Neural Networks

<https://www.arxiv.org/abs/2202.12439>

Following the previous approach that marginal likelihood offers a way to learn invariances in GP, propose a weight-space equivalent approach, by minimizing a lower bound on the marginal likelihood to learn invariances.

### Steerable Partial Differential Operators for Equivariant Neural Networks

<https://www.arxiv.org/abs/2106.10163>

Derive G-steerability constraint that completely characterizes partial differential operators is equivariant, and fully solve the constraint for some important groups. Use this solution as equivariant drop-in replacement for convolution layers, and develop a framework for equivariant maps based on Schwartz distributions.

### A Classification of G-invariant Shallow Neural Networks

<https://www.arxiv.org/abs/2205.09219>

Prove a theorem that classifies all G-invariant single-hidden-layer neural network architectures with ReLU activations for any finite orthogonal group G. The proof is based on a correspondence of every G-SNN to a signed permutation representation of G acting on the hidden neurons, which is given in terms of the first cohomology classes of G. Finally prove that architectures with inequivalent cohomology classes in a given cohomology ring coincide only when their weight matrices are zero.

### On Non-Linear operators for Geometric Deep Learning

<https://www.arxiv.org/abs/2207.03485>

Study the operators mapping vector and scalar fields over a manifold that commutes with its group of diffeomorphisms. Prove that for scalar field, these operators correspond to point-wise non-linearities, while the vector fields only allows scalar multiplications. These justify the use of nonlinearities, however also indicates the diffeomorphism is too rich to motivate design over the symmetries of manifold.

### Deep Neural Network Approximation of Invariant Functions through Dynamical Systems

<https://www.arxiv.org/abs/2208.08707>

Study the approximation of functions which are invariant w.r.t. certain permutations, using flow maps. Prove sufficient condition for universal approximation, vieweing the abstraction of deep residual networks with symmetry constraints. 

### Implicit Bias of Linear Equivariant Networks

<https://arxiv.org/abs/2110.06084>

Show that L-layer full-width linear, group convolution neural networks trained with gradient descent on binary classification, converge to solutions with low-rank Fourier matrix coefficient. This generalize the results in CNN to G-CNN for all finite groups.