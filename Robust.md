### On the regularized risk of distributionally robust learning over deep neural networks

<https://www.arxiv.org/abs/2109.06297>

Using tools from optimal transport theory, derive first order and second order approximations to the distributionally robust problem in terms of appropriate regularized risk minimization problems. 

### The mathematics of adversarial attacks in AI -- Why deep learning is unstable despite the existence of stable neural networks

<https://www.arxiv.org/abs/2109.06098>

Show the mathematical paradox, that any training procedure with a fixed architecture will yield neural networks that are either inaccurate or unstable. The key is that the stable and accurate neural networks must have variable dimensions depending on the input.

### Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks

<https://www.arxiv.org/abs/2108.12001>

Provide a theoretical justification for the finding that adversarial training shrinks two important characteristics of the logit distribution: the max logit values and the logit gaps are on average lower for AT models. 

### On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks

<https://arxiv.org/abs/1901.10371>

Show that adversarial training tends to promote simultaneously low-rank and sparse structure. In the reverse direction, when the low rank structure is promoted by nclear norm regularization, neural networks show significantly improved robustness.

### Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks

<https://arxiv.org/abs/2110.03825>

Provide a theoretical analysis explaning on following observations, that 1) model parameters does not necessarily help adversarial robustness, 2) reducing capacity at the last stage of the network can actually improve adversarial robustness, and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness.

### Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks

<https://arxiv.org/abs/2002.11318>

Prove a quantitative trade-off between spatial and adversarial robustness in a simple statistical setting. 

### How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?

<https://arxiv.org/abs/2012.12896>

Provide a formal framework connecting the robustness of a network to the alignments between its architecture and target functions. Hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise.
