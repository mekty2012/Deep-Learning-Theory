### Global inducing point varaitional posteriors for Bayesian neural networks and deep Gaussian processes

<https://www.arxiv.org/abs/2005.08140>

Consider the optimal approximate posterior over the top-layer weights, and show that it exhibits strong dependencies on the lower-layer weights, and also on Deep GP.
This idea uses learned global inducing points that is propagated through layer. 

### Exact priors of finite neural networks

<https://www.arxiv.org/abs/2104.11734>

Derive exact solutions for the output priors for individual input examples of a class of fintie fully-connected feedforward Bayesian neural networks.

### Bayesian Neural Network Priors Revisited

<https://arxiv.org/abs/2102.06571>

Find that CNN weights display strong spatial correlations, while FCNNs display heavy-tailed weight distributions.

### Pathologies in priors and inference for Bayesian transformers

<https://arxiv.org/abs/2110.04020>

Weight-space inference in transformers does not work well, regardless of the approximate posterior. Also find that the prior is at least partially at fault but that it is very hard to find well-specified weight priors for these models.

### A global convergence theory for deep ReLU implicit networks via over-parameterization

<https://arxiv.org/abs/2110.05645>

Show that a randomly initialized gradient descent converges to a global minimum at a linear rate for the square loss function if the implicit neural network is over parameterized.

### Non-convergence of stochastic gradient descent in the training of deep neural networks

<https://arxiv.org/abs/2006.07075>

Show that stochastic gradient descent can fail if depth is much larger than their width, and the number of random initialization does not increase to infinity fast enough.

### Uniform convergence may be unable to explain generalization in deep learning

<https://arxiv.org/abs/1902.04742>

Present examples of overparameterized linear classifiers and neural networks trained by gradient descent where uniform convergence provably cannot explain generalization, even if we take into account the implicit bias of GD. 

### Asymptotics of representation learning in finite Bayesian neural networks

<https://arxiv.org/abs/2106.00651>

Argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and Gaussian likelihood have a largely universal form. Illustrate this explicitly for linear MLP or CNN and single nonlinear hidden layer.
