### On the stability properties of Gated Recurrent Units neural networks

<https://www.arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteeing the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs. 

### When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?

<https://www.arxiv.org/abs/2109.09444>

Provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound. Show that domain decomposition which decompose solution to simpler parts and make easier to solve, introduces a tradeoff for generalization, where the decomposition leads to less training data being available in each subdomain, prone to overfitting.

### How Well Generative Adversarial Networks Learn Distributions

<https://arxiv.org/abs/1811.03179>

Nonparametrically, derive the optimal minimax rates for distribution estimation under the adversarial framework. Parametrically, estabilsh a theory for general neural network classes that characterized the interplay on the choice of generator and discriminator pair.

### Certifiably Robust Variational Autoencoders

<https://arxiv.org/abs/2102.07559>

Derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount. Then show how these parameters can be controlled, providing a mechanism to ensure desired level of robustness.

### Deep Autoencoders: From Understanding to Generalization Guarantees

<https://arxiv.org/abs/2009.09525>

Reformulate AEs by continuous piecewise affine structure, to show how AEs approximate the data manifold, giving some insights for reconstruction guarantees and interpretation of regularization guarantees. Design two new regularization that leverages the inherent symmetry learning, prove that the regularizations ensure the generalization with assumption on symmetry of the data with Lie group.

### Theory of gating in recurrent neural network

<https://arxiv.org/abs/2007.14823>

Show that gating offers flexible control of two salient features, timescales and dimensionality.

### MomentumRNN: Integrating Momentum into Recurrent Neural Networks

<https://arxiv.org/abs/2006.06919>

Establish a connection between the hidden state dynamics in an RNN and gradient descent, integrating the momentum to this framework, prove that MomentumRNNs alleviate the vanishing gradient issue.

### Understanding and Mitigating Exploding Inverses in Invertible Neural Networks

<https://arxiv.org/abs/2006.09347>

Show that commonly used INN architectures suffer from explodinig inverses, and reveal failures including the non-applicability of the change-of-variables formula on in- and OOD data, incorrect gradients, inability to sample from normalizing flow. 

### The dynamics of representation learning in shallow, non-linear autoencoders

<https://www.arxiv.org/abs/2201.02115>

Derive a set of asymptotically exact equations that describe the generalisation dynamics of autoenoders trained with SGD in the limit of high-dimensional inputs. 

### Fixed points of nonnegative neural networks

<https://www.arxiv.org/abs/2106.16239>

Derive condition for the existence of fixed points of nonnegative neural networks, by recognizing them as monotonic scalable functions within nonlinear Perron Frobenius theory, and show fixed point set's shape is often interval.

### De Rham compatible Deep Neural Networks

<https://www.arxiv.org/abs/2201.05395>

Construct classes of neural networks with ReLU and BiSU activation, emulating the lowest order Finite Element spaces on regular simplicical partitions of polygonal domains for 2, 3 dimension. 

### Self-scalable Tanh (Stan): Faster Convergence and Better Generalization in Physics-informed Neural Networks

<https://www.arxiv.org/abs/2204.12589>

Propose self-scalable Tanh activation for PINNs, show that PINNs with Stan have no spurious stationary points when using gradient descent algorithms.

### alpha-GAN: Convergence and Estimation Guarantees

<https://www.arxiv.org/abs/2205.06393>

Prove a two-way correspondence between general CPE loss function GANs and the minimization of associated f-divergence. Show that the Arimoto divergences induced by a alpha-GAN equivalently converge for all alpha, and provide estimation bounds.

### Pay attention to your loss: understanding misconceptions about 1-Lipscitz neural networks

<https://www.arxiv.org/abs/2104.05097>

Show that the 1-Lipscitz networks are as accuracte as classical one, and can fit arbitrarily difficult boundaries. Then show these 1-Lipscitz neural networks generalize well under milder assumptions, and finally show that hyper-parameters of the loss are crucial for controlling the accuracy-robustness trade-off.

### Universality and approximation bounds for echo state networks with random weights

<https://www.arxiv.org/abs/2206.05669>

For echo state network with only its readout weights are optimized, show that they are universal under weak conditions for the continuous casual time-invariant operators.

### Optimal training of integer-valued neural networks with mixed integer programming

<https://arxiv.org/abs/2009.03825>

Formulate new MIP model improving the training efficiency which can train the integer-valued neural networks, with optimization of the number of neurons and batch training.

### Concentration inequalities and optimal number of layers for stochastic deep neural networks

<https://arxiv.org/abs/2206.11241>

State the concentration and Markov inequality for output of hidden layers and output of SDNN. This introduce expected classifier, and the probabilistic upper bound for the classification error. Also state the optimal number of layers by optimal stopping procedure.

### Distribution Approximation and Statistical Estimation Guarantees of Generative Adversarial Networks

<https://www.arxiv.org/abs/2002.03938>

Consider the approximation of data distributions that have densities in Hoelder space, show that assuming both discriminator and generator are properly chosen, GAN becomes the consistent estimator of data distribution under strong discrepancy metrics including Wasserstein-1 distance. Moreover when data distribution exhibits low-dimensional structure, show that GANs are capable to capture this strcture and achieve a fast statistical convergence, free of curse of the ambient dimensionality.