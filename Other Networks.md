### On the stability properties of Gated Recurrent Units neural networks

<https://www.arxiv.org/abs/2011.06806>

Provide sufficient conditions for guaranteeing the Input-to-State Stability and the Incremental Input-to-State Stability of GRUs. 

### When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?

<https://www.arxiv.org/abs/2109.09444>

Provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound. Show that domain decomposition which decompose solution to simpler parts and make easier to solve, introduces a tradeoff for generalization, where the decomposition leads to less training data being available in each subdomain, prone to overfitting.

### How Well Generative Adversarial Networks Learn Distributions

<https://arxiv.org/abs/1811.03179>

Nonparametrically, derive the optimal minimax rates for distribution estimation under the adversarial framework. Parametrically, estabilsh a theory for general neural network classes that characterized the interplay on the choice of generator and discriminator pair.

### Certifiably Robust Variational Autoencoders

<https://arxiv.org/abs/2102.07559>

Derive actionable bounds on the minimal size of an input perturbation required to change a VAE's reconstruction by more than an allowed amount. Then show how these parameters can be controlled, providing a mechanism to ensure desired level of robustness.

### Deep Autoencoders: From Understanding to Generalization Guarantees

<https://arxiv.org/abs/2009.09525>

Reformulate AEs by continuous piecewise affine structure, to show how AEs approximate the data manifold, giving some insights for reconstruction guarantees and interpretation of regularization guarantees. Design two new regularization that leverages the inherent symmetry learning, prove that the regularizations ensure the generalization with assumption on symmetry of the data with Lie group.

### Theory of gating in recurrent neural network

<https://arxiv.org/abs/2007.14823>

Show that gating offers flexible control of two salient features, timescales and dimensionality.

### MomentumRNN: Integrating Momentum into Recurrent Neural Networks

<https://arxiv.org/abs/2006.06919>

Establish a connection between the hidden state dynamics in an RNN and gradient descent, integrating the momentum to this framework, prove that MomentumRNNs alleviate the vanishing gradient issue.

### Understanding and Mitigating Exploding Inverses in Invertible Neural Networks

<https://arxiv.org/abs/2006.09347>

Show that commonly used INN architectures suffer from explodinig inverses, and reveal failures including the non-applicability of the change-of-variables formula on in- and OOD data, incorrect gradients, inability to sample from normalizing flow. 

### The dynamics of representation learning in shallow, non-linear autoencoders

<https://www.arxiv.org/abs/2201.02115>

Derive a set of asymptotically exact equations that describe the generalisation dynamics of autoenoders trained with SGD in the limit of high-dimensional inputs. 

### Fixed points of nonnegative neural networks

<https://www.arxiv.org/abs/2106.16239>

Derive condition for the existence of fixed points of nonnegative neural networks, by recognizing them as monotonic scalable functions within nonlinear Perron Frobenius theory, and show fixed point set's shape is often interval.

### De Rham compatible Deep Neural Networks

<https://www.arxiv.org/abs/2201.05395>

Construct classes of neural networks with ReLU and BiSU activation, emulating the lowest order Finite Element spaces on regular simplicical partitions of polygonal domains for 2, 3 dimension. 

### Self-scalable Tanh (Stan): Faster Convergence and Better Generalization in Physics-informed Neural Networks

<https://www.arxiv.org/abs/2204.12589>

Propose self-scalable Tanh activation for PINNs, show that PINNs with Stan have no spurious stationary points when using gradient descent algorithms.

### alpha-GAN: Convergence and Estimation Guarantees

<https://www.arxiv.org/abs/2205.06393>

Prove a two-way correspondence between general CPE loss function GANs and the minimization of associated f-divergence. Show that the Arimoto divergences induced by a alpha-GAN equivalently converge for all alpha, and provide estimation bounds.

### Pay attention to your loss: understanding misconceptions about 1-Lipscitz neural networks

<https://www.arxiv.org/abs/2104.05097>

Show that the 1-Lipscitz networks are as accuracte as classical one, and can fit arbitrarily difficult boundaries. Then show these 1-Lipscitz neural networks generalize well under milder assumptions, and finally show that hyper-parameters of the loss are crucial for controlling the accuracy-robustness trade-off.

### Universality and approximation bounds for echo state networks with random weights

<https://www.arxiv.org/abs/2206.05669>

For echo state network with only its readout weights are optimized, show that they are universal under weak conditions for the continuous casual time-invariant operators.

### Optimal training of integer-valued neural networks with mixed integer programming

<https://arxiv.org/abs/2009.03825>

Formulate new MIP model improving the training efficiency which can train the integer-valued neural networks, with optimization of the number of neurons and batch training.

### Concentration inequalities and optimal number of layers for stochastic deep neural networks

<https://arxiv.org/abs/2206.11241>

State the concentration and Markov inequality for output of hidden layers and output of SDNN. This introduce expected classifier, and the probabilistic upper bound for the classification error. Also state the optimal number of layers by optimal stopping procedure.

### Distribution Approximation and Statistical Estimation Guarantees of Generative Adversarial Networks

<https://www.arxiv.org/abs/2002.03938>

Consider the approximation of data distributions that have densities in Hoelder space, show that assuming both discriminator and generator are properly chosen, GAN becomes the consistent estimator of data distribution under strong discrepancy metrics including Wasserstein-1 distance. Moreover when data distribution exhibits low-dimensional structure, show that GANs are capable to capture this strcture and achieve a fast statistical convergence, free of curse of the ambient dimensionality.

### Certified machine learning: A posteriori error estimation for physics-informed neural networks

<https://www.arxiv.org/abs/2203.17055>

Assuming that the underlying differential equation is ODE, derive a rigorous upper limit on the PINN prediction error, for arbitrary point without knowing the solution.

### Convergence of denoising diffusion models under the manifold hypothesis

<https://www.arxiv.org/abs/2208.05314>

The theoretical analysis of denoising diffusion models assume that the target density is absolutely continuous w.r.t. Lebesgue measure, which does not cover setting when the target distribution is supported on a lower-dimensional manifold or is empirical distribution. Provide the first convergence result for more general setting, which is quantitative bounds on the Wasserstein distance of order one between target data distribution and the generative diftribution.

### Diversity and Generalization in Neural Network Ensembles

<https://arxiv.org/abs/2110.13786>

Provide sound answers to the following questions, how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms.

### On a Sparse Shortcut Topology of Artificial Neural Networks

<https://arxiv.org/abs/1811.09003>

Propose new shortcut architecture, and show that it can approximate any univariate continuous function in width-bounded setting, and show the generalization bound.

### A Kernel-Expanded Stochastic Neural Network

<https://www.arxiv.org/abs/2201.05319>

Design new architecture which incorporates support vector regression at its first layer, allowing to break the high-dimensional nonconvex training of neural network to series of low-dimensional convex optimization, and can be trained using imputation-regularized optimization, with a theoretical guarantee to global convergence.

### Deep Layer-wise Networks Have Closed-Form Weights

<https://www.arxiv.org/abs/2202.01210>

Show that layer-wise network, which trains one layer at a time, has a closed form weight given by kernel mean embedding with global optimum. 

### Transformer Vs. MLP-Mixer Exponential Expressive Gap For NLP Problems

<https://www.arxiv.org/abs/2208.08191>

Analyze the expressive power of mlp-based architectures in modelling dependencies between multiple different inputs, and show an exponential gap between the attention and the mlp-based mechanisms.

### Fixed Points of Cone Mapping with the Application to Neural Networks

<https://www.arxiv.org/abs/2207.09947>

The cone mappings are often modelled with non-negative weight neural networks, however the nonnegative data usually do not guarantee nonnegative weight, hence this assumption often fails, and require weakening on the assumption for fixed point, scalability. Derive condition for the existence of fixed points of cone mappings without assuming scalability of functions, therefore available to applied to such NNs.

### A Closer Look at Learned Optimization: Stability, Robustness, and Inductive Biases

<https://www.arxiv.org/abs/2209.11208>

Use tools from dynamical systems to analyze the inductive bias and stability of the optimization algorithms, which allows us to design inductive biases for blackbox optimizers. Then apply this to noisy quadratic model and introduce modification on learned optimizer.

### On the optimization and generalization of overparameterized implicit neural networks

<https://www.arxiv.org/abs/2209.15562>

Usual analysis on implicit neural network collapse to studying only last layer, so study the case when optimizing the implicit layer only. Show that global convergence is guaranteed, and give generalization that is initialization sensitive.

### Rethinking Lipschitz Neural Networks for Certified L-infinity Robustness

<https://www.arxiv.org/abs/2210.01787>

Show that using the norm-bounded affine layers and Lipschitz activation lose the expressive power, while other Lipschitz networks like GroupSort and L-infinity networks bypass these impossibilities.

### Is L2 Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Networks?

<https://www.arxiv.org/abs/2206.02016>

Show that for Hamilton-Jacobi-Bellman equations, for general Lp Physics informed loss, the equation is stable only if p is sufficiently large. Hence it is better to choose L-infinity loss.

### A Functional Perspective on Learning Symmetric Functions with Neural Networks

<https://www.arxiv.org/abs/2008.06952>

View the permutation-invariant network as function over probability measures, and establish approximation and generalization bounds under different regularizations.