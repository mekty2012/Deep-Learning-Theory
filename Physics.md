### Universality of Deep Neural Networks Lottery Tickets: A Renormalization Group Perspective

<https://www.arxiv.org/abs/2110.03210>

Using renormalization group theory, find that iterative magnitude pruning is a renormalization group scheme, which is the method used for discovering winning tickets.

### Asymptotics of wide networks from feynman diagrams

<https://www.arxiv.org/abs/1909.11304>

Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory

<https://www.arxiv.org/abs/2008.08601>

Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.

### Finite size corrections for neural network Gaussian processes

<https://www.arxiv.org/abs/1908.10030>

Demonstrate that an ensemble of large finite FC network with a single hidden layer is well described by a Gaussian perturbed by the fourth Hermite polynomial, and the scale is 
inversely proportional to the number of units and that higher order terms decay more rapidly, recovering Edgeworth expansion.

### Non-Gaussian processes and neural networks at finite widths

<https://www.arxiv.org/abs/1910.00019>

Perturbatively extend NNGP correspondence to finite width neural network, yielding non-Gaussian processes as priors. This allows to track the flow of preactivation by
marginally integrating random variables, reminiscent of renormalization-group flow.

### Learning through atypical "phase transitions" in overparameterized neural networks

<https://www.arxiv.org/abs/2110.00683>

Use methods from statistical physics to analytically study the computational fallout of overparameterization in nonconvex neural network models. 

### A Theoretical Connection Between Statistical Physics and Reinforcement Learning

<https://www.arxiv.org/abs/1906.10228>

Construct a partition function from the ensemble of possible trajectories, which gives its own Bellman equation with solution tightly linked Boltzmann-like policy parameterizations.

### Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models

<https://www.arxiv.org/abs/2010.13933>

Analytically derive bias and variance in two minimal models, linear regression and two-layer neural newtork, using statistical physics. 

### Separation of scales and a thermodynamics description of feature learning in some CNNs

<https://www.arxiv.org/abs/2112.15383>

Show that some variables have slow change while some are faster, so slow varaiables can be averaged out, and pre-activations fluctuate in a nearly Gaussian manner with a deterministic kernel, which adapt to data unlike infinite case.

### Unified Field Theory for Deep and Recurrent Neural Networks

<https://www.arxiv.org/abs/2112.05589>

Present derivation of the mean field theory for both architectures that starts from first principles by employing established methods from statistical physics, elucidates that while the mean-field equations are different to their temporal structure, they yield identical Guassian kernels when readouts are taken.

### Trainability and Accuracy of Neural Networks: An Interacting Particle System Approach

<https://www.arxiv.org/abs/1805.00915>

Establish condition for global convergence of SGD, by reinterpreting SGD as the evolution of particle system with interaction goverenbed by a potential related to the loss. Show that when the number of units are large, the empirical distribution descends on a convex landscape towards the global minimum, with approximation error scaling inverse to n.

### The edge of chaos: quantum field theory and deep neural networks

<https://www.arxiv.org/abs/2109.13247>

Construct the quantum field theory corresponding to deep neural networks including recurrent and feedforward architectures. Consider the mean-field theory obtained as the leading saddlepoint in the action, and derive the condition for criticality via the largest Lypunov exponent.

### Nonperturbative renormalization for the neural network-QFT correspondence

<https://www.arxiv.org/abs/2108.01403>

Improve the description of neural networks in terms of Wilsonian effective field theory, by providing an analysis in terms of nonperturbative renormalization group using Wetterich-Morris equation. Provide a useful formalism to investigate neural networks behavior beyond the large-width limit, and show that the changing the standard deviation of the neural network distribution can be interpreted as a renormalization flow in the space of networks.

### Correlation Functions in Random Fully Connected Neural Networks at Finite Width

<https://www.arxiv.org/abs/2204.01058>

Give estimate of the joint correlation function of network output and derivative in deep FCNN, which is controlled by depth-to-width ratio.

### The Mori-Zwanzi formulation of deep learning

<https://www.arxiv.org/abs/2209.05544>

Using the duality between deep neural networks and discrete stochastic dynamical systems, that allows to propagate conditional expectations and probability density functions forward and backward. This introduces memory of the network, and develop sufficient conditions for the memory to decay with the number of layers.

### Do ideas have shape? Idea registration as the continuous limit of artificial neural networks

<https://www.arxiv.org/abs/2008.03920>

Show that ResNet and their GP generalization converge, in the infinite depth limit, to a generalization of image registration variational algorithms. Then prove that the convergence with trained weights toward a Hamiltonian dynamics driven flow.

### Renormalization in the neural network-quantum field theory correspondence

<https://www.arxiv.org/abs/2212.11811>

Describe how to implement renormalization of finite N correction of infinite neural networks. 

### Random Fully Connected Neural Networks as Perturbatively Solvable Hierarchies

<https://www.arxiv.org/abs/2204.01058>

For polynomially bounded activations, derive sharp estiamte in powers of 1/width, for joint cumulants of the network output and its derivatives. Show that the form perturbatively solvable hierarchy in powers of 1/width, so that k-th order cumulant have recursion depending on leading order in 1/width only for earlier layers and j <= k cumulants. 

### Injectivity of ReLU networks: perspective from statistical physics

<https://www.arxiv.org/abs/2302.14112>

Show that the injectivity is equivalent to a property of the ground state of the spherical perceptron, and derive analytical equations for the threshold.

### Effective Theory of Transformers at Initialization

<https://www.arxiv.org/abs/2304.02034>

Perform an effective-theory analysis of forward-backward signal propagation in wide and deep Transformers, and suggest particular width scalings and training hyperparamters.