### Universality of Deep Neural Networks Lottery Tickets: A Renormalization Group Perspective

<https://arxiv.org/abs/2110.03210>

Using renormalization group theory, find that iterative magnitude pruning is a renormalization group scheme, which is the method used for discovering winning tickets.

### Asymptotics of wide networks from feynman diagrams

<https://www.arxiv.org/abs/1909.11304>

Use Feynman diagrams, to compute multivariate Gaussian integrals, study training dynamics, improve existing bounds.

### Neural Networks and Quantum Field Theory

<https://www.arxiv.org/abs/2008.08601>

Instead of asymptotic limit, allowing particle interaction makes Wilsonian effective field theory.

### Finite size corrections for neural network Gaussian processes

<https://www.arxiv.org/abs/1908.10030>

Demonstrate that an ensemble of large finite FC network with a single hidden layer is well described by a Gaussian perturbed by the fourth Hermite polynomial, and the scale is 
inversely proportional to the number of units and that higher order terms decay more rapidly, recovering Edgeworth expansion.

### Non-Gaussian processes and neural networks at finite widths

<https://www.arxiv.org/abs/1910.00019>

Perturbatively extend NNGP correspondence to finite width neural network, yielding non-Gaussian processes as priors. This allows to track the flow of preactivation by
marginally integrating random variables, reminiscent of renormalization-group flow.

### Learning through atypical "phase transitions" in overparameterized neural networks

<https://arxiv.org/abs/2110.00683>

Use methods from statistical physics to analytically study the computational fallout of overparameterization in nonconvex neural network models. 

### A Theoretical Connection Between Statistical Physics and Reinforcement Learning

<https://arxiv.org/abs/1906.10228>

Construct a partition function from the ensemble of possible trajectories, which gives its own Bellman equation with solution tightly linked Boltzmann-like policy parameterizations.

### Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models

<https://www.arxiv.org/abs/2010.13933>

Analytically derive bias and variance in two minimal models, linear regression and two-layer neural newtork, using statistical physics. 
